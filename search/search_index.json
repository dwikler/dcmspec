{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DCMspec Documentation","text":"<p>dcmspec is a Python library for downloading, parsing, modeling, and working with DICOM specifications.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Provides an API to parse DICOM standard and IHE document specifications into structured models.</li> <li>Includes sample CLI applications demonstrating extraction and handling of DICOM specifications.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>For setup instructions, refer to the Installation page. Information about configuration options and caching behavior can be found in the Configuration &amp; Caching page.</p> <p>Further information is available in the GitHub repository.</p>"},{"location":"configuration/","title":"Configuration and Caching","text":"<p>By default, downloaded specification documents (such as DICOM standards and IHE profiles) and generated data models (such as JSON files) are stored in a platform-specific cache directory. This location can be customized by specifying a configuration file.</p>"},{"location":"configuration/#default-cache-directory","title":"Default Cache Directory","text":"<ul> <li> <p>Default cache directory location:</p> <p> \ud83c\udf4f MacOS <code>~/Library/Caches/dcmspec</code> \ud83d\udc27 Linux <code>~/.cache/dcmspec</code> \ud83e\ude9f Windows <code>%USERPROFILE%\\AppData\\Local\\dcmspec\\Cache</code> </p> </li> </ul>"},{"location":"configuration/#configuration-of-cache-directory","title":"Configuration of Cache Directory","text":"<p>The cache directory used by API and CLI applications can be changed by providing a configuration file.</p> <p>This file can be named <code>config.json</code> and placed in the default configuration folder, or its location can be specified using the <code>--config</code> command-line option or the <code>DCMSPEC_CONFIG</code> environment variable.</p> <ul> <li> <p>Using the default config file    If no config file is specified, dcmspec searches for <code>config.json</code> in the default configuration folder for the operating system:</p> <p> \ud83c\udf4f MacOS <code>~/Library/Application Support/dcmspec</code> \ud83d\udc27 Linux <code>~/.config/dcmspec</code> \ud83e\ude9f Windows <code>%USERPROFILE%\\AppData\\Local\\dcmspec</code> or <code>%USERPROFILE%\\AppData\\Roaming\\dcmspec</code> </p> </li> </ul> <p>Example <code>config.json</code>:</p> <pre><code>{\n  \"cache_dir\": \"./cache\"\n}\n</code></pre> <ul> <li>Using the <code>--config</code> option:    The path to the config file can be provided on the command line:</li> </ul> <pre><code>poetry run python -m src.dcmspec.cli.modattributes &lt;table_id&gt; --config myconfig.json\n</code></pre> <ul> <li>Using the <code>DCMSPEC_CONFIG</code> environment variable:    The environment variable can be set to the path of the config file:</li> </ul> <pre><code>export DCMSPEC_CONFIG=./myconfig.json\npoetry run python -m src.dcmspec.cli.modattributes &lt;table_id&gt;\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or newer</li> <li>Poetry (optional for CLI users, recommended for developers)</li> </ul>"},{"location":"installation/#for-users-of-cli-applications","title":"For Users of CLI Applications","text":"<ul> <li>Install with pip (recommended for CLI use):</li> </ul> <pre><code>pip install \"git+https://github.com/dwikler/dcmspec.git@v0.1.0\"\n</code></pre> <ul> <li>Alternatively, install with Poetry (requires cloning the repo):</li> </ul> <pre><code>git clone https://github.com/dwikler/dcmspec.git\ncd dcmspec\npoetry install\n</code></pre> <ul> <li>Run CLI applications (replace <code>&lt;script_name&gt;</code> with one of the following):</li> </ul> <pre><code>python -m dcmspec.cli.&lt;script_name&gt; --help\n</code></pre> <p>Examples:</p> <pre><code>python -m dcmspec.cli.iodattributes --help\npython -m dcmspec.cli.tdwiimoddefinition --help\n</code></pre> <p>Or, if using Poetry:</p> <pre><code>poetry run python -m src.dcmspec.cli.&lt;script_name&gt; --help\n</code></pre> <p>See the CLI Applications for available scripts and usage examples.</p>"},{"location":"installation/#for-users-of-the-gui-application","title":"For Users of the GUI Application","text":"<ul> <li>Install the package with the GUI extra (installs <code>tkhtmlview</code>):</li> </ul> <pre><code>pip install \"git+https://github.com/dwikler/dcmspec.git@v0.1.0#egg=dcmspec[gui]\"\n</code></pre> <p>Or, with Poetry:</p> <pre><code>git clone https://github.com/dwikler/dcmspec.git\ncd dcmspec\npoetry install --with gui\n</code></pre> <ul> <li>tkinter is also required for the GUI, but is not installed via pip or Poetry.</li> </ul> <p>Note: <code>tkinter</code> is part of the Python standard library, but on some Linux distributions and on macOS with Homebrew Python, it must be installed separately.</p> <ul> <li>On Ubuntu/Debian: <code>sudo apt install python3-tk</code></li> <li>On Fedora: <code>sudo dnf install python3-tkinter</code></li> <li>On macOS (Homebrew Python): <code>brew install tcl-tk</code></li> <li>You may also need to set environment variables so Python can find the Tk libraries. See Homebrew Python and Tkinter for details.</li> <li>On Windows/macOS (python.org installer): Usually included with the official Python installer.</li> </ul> <p>If you get an error about <code>tkinter</code> not being found, please install it as shown above.</p> <ul> <li>Run the GUI application:</li> </ul> <pre><code>poetry run iod-explorer\n</code></pre> <p>Or, after activating your environment:</p> <pre><code>iod-explorer\n</code></pre>"},{"location":"installation/#for-developers-using-the-api","title":"For Developers Using the API","text":"<ul> <li>Add the following to your <code>pyproject.toml</code> (for Poetry-based projects):</li> </ul> <pre><code>[tool.poetry.dependencies]\ndcmspec = { git = \"https://github.com/dwikler/dcmspec.git\", tag = \"v0.1.0\" }\n</code></pre> <ul> <li>Install the dependencies:</li> </ul> <pre><code>poetry install\n</code></pre> <ul> <li>(Optional) Activate the virtual environment:</li> </ul> <pre><code>poetry shell\n</code></pre> <ul> <li>Import and use the API in your Python code:</li> </ul> <pre><code>from dcmspec.spec_model import SpecModel\n\n# ... your code here ...\n</code></pre>"},{"location":"installation/#dependencies-and-optional-features","title":"Dependencies and Optional Features","text":""},{"location":"installation/#core-dependencies","title":"Core Dependencies","text":"<p>The core <code>dcmspec</code> library is designed to be lightweight and only requires a minimal set of dependencies for parsing and working with DICOM specification tables in HTML/XHTML format.</p> <p>Core dependencies include:</p> <ul> <li>anytree</li> <li>platformdirs</li> <li>unidecode</li> <li>bs4 (BeautifulSoup)</li> <li>requests</li> <li>lxml</li> <li>rich</li> </ul> <p>These are sufficient for most use cases, including all parsing and tree-building from DICOM standard HTML/XHTML documents.</p> <p>Note on lxml: <code>lxml</code> is a core dependency for fast and robust XML/HTML parsing. It is not a pure Python package, but pre-built wheels are available for most platforms. On some Linux systems, you may need to install system packages (e.g., <code>libxml2-dev</code>, <code>libxslt-dev</code>, and <code>python3-dev</code>) before installing <code>lxml</code>. See the lxml installation docs for details.</p>"},{"location":"installation/#optional-pdftable-extraction-dependencies","title":"Optional PDF/Table Extraction Dependencies","text":"<p>Some features, such as extracting tables directly from PDF files, require additional heavy dependencies. These are not installed by default and are grouped under the <code>pdf</code> optional dependency.</p> <p>To install with PDF/table extraction support:</p> <pre><code>poetry add \"dcmspec[pdf]\"@git+https://github.com/dwikler/dcmspec.git\n</code></pre> <p>Optional dependencies for PDF/table extraction:</p> <ul> <li>pdfplumber</li> <li>camelot-py</li> <li>pandas</li> <li>openpyxl</li> <li>opencv-python-headless</li> </ul> <p>These are only needed if you want to extract tables from PDF documents.</p>"},{"location":"installation/#gui-dependencies","title":"GUI Dependencies","text":"<p>If you want to use the sample GUI explorer app, you can install the <code>gui</code> extra:</p> <pre><code>poetry add \"dcmspec[gui]\"@git+https://github.com/dwikler/dcmspec.git\n</code></pre> <p>This will install:</p> <ul> <li>tkhtmlview</li> </ul>"},{"location":"installation/#summary","title":"Summary","text":"<ul> <li>Default install: Lightweight, core parsing features only.</li> <li>With <code>[pdf]</code> extra: Adds PDF/table extraction support.</li> <li>With <code>[gui]</code> extra: Adds GUI dependencies for the sample explorer app.</li> </ul> <p>See the pyproject.toml for the full list of dependencies and extras.</p> <p>See the API Reference for details on available classes.</p>"},{"location":"api/","title":"API Overview","text":"<p>This section documents the main modules and classes provided by dcmspec.</p> <p>The API enables extraction, parsing, and processing of DICOM specification tables and related data from the DICOM standard and IHE documents for use in Python projects. The modules are organized by functional area and typical workflow, facilitating the discovery of related concepts and effective use of the library.</p>"},{"location":"api/#available-api-modules","title":"Available API Modules","text":"<ul> <li>Constants Modules: Default values and constants used throughout the library.</li> <li>Core Classes: Main data models and factories for working with DICOM specifications.</li> <li>Parse Classes: Classes for parsing DICOM tables from various formats.</li> <li>Load Classes: Classes for loading and handling DICOM documents.</li> <li>Store Classes: Classes for storing and caching parsed specifications.</li> <li>Print Classes: Classes for printing and displaying specification data.</li> <li>Utils Classes: Utility classes for configuration and DOM manipulation.</li> </ul>"},{"location":"api/#how-to-use","title":"How to Use","text":"<p>You can import and use any API module in your own Python code. For example:</p> <pre><code>from dcmspec.spec_model import SpecModel from dcmspec.spec_factory import SpecFactory\n</code></pre>"},{"location":"api/config/","title":"Config","text":""},{"location":"api/config/#dcmspec.config.Config","title":"<code>dcmspec.config.Config</code>","text":"<p>Manages application configuration.</p> <p>Reserved configuration keys: - cache_dir: Cache Directory path used by the library. If not set by the user, OS-specific default is used.</p> <p>Users may add their own keys, but should not overwrite reserved keys unless they intend to change library behavior.</p> Source code in <code>src/dcmspec/config.py</code> <pre><code>class Config:\n    \"\"\"Manages application configuration.\n\n    Reserved configuration keys:\n    - cache_dir: Cache Directory path used by the library. If not set by the user, OS-specific default is used.\n\n    Users may add their own keys, but should not overwrite reserved keys unless they intend to change library behavior.\n    \"\"\"\n\n    def __init__(self, app_name: str = \"dcmspec\", config_file: Optional[str] = None):\n        \"\"\"Initialize the Config object.\n\n        Args:\n            app_name: The application name used for determining default config/cache directories.\n            config_file: Optional path to a specific config file. If not provided, a default location is used.\n\n        \"\"\"\n        self.app_name: str = app_name\n        self.config_file: str = config_file or os.path.join(user_config_dir(app_name), \"config.json\")\n\n        # Check if config_file is a directory; if so, warn and fall back to default config\n        if os.path.isdir(self.config_file):\n            print(f\"Warning: The config_file path '{self.config_file}' is a directory, not a file. Using default.\")\n            self._data: Dict[str, Any] = {\"cache_dir\": user_cache_dir(app_name)}\n            return\n\n        # Initialize config with OS-specific default value for cache directory\n        self._data: Dict[str, Any] = {\"cache_dir\": user_cache_dir(app_name)}\n\n        self.load_config()\n\n    def load_config(self) -&gt; None:\n        \"\"\"Load configuration from the config file if it exists.\n\n        Creates the cache directory if it does not exist.\n        \"\"\"\n        try:\n            if os.path.exists(self.config_file):\n                with open(self.config_file, \"r\", encoding=\"utf-8\") as f:\n                    config: Dict[str, Any] = json.load(f)\n                    self._data.update(config)\n        except (OSError, json.JSONDecodeError) as e:\n            print(f\"Failed to load configuration file {self.config_file}: {e}\")\n\n        cache_dir = self.get_param(\"cache_dir\")\n        try:\n            os.makedirs(cache_dir, exist_ok=True)\n        except FileExistsError:\n            print(f\"Error: The cache_dir path '{cache_dir}' exists and is not a directory.\")\n            return\n\n        # Handle rare case where the path may not be a directory and, for any reason, os.makedirs did not fail.\n        if not os.path.isdir(cache_dir):\n            print(f\"Error: The cache_dir path '{cache_dir}' is not a directory.\")\n            return\n\n    def save_config(self) -&gt; None:\n        \"\"\"Save the current configuration to the config file.\"\"\"\n        try:\n            os.makedirs(os.path.dirname(self.config_file), exist_ok=True)\n            with open(self.config_file, \"w\", encoding=\"utf-8\") as f:\n                json.dump(self._data, f, indent=4)\n        except OSError as e:\n            print(f\"Failed to save configuration file {self.config_file}: {e}\")\n\n    def set_param(self, key: str, value: Any) -&gt; None:\n        \"\"\"Set a configuration parameter.\"\"\"\n        self._data[key] = value\n\n    def get_param(self, key: str) -&gt; Optional[Any]:\n        \"\"\"Get a configuration parameter by key.\"\"\"\n        return self._data.get(key)\n\n    @property\n    def cache_dir(self) -&gt; str:\n        \"\"\"Access the cache directory used by the library.\"\"\"\n        return self.get_param(\"cache_dir\")\n</code></pre>"},{"location":"api/config/#dcmspec.config.Config.cache_dir","title":"<code>cache_dir</code>  <code>property</code>","text":"<p>Access the cache directory used by the library.</p>"},{"location":"api/config/#dcmspec.config.Config.__init__","title":"<code>__init__(app_name='dcmspec', config_file=None)</code>","text":"<p>Initialize the Config object.</p> PARAMETER DESCRIPTION <code>app_name</code> <p>The application name used for determining default config/cache directories.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'dcmspec'</code> </p> <code>config_file</code> <p>Optional path to a specific config file. If not provided, a default location is used.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/dcmspec/config.py</code> <pre><code>def __init__(self, app_name: str = \"dcmspec\", config_file: Optional[str] = None):\n    \"\"\"Initialize the Config object.\n\n    Args:\n        app_name: The application name used for determining default config/cache directories.\n        config_file: Optional path to a specific config file. If not provided, a default location is used.\n\n    \"\"\"\n    self.app_name: str = app_name\n    self.config_file: str = config_file or os.path.join(user_config_dir(app_name), \"config.json\")\n\n    # Check if config_file is a directory; if so, warn and fall back to default config\n    if os.path.isdir(self.config_file):\n        print(f\"Warning: The config_file path '{self.config_file}' is a directory, not a file. Using default.\")\n        self._data: Dict[str, Any] = {\"cache_dir\": user_cache_dir(app_name)}\n        return\n\n    # Initialize config with OS-specific default value for cache directory\n    self._data: Dict[str, Any] = {\"cache_dir\": user_cache_dir(app_name)}\n\n    self.load_config()\n</code></pre>"},{"location":"api/config/#dcmspec.config.Config.get_param","title":"<code>get_param(key)</code>","text":"<p>Get a configuration parameter by key.</p> Source code in <code>src/dcmspec/config.py</code> <pre><code>def get_param(self, key: str) -&gt; Optional[Any]:\n    \"\"\"Get a configuration parameter by key.\"\"\"\n    return self._data.get(key)\n</code></pre>"},{"location":"api/config/#dcmspec.config.Config.load_config","title":"<code>load_config()</code>","text":"<p>Load configuration from the config file if it exists.</p> <p>Creates the cache directory if it does not exist.</p> Source code in <code>src/dcmspec/config.py</code> <pre><code>def load_config(self) -&gt; None:\n    \"\"\"Load configuration from the config file if it exists.\n\n    Creates the cache directory if it does not exist.\n    \"\"\"\n    try:\n        if os.path.exists(self.config_file):\n            with open(self.config_file, \"r\", encoding=\"utf-8\") as f:\n                config: Dict[str, Any] = json.load(f)\n                self._data.update(config)\n    except (OSError, json.JSONDecodeError) as e:\n        print(f\"Failed to load configuration file {self.config_file}: {e}\")\n\n    cache_dir = self.get_param(\"cache_dir\")\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n    except FileExistsError:\n        print(f\"Error: The cache_dir path '{cache_dir}' exists and is not a directory.\")\n        return\n\n    # Handle rare case where the path may not be a directory and, for any reason, os.makedirs did not fail.\n    if not os.path.isdir(cache_dir):\n        print(f\"Error: The cache_dir path '{cache_dir}' is not a directory.\")\n        return\n</code></pre>"},{"location":"api/config/#dcmspec.config.Config.save_config","title":"<code>save_config()</code>","text":"<p>Save the current configuration to the config file.</p> Source code in <code>src/dcmspec/config.py</code> <pre><code>def save_config(self) -&gt; None:\n    \"\"\"Save the current configuration to the config file.\"\"\"\n    try:\n        os.makedirs(os.path.dirname(self.config_file), exist_ok=True)\n        with open(self.config_file, \"w\", encoding=\"utf-8\") as f:\n            json.dump(self._data, f, indent=4)\n    except OSError as e:\n        print(f\"Failed to save configuration file {self.config_file}: {e}\")\n</code></pre>"},{"location":"api/config/#dcmspec.config.Config.set_param","title":"<code>set_param(key, value)</code>","text":"<p>Set a configuration parameter.</p> Source code in <code>src/dcmspec/config.py</code> <pre><code>def set_param(self, key: str, value: Any) -&gt; None:\n    \"\"\"Set a configuration parameter.\"\"\"\n    self._data[key] = value\n</code></pre>"},{"location":"api/csv_table_spec_parser/","title":"CSVTableSpecParser","text":""},{"location":"api/csv_table_spec_parser/#dcmspec.csv_table_spec_parser.CSVTableSpecParser","title":"<code>dcmspec.csv_table_spec_parser.CSVTableSpecParser</code>","text":"<p>               Bases: <code>SpecParser</code></p> <p>Base parser for DICOM Specification IHE tables in CSV-like format.</p> Source code in <code>src/dcmspec/csv_table_spec_parser.py</code> <pre><code>class CSVTableSpecParser(SpecParser):\n    \"\"\"Base parser for DICOM Specification IHE tables in CSV-like format.\"\"\"\n\n    def parse(\n        self,\n        table: dict,\n        column_to_attr,\n        name_attr=\"elem_name\",\n        table_id=None,\n        include_depth=None,\n    ) -&gt; Tuple[Node, Node]:\n        \"\"\"Parse specification metadata and content from a single table dict.\n\n        Args:\n            table (dict): A table dict as output by PDFDocHandler.concat_tables, with 'header' and 'data' keys.\n            column_to_attr (dict): Mapping from column indices to node attribute names.\n            name_attr (str): The attribute to use for node names.\n            table_id (str, optional): Table identifier for model parsing.\n            include_depth (int, optional): The depth to which included tables should be parsed.\n\n        Returns:\n            tuple: (metadata_node, content_node)\n\n        \"\"\"\n        # Use the header and data from the grouped table dict\n        header = table.get(\"header\", [])\n        data = table.get(\"data\", [])\n\n        metadata = Node(\"metadata\")\n        metadata.header = header\n        metadata.column_to_attr = column_to_attr\n        metadata.table_id = table_id\n        if include_depth is not None:\n            metadata.include_depth = int(include_depth)\n        content = self.parse_table([data], column_to_attr, name_attr)\n        return metadata, content\n\n    def parse_table(\n        self,\n        tables: list,  # List of tables, each a list of rows (list of str)\n        column_to_attr: dict,\n        name_attr: str = \"elem_name\",\n    ) -&gt; Node:\n        \"\"\"Build a tree from tables using column mapping and '&gt;' nesting logic.\n\n        Args:\n            tables (list): List of tables, each a list of rows (list of str).\n            column_to_attr (dict): Mapping from column indices to node attribute names.\n            name_attr (str): The attribute to use for node names.\n\n        Returns:\n            Node: The root node of the tree.\n\n        \"\"\"\n        root = Node(\"content\")\n        parent_nodes = {0: root}\n        for table in tables:\n            for row in table:\n                row_data = {}\n                for col_idx, attr in column_to_attr.items():\n                    value = row[col_idx] if col_idx &lt; len(row) else \"\"\n                    # Clean up newlines in the cell to be used as node name\n                    if attr == name_attr:\n                        value = value.replace(\"\\n\", \" \")\n                    row_data[attr] = value\n                node_name = row_data[name_attr]\n                level = node_name.count(\"&gt;\") + 1\n                # Ensure all parent levels exist\n                if (level - 1) not in parent_nodes:\n                    # If a parent is missing, attach to root\n                    parent_nodes[level - 1] = root\n                parent = parent_nodes[level - 1]\n                child = Node(node_name, parent=parent, **row_data)\n                parent_nodes[level] = child\n        return root\n</code></pre>"},{"location":"api/csv_table_spec_parser/#dcmspec.csv_table_spec_parser.CSVTableSpecParser.parse","title":"<code>parse(table, column_to_attr, name_attr='elem_name', table_id=None, include_depth=None)</code>","text":"<p>Parse specification metadata and content from a single table dict.</p> PARAMETER DESCRIPTION <code>table</code> <p>A table dict as output by PDFDocHandler.concat_tables, with 'header' and 'data' keys.</p> <p> TYPE: <code>dict</code> </p> <code>column_to_attr</code> <p>Mapping from column indices to node attribute names.</p> <p> TYPE: <code>dict</code> </p> <code>name_attr</code> <p>The attribute to use for node names.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'elem_name'</code> </p> <code>table_id</code> <p>Table identifier for model parsing.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>include_depth</code> <p>The depth to which included tables should be parsed.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>(metadata_node, content_node)</p> <p> TYPE: <code>Tuple[Node, Node]</code> </p> Source code in <code>src/dcmspec/csv_table_spec_parser.py</code> <pre><code>def parse(\n    self,\n    table: dict,\n    column_to_attr,\n    name_attr=\"elem_name\",\n    table_id=None,\n    include_depth=None,\n) -&gt; Tuple[Node, Node]:\n    \"\"\"Parse specification metadata and content from a single table dict.\n\n    Args:\n        table (dict): A table dict as output by PDFDocHandler.concat_tables, with 'header' and 'data' keys.\n        column_to_attr (dict): Mapping from column indices to node attribute names.\n        name_attr (str): The attribute to use for node names.\n        table_id (str, optional): Table identifier for model parsing.\n        include_depth (int, optional): The depth to which included tables should be parsed.\n\n    Returns:\n        tuple: (metadata_node, content_node)\n\n    \"\"\"\n    # Use the header and data from the grouped table dict\n    header = table.get(\"header\", [])\n    data = table.get(\"data\", [])\n\n    metadata = Node(\"metadata\")\n    metadata.header = header\n    metadata.column_to_attr = column_to_attr\n    metadata.table_id = table_id\n    if include_depth is not None:\n        metadata.include_depth = int(include_depth)\n    content = self.parse_table([data], column_to_attr, name_attr)\n    return metadata, content\n</code></pre>"},{"location":"api/csv_table_spec_parser/#dcmspec.csv_table_spec_parser.CSVTableSpecParser.parse_table","title":"<code>parse_table(tables, column_to_attr, name_attr='elem_name')</code>","text":"<p>Build a tree from tables using column mapping and '&gt;' nesting logic.</p> PARAMETER DESCRIPTION <code>tables</code> <p>List of tables, each a list of rows (list of str).</p> <p> TYPE: <code>list</code> </p> <code>column_to_attr</code> <p>Mapping from column indices to node attribute names.</p> <p> TYPE: <code>dict</code> </p> <code>name_attr</code> <p>The attribute to use for node names.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'elem_name'</code> </p> RETURNS DESCRIPTION <code>Node</code> <p>The root node of the tree.</p> <p> TYPE: <code>Node</code> </p> Source code in <code>src/dcmspec/csv_table_spec_parser.py</code> <pre><code>def parse_table(\n    self,\n    tables: list,  # List of tables, each a list of rows (list of str)\n    column_to_attr: dict,\n    name_attr: str = \"elem_name\",\n) -&gt; Node:\n    \"\"\"Build a tree from tables using column mapping and '&gt;' nesting logic.\n\n    Args:\n        tables (list): List of tables, each a list of rows (list of str).\n        column_to_attr (dict): Mapping from column indices to node attribute names.\n        name_attr (str): The attribute to use for node names.\n\n    Returns:\n        Node: The root node of the tree.\n\n    \"\"\"\n    root = Node(\"content\")\n    parent_nodes = {0: root}\n    for table in tables:\n        for row in table:\n            row_data = {}\n            for col_idx, attr in column_to_attr.items():\n                value = row[col_idx] if col_idx &lt; len(row) else \"\"\n                # Clean up newlines in the cell to be used as node name\n                if attr == name_attr:\n                    value = value.replace(\"\\n\", \" \")\n                row_data[attr] = value\n            node_name = row_data[name_attr]\n            level = node_name.count(\"&gt;\") + 1\n            # Ensure all parent levels exist\n            if (level - 1) not in parent_nodes:\n                # If a parent is missing, attach to root\n                parent_nodes[level - 1] = root\n            parent = parent_nodes[level - 1]\n            child = Node(node_name, parent=parent, **row_data)\n            parent_nodes[level] = child\n    return root\n</code></pre>"},{"location":"api/doc_handler/","title":"DocHandler","text":""},{"location":"api/doc_handler/#dcmspec.doc_handler.DocHandler","title":"<code>dcmspec.doc_handler.DocHandler</code>","text":"<p>Base class for DICOM document handlers.</p> <p>Handles DICOM documents in various formats (e.g., XHTML, PDF). Subclasses must implement the <code>load_document</code> method to handle reading/parsing input files. The base class provides a generic download method for both text and binary files.</p> <p>Progress Reporting: The observer pattern is used for progress reporting. Subclasses may extend the Progress class and use the progress_observer to report additional information (e.g., status, errors, or other context) beyond percent complete, enabling future extensibility for richer progress tracking.</p> Source code in <code>src/dcmspec/doc_handler.py</code> <pre><code>class DocHandler:\n    \"\"\"Base class for DICOM document handlers.\n\n    Handles DICOM documents in various formats (e.g., XHTML, PDF).\n    Subclasses must implement the `load_document` method to handle\n    reading/parsing input files. The base class provides a generic\n    download method for both text and binary files.\n\n    Progress Reporting:\n    The observer pattern is used for progress reporting. Subclasses may extend\n    the Progress class and use the progress_observer to report additional information\n    (e.g., status, errors, or other context) beyond percent complete, enabling future\n    extensibility for richer progress tracking.\n    \"\"\"\n\n    def __init__(self, config: Optional[Config] = None, logger: Optional[logging.Logger] = None):\n        \"\"\"Initialize the document handler with an optional logger.\n\n        Args:\n            config (Optional[Config]): Config instance to use. If None, a default Config is created.\n            logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n        Logging:\n            A logger may be passed for custom logging control. If no logger is provided,\n            a default logger for this class is used. In both cases, no logging handlers\n            are added by default. To see log output, logging should be configured in the\n            application (e.g., with logging.basicConfig()).\n\n        \"\"\"\n        if logger is not None and not isinstance(logger, logging.Logger):\n            raise TypeError(\"logger must be an instance of logging.Logger or None\")\n        self.logger = logger or logging.getLogger(self.__class__.__name__)\n\n        if config is not None and not isinstance(config, Config):\n            raise TypeError(\"config must be an instance of Config or None\")\n        self.config = config or Config()\n\n    def download(\n        self,\n        url: str,\n        file_path: str,\n        binary: bool = False,\n        progress_observer: 'Optional[ProgressObserver]' = None,\n        # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n        progress_callback: 'Optional[Callable[[int], None]]' = None\n        # END LEGACY SUPPORT\n    ) -&gt; str:\n        \"\"\"Download a file from a URL and save it to the specified path.\n\n        Args:\n            url (str): The URL to download the file from.\n            file_path (str): The path to save the downloaded file.\n            binary (bool): If True, save as binary. If False, save as UTF-8 text.\n            progress_observer (Optional[ProgressObserver]): Optional observer to report download progress.\n            progress_callback (Optional[Callable[[int], None]]): [LEGACY, Deprecated] Optional callback to\n                report progress as an integer percent (0-100, or -1 if indeterminate). Use progress_observer\n                instead. Will be removed in a future release.\n\n        Returns:\n            str: The file path where the document was saved.\n\n        Raises:\n            RuntimeError: If the download or save fails.\n\n        \"\"\"\n        # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n        progress_observer = handle_legacy_callback(progress_observer, progress_callback)\n        # END LEGACY SUPPORT\n        self.logger.info(f\"Downloading document from {url} to {file_path}\")\n        try:\n            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n        except OSError as e:\n            self.logger.error(f\"Failed to create directory for {file_path}: {e}\")\n            raise RuntimeError(f\"Failed to create directory for {file_path}: {e}\") from e\n        try:\n            with requests.get(url, timeout=30, stream=True, headers={\"Accept-Encoding\": \"identity\"}) as response:\n                response.raise_for_status()\n                total = int(response.headers.get('content-length', 0))\n                chunk_size = 8192\n                if binary:\n                    self._download_binary(response, file_path, total, chunk_size, progress_observer)\n                else:\n                    self._download_text(response, file_path, total, chunk_size, progress_observer)\n            self.logger.info(f\"Document downloaded to {file_path}\")\n            return file_path\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"Failed to download {url}: {e}\")\n            raise RuntimeError(f\"Failed to download {url}: {e}\") from e\n        except OSError as e:\n            self.logger.error(f\"Failed to save file {file_path}: {e}\")\n            raise RuntimeError(f\"Failed to save file {file_path}: {e}\") from e\n\n    def _report_progress(self, downloaded, total, progress_observer, last_percent):\n        \"\"\"Report progress if percent changed.\n\n        If the total file size is unknown or invalid, calls the observer with -1 to indicate\n        indeterminate progress. Otherwise, calls the observer with the integer percent (0-100).\n        Adds status=ProgressStatus.DOWNLOADING to all progress events.\n        \"\"\"\n        if not progress_observer:\n            return\n\n        percent = calculate_percent(downloaded, total)\n        if percent != last_percent[0]:\n            progress_observer(Progress(percent, status=ProgressStatus.DOWNLOADING))\n            last_percent[0] = percent\n\n    def _download_binary(self, response, file_path, total, chunk_size, progress_observer):\n        \"\"\"Download and save a binary file with progress reporting.\n\n        Streams each chunk directly to the file to avoid high memory usage.\n        Reports progress using the provided observer.\n        \"\"\"\n        downloaded = 0\n        last_percent = [None]\n        with open(file_path, \"wb\") as f:\n            for chunk in response.iter_content(chunk_size=chunk_size):\n                if chunk:\n                    # For binary, no cleaning is needed\n                    f.write(chunk)\n                    downloaded += len(chunk)\n                    self._report_progress(downloaded, total, progress_observer, last_percent)\n\n    def _download_text(self, response, file_path, total, chunk_size, progress_observer):\n        \"\"\"Download and save a text file with progress reporting.\n\n        Streams cleaned chunks directly to the file to avoid high memory usage.\n        Reports progress using the provided observer using response.encoding if available\n        for accurate byte counting.\n        \"\"\"\n        downloaded = 0\n        last_percent = [None]\n        encoding = response.encoding or \"utf-8\"\n        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n            for chunk in response.iter_content(chunk_size=chunk_size, decode_unicode=True):\n                if chunk:\n                    cleaned_chunk = self.clean_text(chunk)\n                    f.write(cleaned_chunk)\n                    chunk_bytes = cleaned_chunk.encode(encoding)\n                    downloaded += len(chunk_bytes)\n                    self._report_progress(downloaded, total, progress_observer, last_percent)\n\n    def clean_text(self, text: str) -&gt; str:\n        \"\"\"Clean text content before saving.\n\n        Subclasses can override this to perform format-specific cleaning (e.g., remove ZWSP/NBSP for XHTML).\n        By default, returns the text unchanged.\n\n        Args:\n            text (str): The text content to clean.\n\n        Returns:\n            str: The cleaned text.\n\n        \"\"\"\n        return text\n\n    def load_document(\n        self,\n        cache_file_name: str,\n        url: Optional[str] = None,\n        force_download: bool = False,\n        progress_observer: 'Optional[ProgressObserver]' = None,\n        # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n        progress_callback: 'Optional[Callable[[int], None]]' = None,\n        # END LEGACY SUPPORT\n        *args: Any,\n        **kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Implement this method to read and parse the document file, returning a parsed object.\n\n        Subclasses should implement this method to load and parse a document file\n        (e.g., XHTML, PDF, CSV) and return a format-specific parsed object.\n        The exact type of the returned object depends on the subclass\n        (e.g., BeautifulSoup for XHTML, pdfplumber.PDF for PDF).\n\n        Args:\n            cache_file_name (str): Path or name of the local cached file.\n            url (str, optional): URL to download the file from if not cached or if force_download is True.\n            force_download (bool, optional): If True, download the file even if it exists locally.\n            progress_observer (Optional[ProgressObserver]): Optional observer to report download progress.\n            progress_callback (Optional[Callable[[int], None]]): [LEGACY, Deprecated] Optional callback to\n                report progress as an integer percent (0-100, or -1 if indeterminate). Use progress_observer\n                instead. Will be removed in a future release.\n            *args: Additional positional arguments for format-specific loading.\n            **kwargs: Additional keyword arguments for format-specific loading.\n\n        Returns:\n            Any: The parsed document object (type depends on subclass).\n\n        \"\"\"\n        # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n        progress_observer = handle_legacy_callback(progress_observer, progress_callback)\n        # END LEGACY SUPPORT\n        raise NotImplementedError(\"Subclasses must implement load_document()\")\n</code></pre>"},{"location":"api/doc_handler/#dcmspec.doc_handler.DocHandler.__init__","title":"<code>__init__(config=None, logger=None)</code>","text":"<p>Initialize the document handler with an optional logger.</p> PARAMETER DESCRIPTION <code>config</code> <p>Config instance to use. If None, a default Config is created.</p> <p> TYPE: <code>Optional[Config]</code> DEFAULT: <code>None</code> </p> <code>logger</code> <p>Logger instance to use. If None, a default logger is created.</p> <p> TYPE: <code>Optional[Logger]</code> DEFAULT: <code>None</code> </p> Logging <p>A logger may be passed for custom logging control. If no logger is provided, a default logger for this class is used. In both cases, no logging handlers are added by default. To see log output, logging should be configured in the application (e.g., with logging.basicConfig()).</p> Source code in <code>src/dcmspec/doc_handler.py</code> <pre><code>def __init__(self, config: Optional[Config] = None, logger: Optional[logging.Logger] = None):\n    \"\"\"Initialize the document handler with an optional logger.\n\n    Args:\n        config (Optional[Config]): Config instance to use. If None, a default Config is created.\n        logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n    Logging:\n        A logger may be passed for custom logging control. If no logger is provided,\n        a default logger for this class is used. In both cases, no logging handlers\n        are added by default. To see log output, logging should be configured in the\n        application (e.g., with logging.basicConfig()).\n\n    \"\"\"\n    if logger is not None and not isinstance(logger, logging.Logger):\n        raise TypeError(\"logger must be an instance of logging.Logger or None\")\n    self.logger = logger or logging.getLogger(self.__class__.__name__)\n\n    if config is not None and not isinstance(config, Config):\n        raise TypeError(\"config must be an instance of Config or None\")\n    self.config = config or Config()\n</code></pre>"},{"location":"api/doc_handler/#dcmspec.doc_handler.DocHandler.clean_text","title":"<code>clean_text(text)</code>","text":"<p>Clean text content before saving.</p> <p>Subclasses can override this to perform format-specific cleaning (e.g., remove ZWSP/NBSP for XHTML). By default, returns the text unchanged.</p> PARAMETER DESCRIPTION <code>text</code> <p>The text content to clean.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The cleaned text.</p> <p> TYPE: <code>str</code> </p> Source code in <code>src/dcmspec/doc_handler.py</code> <pre><code>def clean_text(self, text: str) -&gt; str:\n    \"\"\"Clean text content before saving.\n\n    Subclasses can override this to perform format-specific cleaning (e.g., remove ZWSP/NBSP for XHTML).\n    By default, returns the text unchanged.\n\n    Args:\n        text (str): The text content to clean.\n\n    Returns:\n        str: The cleaned text.\n\n    \"\"\"\n    return text\n</code></pre>"},{"location":"api/doc_handler/#dcmspec.doc_handler.DocHandler.download","title":"<code>download(url, file_path, binary=False, progress_observer=None, progress_callback=None)</code>","text":"<p>Download a file from a URL and save it to the specified path.</p> PARAMETER DESCRIPTION <code>url</code> <p>The URL to download the file from.</p> <p> TYPE: <code>str</code> </p> <code>file_path</code> <p>The path to save the downloaded file.</p> <p> TYPE: <code>str</code> </p> <code>binary</code> <p>If True, save as binary. If False, save as UTF-8 text.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>progress_observer</code> <p>Optional observer to report download progress.</p> <p> TYPE: <code>Optional[ProgressObserver]</code> DEFAULT: <code>None</code> </p> <code>progress_callback</code> <p>[LEGACY, Deprecated] Optional callback to report progress as an integer percent (0-100, or -1 if indeterminate). Use progress_observer instead. Will be removed in a future release.</p> <p> TYPE: <code>Optional[Callable[[int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The file path where the document was saved.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If the download or save fails.</p> Source code in <code>src/dcmspec/doc_handler.py</code> <pre><code>def download(\n    self,\n    url: str,\n    file_path: str,\n    binary: bool = False,\n    progress_observer: 'Optional[ProgressObserver]' = None,\n    # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n    progress_callback: 'Optional[Callable[[int], None]]' = None\n    # END LEGACY SUPPORT\n) -&gt; str:\n    \"\"\"Download a file from a URL and save it to the specified path.\n\n    Args:\n        url (str): The URL to download the file from.\n        file_path (str): The path to save the downloaded file.\n        binary (bool): If True, save as binary. If False, save as UTF-8 text.\n        progress_observer (Optional[ProgressObserver]): Optional observer to report download progress.\n        progress_callback (Optional[Callable[[int], None]]): [LEGACY, Deprecated] Optional callback to\n            report progress as an integer percent (0-100, or -1 if indeterminate). Use progress_observer\n            instead. Will be removed in a future release.\n\n    Returns:\n        str: The file path where the document was saved.\n\n    Raises:\n        RuntimeError: If the download or save fails.\n\n    \"\"\"\n    # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n    progress_observer = handle_legacy_callback(progress_observer, progress_callback)\n    # END LEGACY SUPPORT\n    self.logger.info(f\"Downloading document from {url} to {file_path}\")\n    try:\n        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    except OSError as e:\n        self.logger.error(f\"Failed to create directory for {file_path}: {e}\")\n        raise RuntimeError(f\"Failed to create directory for {file_path}: {e}\") from e\n    try:\n        with requests.get(url, timeout=30, stream=True, headers={\"Accept-Encoding\": \"identity\"}) as response:\n            response.raise_for_status()\n            total = int(response.headers.get('content-length', 0))\n            chunk_size = 8192\n            if binary:\n                self._download_binary(response, file_path, total, chunk_size, progress_observer)\n            else:\n                self._download_text(response, file_path, total, chunk_size, progress_observer)\n        self.logger.info(f\"Document downloaded to {file_path}\")\n        return file_path\n    except requests.exceptions.RequestException as e:\n        self.logger.error(f\"Failed to download {url}: {e}\")\n        raise RuntimeError(f\"Failed to download {url}: {e}\") from e\n    except OSError as e:\n        self.logger.error(f\"Failed to save file {file_path}: {e}\")\n        raise RuntimeError(f\"Failed to save file {file_path}: {e}\") from e\n</code></pre>"},{"location":"api/doc_handler/#dcmspec.doc_handler.DocHandler.load_document","title":"<code>load_document(cache_file_name, url=None, force_download=False, progress_observer=None, progress_callback=None, *args, **kwargs)</code>","text":"<p>Implement this method to read and parse the document file, returning a parsed object.</p> <p>Subclasses should implement this method to load and parse a document file (e.g., XHTML, PDF, CSV) and return a format-specific parsed object. The exact type of the returned object depends on the subclass (e.g., BeautifulSoup for XHTML, pdfplumber.PDF for PDF).</p> PARAMETER DESCRIPTION <code>cache_file_name</code> <p>Path or name of the local cached file.</p> <p> TYPE: <code>str</code> </p> <code>url</code> <p>URL to download the file from if not cached or if force_download is True.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>force_download</code> <p>If True, download the file even if it exists locally.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>progress_observer</code> <p>Optional observer to report download progress.</p> <p> TYPE: <code>Optional[ProgressObserver]</code> DEFAULT: <code>None</code> </p> <code>progress_callback</code> <p>[LEGACY, Deprecated] Optional callback to report progress as an integer percent (0-100, or -1 if indeterminate). Use progress_observer instead. Will be removed in a future release.</p> <p> TYPE: <code>Optional[Callable[[int], None]]</code> DEFAULT: <code>None</code> </p> <code>*args</code> <p>Additional positional arguments for format-specific loading.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>Additional keyword arguments for format-specific loading.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The parsed document object (type depends on subclass).</p> <p> TYPE: <code>Any</code> </p> Source code in <code>src/dcmspec/doc_handler.py</code> <pre><code>def load_document(\n    self,\n    cache_file_name: str,\n    url: Optional[str] = None,\n    force_download: bool = False,\n    progress_observer: 'Optional[ProgressObserver]' = None,\n    # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n    progress_callback: 'Optional[Callable[[int], None]]' = None,\n    # END LEGACY SUPPORT\n    *args: Any,\n    **kwargs: Any\n) -&gt; Any:\n    \"\"\"Implement this method to read and parse the document file, returning a parsed object.\n\n    Subclasses should implement this method to load and parse a document file\n    (e.g., XHTML, PDF, CSV) and return a format-specific parsed object.\n    The exact type of the returned object depends on the subclass\n    (e.g., BeautifulSoup for XHTML, pdfplumber.PDF for PDF).\n\n    Args:\n        cache_file_name (str): Path or name of the local cached file.\n        url (str, optional): URL to download the file from if not cached or if force_download is True.\n        force_download (bool, optional): If True, download the file even if it exists locally.\n        progress_observer (Optional[ProgressObserver]): Optional observer to report download progress.\n        progress_callback (Optional[Callable[[int], None]]): [LEGACY, Deprecated] Optional callback to\n            report progress as an integer percent (0-100, or -1 if indeterminate). Use progress_observer\n            instead. Will be removed in a future release.\n        *args: Additional positional arguments for format-specific loading.\n        **kwargs: Additional keyword arguments for format-specific loading.\n\n    Returns:\n        Any: The parsed document object (type depends on subclass).\n\n    \"\"\"\n    # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n    progress_observer = handle_legacy_callback(progress_observer, progress_callback)\n    # END LEGACY SUPPORT\n    raise NotImplementedError(\"Subclasses must implement load_document()\")\n</code></pre>"},{"location":"api/dom_table_spec_parser/","title":"DOMTableSpecParser","text":""},{"location":"api/dom_table_spec_parser/#dcmspec.dom_table_spec_parser.DOMTableSpecParser","title":"<code>dcmspec.dom_table_spec_parser.DOMTableSpecParser</code>","text":"<p>               Bases: <code>SpecParser</code></p> <p>Parser for DICOM specification tables in XHTML DOM format.</p> <p>Provides methods to extract, parse, and structure DICOM specification tables from XHTML documents, returning anytree Node objects as structured in-memory representations. Inherits logging from SpecParser.</p> Source code in <code>src/dcmspec/dom_table_spec_parser.py</code> <pre><code>class DOMTableSpecParser(SpecParser):\n    \"\"\"Parser for DICOM specification tables in XHTML DOM format.\n\n    Provides methods to extract, parse, and structure DICOM specification tables from XHTML documents,\n    returning anytree Node objects as structured in-memory representations.\n    Inherits logging from SpecParser.\n    \"\"\"\n\n    def __init__(self, logger: Optional[Any] = None):\n        \"\"\"Initialize the DOMTableSpecParser.\n\n        Sets up the parser with an optional logger and a DOMUtils instance for DOM navigation.\n\n        Args:\n            logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n        \"\"\"\n        super().__init__(logger=logger)\n\n        self.dom_utils = DOMUtils(logger=self.logger)\n\n    def parse(\n        self,\n        dom: BeautifulSoup,\n        table_id: str,\n        column_to_attr: Dict[int, str],\n        name_attr: str,\n        include_depth: Optional[int] = None,  # None means unlimited\n        progress_observer: Optional[Any] = None,\n        skip_columns: Optional[list[int]] = None,\n        unformatted: Optional[Union[bool, Dict[int, bool]]] = True,\n    ) -&gt; tuple[Node, Node]:\n        \"\"\"Parse specification metadata and content from tables in the DOM.\n\n        Parses tables within the DOM of a DICOM document and returns a tuple containing\n        the metadata node and the table content node as structured in-memory representations.\n\n        Args:\n            dom (BeautifulSoup): The parsed XHTML DOM object.\n            table_id (str): The ID of the table to parse.\n            column_to_attr (Dict[int, str]): Mapping from column indices to attribute names for tree nodes.\n            name_attr (str): The attribute name to use for building node names.\n            include_depth (Optional[int], optional): The depth to which included tables should be parsed. \n                None means unlimited.\n            progress_observer (Optional[object], optional): Optional observer to report download progress.\n            skip_columns (Optional[list[int]]): List of column indices to skip if the row is missing a column.\n                This argument is typically set via `parser_kwargs` when using SpecFactory.\n            unformatted (Optional[Union[bool, Dict[int, bool]]]): \n                Whether to extract unformatted (plain text) cell content (default True).\n                Can be a bool (applies to all columns) or a dict mapping column indices to bools.\n                This argument is typically set via `parser_kwargs` when using SpecFactory.\n\n        Returns:\n            Tuple[Node, Node]: The metadata node and the table content node.\n\n        \"\"\"\n        self._skipped_columns_flag = False\n\n        # Build a list of booleans indicating, for each column, whether to extract its cells as unformatted text.\n        # Default is True (extract as unformatted text) for all columns.\n        num_columns = max(column_to_attr.keys()) + 1\n        if isinstance(unformatted, dict):\n            unformatted_list = [unformatted.get(i, True) for i in range(num_columns)]\n        else:\n            unformatted_list = [unformatted] * num_columns\n\n        content = self.parse_table(\n            dom, \n            table_id, \n            column_to_attr, \n            name_attr, \n            include_depth=include_depth, \n            progress_observer=progress_observer,\n            skip_columns=skip_columns, \n            unformatted_list=unformatted_list\n        )\n\n        # If we ever skipped columns, remove them from metadata.column_to_attr and realign keys\n        if skip_columns and self._skipped_columns_flag:\n            kept_items = [(k, v) for k, v in column_to_attr.items() if k not in skip_columns]\n            filtered_column_to_attr = {i: v for i, (k, v) in enumerate(kept_items)}\n        else:\n            filtered_column_to_attr = column_to_attr\n\n        metadata = self.parse_metadata(dom, table_id, filtered_column_to_attr)\n        metadata.column_to_attr = filtered_column_to_attr\n        metadata.table_id = table_id\n        if include_depth is not None:\n            metadata.include_depth = int(include_depth)\n        return metadata, content\n\n    @contextmanager\n    def _visit_table(self, table_id: str, visited_tables: set) -&gt; Any:\n        \"\"\"Context manager to temporarily add a table_id to the visited_tables set during recursion.\n\n        Ensures that table_id is added to visited_tables when entering the context,\n        and always removed when exiting, even if an exception occurs.\n\n        Args:\n            table_id: The ID of the table being visited.\n            visited_tables: The set of table IDs currently being visited in the recursion stack.\n\n        \"\"\"\n        visited_tables.add(table_id)\n        try:\n            yield\n        finally:\n            visited_tables.remove(table_id)\n\n    def parse_table(\n        self,\n        dom: BeautifulSoup,\n        table_id: str,\n        column_to_attr: Dict[int, str],\n        name_attr: str,\n        table_nesting_level: int = 0,\n        include_depth: Optional[int] = None,  # None means unlimited\n        progress_observer: Optional[Any] = None,\n        skip_columns: Optional[list[int]] = None,\n        visited_tables: Optional[set] = None,\n        unformatted_list: Optional[list[bool]] = None,\n    ) -&gt; Node:\n        \"\"\"Parse specification content from tables within the DOM of a DICOM document.\n\n        This method extracts data from each row of the table, handles nested\n        tables indicated by \"Include\" links, and builds a tree-like structure\n        of the DICOM attributes which root node is assigned to the attribute\n        model.\n\n        Args:\n            dom: The BeautifulSoup DOM object.\n            table_id: The ID of the table to parse.\n            column_to_attr: Mapping between index of columns to parse and tree nodes attributes names\n            name_attr: tree node attribute name to use to build node name\n            table_nesting_level: The nesting level of the table (used for recursion call only).\n            include_depth: The depth to which included tables should be parsed.\n            progress_observer (Optional[object], optional): Optional observer to report download progress.\n            skip_columns (Optional[list[int]]): List of column indices to skip if the row is missing a column.\n            visited_tables (Optional[set]): Set of table IDs that have been visited to prevent infinite recursion.\n            unformatted_list (Optional[list[bool]]): List of booleans indicating whether to extract each column as \n                unformatted text.\n\n        Returns:\n            root: The root node of the tree representation of the specification table.\n\n        \"\"\"\n        self.logger.info(f\"Nesting Level: {table_nesting_level}, Parsing table with id {table_id}\")\n\n        if unformatted_list is None:\n            num_columns = max(column_to_attr.keys()) + 1\n            unformatted_list = [True] * num_columns\n\n        self._enforce_unformatted_for_name_attr(column_to_attr, name_attr, unformatted_list)\n\n        # Initialize visited_tables set if not provided (first call)\n        if visited_tables is None:\n            visited_tables = set()\n\n        # Use a context manager to ensure table_id is always added to and removed from\n        # visited_tables, even if an exception occurs.\n        with self._visit_table(table_id, visited_tables):\n            # Maps column indices in the DICOM standard table to corresponding node attribute names\n            # for constructing a tree-like representation of the table's data.\n            # self.column_to_attr = {**{0: \"elem_name\", 1: \"elem_tag\"}, **(column_to_attr or {})}\n\n            table = self.dom_utils.get_table(dom, table_id)\n            if not table:\n                raise ValueError(f\"Table with id '{table_id}' not found.\")\n\n            if not column_to_attr:\n                raise ValueError(\"Columns to node attributes missing.\")\n            else:\n                self.column_to_attr = column_to_attr\n\n            root = Node(\"content\")\n            level_nodes: Dict[int, Node] = {0: root}\n\n\n            self._process_table_rows(\n                table=table,\n                dom=dom,\n                column_to_attr=column_to_attr,\n                name_attr=name_attr,\n                table_nesting_level=table_nesting_level,\n                include_depth=include_depth,\n                skip_columns=skip_columns,\n                visited_tables=visited_tables,\n                unformatted_list=unformatted_list,\n                level_nodes=level_nodes,\n                root=root,\n                progress_observer=progress_observer if table_nesting_level == 0 else None,\n            )\n\n            self.logger.info(f\"Nesting Level: {table_nesting_level}, Table parsed successfully\")\n\n            return root\n\n    def parse_metadata(\n        self,\n        dom: BeautifulSoup,\n        table_id: str,\n        column_to_attr: Dict[int, str],\n    ) -&gt; Node:\n        \"\"\"Parse specification metadata from the document and the table within the DOM of a DICOM document.\n\n        This method extracts the version of the DICOM standard and the headers of the tables.\n\n        Args:\n            dom: The BeautifulSoup DOM object.\n            table_id: The ID of the table to parse.\n            column_to_attr: Mapping between index of columns to parse and attributes name.\n\n        Returns:\n            metadata_node: The root node of the tree representation of the specification metadata.\n\n        \"\"\"\n        table = self.dom_utils.get_table(dom, table_id)\n        if not table:\n            raise ValueError(f\"Table with id '{table_id}' not found.\")\n\n        metadata = Node(\"metadata\")\n        # Parse the DICOM Standard document information\n        version = self.get_version(dom, table_id)\n        metadata.version = version\n        # Parse the Attribute table header\n        header = self._extract_header(table, column_to_attr=column_to_attr)\n        metadata.header = header\n\n        return metadata\n\n    def get_version(self, dom: BeautifulSoup, table_id: str) -&gt; str:\n        \"\"\"Retrieve the DICOM Standard version from the DOM.\n\n        Args:\n            dom: The BeautifulSoup DOM object.\n            table_id: The ID of the table to retrieve.\n\n        Returns:\n            info_node: The info tree node.\n\n        \"\"\"\n        version = self._version_from_book(dom) or self._version_from_section(dom)\n        if not version:\n            version = \"\"\n            self.logger.warning(\"DICOM Standard version not found\")\n        return version\n\n    def _version_from_book(self, dom: BeautifulSoup) -&gt; Optional[str]:\n        \"\"\"Extract version of DICOM books in HTML format.\"\"\"\n        titlepage = dom.find(\"div\", class_=\"titlepage\")\n        if titlepage:\n            subtitle = titlepage.find(\"h2\", class_=\"subtitle\")\n        return subtitle.text.split()[2] if subtitle else None\n\n    def _version_from_section(self, dom: BeautifulSoup) -&gt; Optional[str]:\n        \"\"\"Extract version of DICOM sections in the CHTML format.\"\"\"\n        document_release = dom.find(\"span\", class_=\"documentreleaseinformation\")\n        return document_release.text.split()[2] if document_release else None\n\n    def _process_table_rows(\n        self,\n        table: Tag,\n        dom: BeautifulSoup,\n        column_to_attr: Dict[int, str],\n        name_attr: str,\n        table_nesting_level: int,\n        include_depth: Optional[int],\n        skip_columns: Optional[list[int]],\n        visited_tables: set,\n        unformatted_list: list[bool],\n        level_nodes: Dict[int, Node],\n        root: Node,\n        progress_observer: Optional[Any] = None\n    ) -&gt; None:\n        \"\"\"Process all rows in the table, handling recursion, nesting, and node creation.\"\"\"\n        rows = table.find_all(\"tr\")[1:]\n        total_rows = len(rows)\n        for idx, row in enumerate(rows):\n            row_data = self._extract_row_data(row, skip_columns=skip_columns, unformatted_list=unformatted_list)\n            if row_data[name_attr] is None:\n                continue  # Skip empty rows\n            row_nesting_level = table_nesting_level + row_data[name_attr].count(\"&gt;\")\n\n            # Add nesting level symbols to included table element names except if row is a title\n            if table_nesting_level &gt; 0 and not row_data[name_attr].isupper():\n                row_data[name_attr] = \"&gt;\" * table_nesting_level + row_data[name_attr]\n\n            # Process Include statement unless include_depth is defined and not reached\n            if \"Include\" in row_data[name_attr] and (include_depth is None or include_depth &gt; 0):\n                next_depth = None if include_depth is None else include_depth - 1\n\n                should_include = self._check_circular_reference(row, visited_tables, table_nesting_level)\n                if should_include:\n                    self._parse_included_table(\n                        dom, row, column_to_attr, name_attr, row_nesting_level, next_depth,\n                        level_nodes, root, visited_tables, unformatted_list\n                    )\n                else:\n                    # Create a node to represent the circular reference instead of recursing\n                    node_name = self._sanitize_string(row_data[name_attr])\n                    self._create_node(node_name, row_data, row_nesting_level, level_nodes, root)\n            else:\n                node_name = self._sanitize_string(row_data[name_attr])\n                self._create_node(node_name, row_data, row_nesting_level, level_nodes, root)\n            # Only report progress for the root table\n            if progress_observer is not None:\n                percent = calculate_percent(idx + 1, total_rows)\n                progress_observer(Progress(\n                    percent,\n                    status=ProgressStatus.PARSING_TABLE,\n                ))\n\n    def _extract_row_data(\n        self,\n        row: Tag,\n        skip_columns: Optional[list[int]] = None,\n        unformatted_list: Optional[list[bool]] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Extract data from a table row.\n\n        Processes each cell in the row, accounting for colspans and rowspans and extract formatted (HTML)\n        or unformatted value from paragraphs within the cells.\n        Constructs a dictionary containing the extracted values for each logical column requested by the parser\n        (each column defined in `self.column_to_attr`).\n\n        If, after accounting for colspans and rowspans, the row has one fewer value than the number of logical columns\n        in the mapping and if skip_columns is set, those columns will be skipped for this row, allowing for robust\n        alignment when Module Tables and nested Attributes Tables may not have the same number of columns as it may be\n        for normalized IOD Modules.\n\n        Args:\n            row: The table row element (BeautifulSoup Tag for &lt;tr&gt; element).\n            skip_columns (Optional[list[int]]): List of column indices to skip if the row is missing a logical column.\n            unformatted_list (Optional[list[bool]]): List of booleans indicating whether to extract each column value as\n                unformatted (HTML) or formatted (ASCII) data.\n\n        Returns:\n            Dict[str, Any]: A dictionary mapping attribute names to cell values of the logical columns for the row.\n\n            - The **key** is the attribute name as defined in `self.column_to_attr` \n                (e.g., \"ie\", \"module\", \"ref\", \"usage\").\n            - The **value** is the cell value for that column in this row, which may be:\n                - The value physically present in the current row,\n                - Or a value carried over from a previous row due to rowspan.\n\n        \"\"\"\n        # Initialize rowspan trackers if not present\n        if not hasattr(self, \"_rowspan_trackers\") or self._rowspan_trackers is None:\n            self._rowspan_trackers = []\n\n        num_logical_columns = len(self.column_to_attr)  # Number of logical columns, hence expected number of cells\n        logical_cells = []  # List to hold the logical cell values\n        logical_col_idx = 0  # Logical column index in the table, index of the attribute in column_to_attr, 0-based\n        physical_col_idx = 0  # Physical column index in the DOM, index of the &lt;td&gt; cell in the &lt;tr&gt;, 0-based\n\n        # Iterator for the &lt;td&gt; elements in the current row\n        cell_iter = iter(row.find_all(\"td\"))\n        num_physical_cells = len(row.find_all(\"td\"))\n\n        # Only apply skip_columns if the row is missing exactly that many columns\n        apply_skip = (\n            skip_columns\n            and num_physical_cells == num_logical_columns - len(skip_columns)\n        )\n\n        # 1. Handle carried-forward cells from rowspans\n        logical_cells, logical_col_idx, physical_col_idx = self._handle_rowspan_cells(\n            logical_cells, logical_col_idx, physical_col_idx, num_logical_columns\n        )\n\n        # 2. Process each logical column in the row, extracting values from physical &lt;td&gt; cells\n        while logical_col_idx &lt; num_logical_columns:\n            # Skip this logical column if requested and missing in the row\n            if apply_skip and logical_col_idx in skip_columns:\n                logical_col_idx += 1\n                continue\n\n            logical_cells, logical_col_idx, physical_col_idx = self._process_logical_column(\n                cell_iter, logical_cells, logical_col_idx, physical_col_idx, skip_columns, unformatted_list\n            )\n\n        # 3. Trim _rowspan_trackers to match the number of physical columns in this row\n        if len(self._rowspan_trackers) &gt; physical_col_idx:\n            self._rowspan_trackers = self._rowspan_trackers[:physical_col_idx]\n\n        # 4. Map logical cells to attributes, omitting skipped columns if missing in the row\n        attr_indices = list(self.column_to_attr.keys())\n        if skip_columns and len(logical_cells) == len(self.column_to_attr) - len(skip_columns):\n            return self._map_cells_with_skipped_columns(\n                logical_cells, attr_indices, skip_columns\n            )\n        else:\n            return self._map_cells_to_attributes(logical_cells, attr_indices)\n\n\n    def _handle_rowspan_cells(\n        self,\n        logical_cells: list,\n        logical_col_idx: int,\n        physical_col_idx: int,\n        num_logical_columns: int\n    ) -&gt; tuple[list, int, int]:\n        \"\"\"Handle carried-forward cells from rowspans for the current row.\n\n        For each logical column, if a rowspan tracker is active for the current physical column,\n        use its carried-forward value for this logical column.\n        Advances logical and physical indices as needed.\n\n        Returns:\n            tuple: (logical_cells, logical_col_idx, physical_col_idx)\n                - logical_cells: The updated list of extracted cell values for the row.\n                - logical_col_idx: The next logical column index to process.\n                - physical_col_idx: The next physical column index to process.\n\n        \"\"\"\n        while (\n            physical_col_idx &lt; len(self._rowspan_trackers)\n            and logical_col_idx &lt; num_logical_columns\n            and self._rowspan_trackers[physical_col_idx]\n            and self._rowspan_trackers[physical_col_idx][\"rows_left\"] &gt; 0\n        ):\n            # Use carried-forward value for this logical column\n            value = self._rowspan_trackers[physical_col_idx][\"value\"]\n            logical_cells.append(value)\n            self._rowspan_trackers[physical_col_idx][\"rows_left\"] -= 1\n\n        # Advance to next logical column and past all physical columns spanned by the carried-forward cell\n            physical_col_idx += self._rowspan_trackers[physical_col_idx][\"colspan\"]\n            logical_col_idx += 1\n\n        return logical_cells, logical_col_idx, physical_col_idx\n\n\n    def _process_logical_column(\n        self,\n        cell_iter: Any,\n        logical_cells: list,\n        logical_col_idx: int,\n        physical_col_idx: int,\n        skip_columns: Optional[list[int]],\n        unformatted_list: Optional[list[bool]]\n    ) -&gt; tuple[list, int, int]:\n        \"\"\"Process a single logical column in the row.\n\n        Extract the value from the corresponding physical &lt;td&gt; cell (if present),\n        handle colspans and rowspans, and update logical and physical indices.\n\n        Returns:\n            tuple: (logical_cells, logical_col_idx, physical_col_idx)\n                - logical_cells: The updated list of extracted cell values for the row.\n                - logical_col_idx: The next logical column index to process.\n                - physical_col_idx: The next physical column index to process.\n\n        \"\"\"\n        # Ensure _rowspan_trackers has an entry for this physical column\n        if physical_col_idx &gt;= len(self._rowspan_trackers):\n            self._rowspan_trackers.append(None)\n\n        # Ensure logical_cells has an entry for this logical column (fill with None if missing in DOM)\n        try:\n            cell = next(cell_iter)\n        except StopIteration:\n            logical_cells.append(None)\n            logical_col_idx += 1\n            return logical_cells, logical_col_idx, physical_col_idx\n\n        # Extract value for the current logical column using the specified unformatted setting\n        value = self._extract_cell_value(cell, logical_col_idx, unformatted_list)\n\n        # Determine colspan and rowspan\n        colspan = int(cell.get(\"colspan\", 1))\n        rowspan = int(cell.get(\"rowspan\", 1))\n\n        # Add the value for the first logical column spanned by this cell\n        logical_cells.append(value)\n        # Add None for each additional logical column spanned by colspan, unless skipped\n        logical_cells.extend(\n            None\n            for j in range(1, colspan)\n            if not skip_columns or logical_col_idx + j not in skip_columns\n        )\n\n        # Update rowspan tracker for each physical column spanned by this cell\n        self._update_rowspan_trackers(physical_col_idx, colspan, rowspan, value)\n\n        # Advance logical and physical column indices by colspan\n        physical_col_idx += colspan\n        logical_col_idx += colspan\n\n        return logical_cells, logical_col_idx, physical_col_idx\n\n    def _extract_cell_value(\n        self,\n        cell: Tag,\n        logical_col_idx: int,\n        unformatted_list: list[bool]\n    ) -&gt; str:\n        \"\"\"Extract and clean the value from a cell as unformatted text or HTML.\"\"\"\n        use_unformatted = (\n            unformatted_list[logical_col_idx]\n            if unformatted_list and logical_col_idx &lt; len(unformatted_list)\n            else True\n        )\n        if use_unformatted:\n            return self._clean_extracted_text(cell.get_text(separator=\"\\n\", strip=True))\n        else:\n            return self._clean_extracted_text(cell.decode_contents())\n\n    def _update_rowspan_trackers(\n        self,\n        physical_col_idx: int,\n        colspan: int,\n        rowspan: int,\n        value: Any\n    ) -&gt; None:\n        \"\"\"Update the rowspan tracker for each physical column spanned by the cell.\"\"\"\n        for i in range(colspan):\n            while len(self._rowspan_trackers) &lt;= physical_col_idx + i:\n                self._rowspan_trackers.append(None)\n            if rowspan &gt; 1:\n                value_for_tracker = value if i == 0 else None\n                self._rowspan_trackers[physical_col_idx + i] = {\n                    \"value\": value_for_tracker,\n                    \"rows_left\": rowspan - 1,\n                    \"colspan\": 1,\n                }\n            else:\n                self._rowspan_trackers[physical_col_idx + i] = None\n\n    def _map_cells_with_skipped_columns(\n        self,\n        cells: list,\n        attr_indices: list[int],\n        skip_columns: list[int]\n    ) -&gt; dict:\n        \"\"\"Map the list of extracted cell values to the attribute names for this row in presence of skipped columns.\n\n        This method is used when skip_columns is set and the number of logical cells\n        matches the expected number of non-skipped columns. It ensures that only the\n        non-skipped attributes are present in the output dictionary.\n\n        Args:\n            cells (list): Extracted cell values for the row, in logical column order (excluding skipped columns).\n            attr_indices (list): Column indices (keys from column_to_attr) in logical order.\n            skip_columns (list): Column indices to skip.\n\n        Returns:\n            dict: Dictionary mapping attribute names to cell values (excluding skipped columns).\n\n        \"\"\"\n        attr_indices = [i for i in attr_indices if i not in skip_columns]\n\n        # Flag if the skipped_columns were actually skipped\n        self._skipped_columns_flag = True\n\n        # Map the remaining cells to the correct attributes\n        return {\n            self.column_to_attr[attr_indices[attr_index]]: cell\n            for attr_index, cell in enumerate(cells)\n            if attr_index &lt; len(attr_indices)\n        }\n\n    def _map_cells_to_attributes(\n        self,\n        cells: list,\n        attr_indices: list[int]\n    ) -&gt; dict:\n        \"\"\"Map the list of extracted cell values to the attribute names for this row.\n\n        This method builds a dictionary mapping each attribute name (from column_to_attr)\n        to the corresponding value in the `cells` list. If there are fewer cells than attributes,\n        the remaining attributes are filled with None.\n\n        Args:\n            cells (list): List of extracted cell values for the row, in logical column order.\n            attr_indices (list): List of column indices (keys from column_to_attr) in logical order.\n\n        Returns:\n            dict: Dictionary mapping attribute names to cell values (or None if missing).\n\n        \"\"\"\n        row_data = {}\n        attr_indices = sorted(attr_indices)\n        for i, attr_idx in enumerate(attr_indices):\n            attr = self.column_to_attr[attr_idx]\n            row_data[attr] = cells[i] if i &lt; len(cells) else None\n        return row_data\n\n    def _handle_pending_rowspans(self) -&gt; tuple[list, list, list, int, int]:\n        \"\"\"Handle cells that are carried forward from previous rows due to rowspan.\n\n        This method checks the internal _rowspan_trackers for any cells that are being\n        carried forward from previous rows (i.e., have rows_left &gt; 0). For each such cell,\n        it appends the carried-forward value to the current row's cell list, and updates\n        the physical and logical column indices accordingly.\n\n        Returns:\n            tuple: (cells, colspans, rowspans, physical_col_idx, logical_col_idx)\n                - cells: list of carried-forward cell values for this row\n                - colspans: list of colspans for each carried-forward cell\n                - rowspans: list of remaining rowspans for each carried-forward cell\n                - physical_col_idx: the next available physical column index in the row\n                - logical_col_idx: the next available logical column index in the row\n\n        Note:\n            - physical_col_idx tracks the actual position in the HTML table, including colspans.\n            - logical_col_idx tracks the logical data model column, incremented by 1 per cell.\n\n        \"\"\"\n        cells = []\n        colspans = []\n        rowspans = []\n        physical_col_idx = 0\n        logical_col_idx = 0\n\n        for tracker in self._rowspan_trackers:\n            if tracker and tracker[\"rows_left\"] &gt; 0:\n                cells.append(tracker[\"value\"])\n                colspans.append(tracker[\"colspan\"])\n                rowspans.append(tracker[\"rows_left\"])\n                tracker[\"rows_left\"] -= 1\n                physical_col_idx += tracker[\"colspan\"]\n                logical_col_idx += 1\n\n        return cells, colspans, rowspans, physical_col_idx, logical_col_idx\n\n    def _enforce_unformatted_for_name_attr(\n        self,\n        column_to_attr: dict[int, str],\n        name_attr: str,\n        unformatted_list: list[bool]\n    ) -&gt; None:\n        \"\"\"Enforce unformatted=True for the name_attr column if it is not already set.\"\"\"\n        name_attr_col = next((col_idx for col_idx, attr in column_to_attr.items() if attr == name_attr), None)\n        if name_attr_col is not None and not unformatted_list[name_attr_col]:\n            unformatted_list[name_attr_col] = True\n            if self.logger:\n                self.logger.warning(\n                    f\"unformatted=False for name_attr column '{name_attr}' (index {name_attr_col}) is not allowed. \"\n                    \"Forcing unformatted=True for this column to ensure correct parsing.\"\n                )\n\n    def _check_circular_reference(\n        self,\n        row: Tag,\n        visited_tables: set,\n        table_nesting_level: int\n    ) -&gt; bool:\n        \"\"\"Check for circular reference before attempting to parse an included table.\n\n        Returns:\n            bool: True if the table should be included (no circular reference), False otherwise.\n\n        \"\"\"\n        include_anchor = row.find(\"a\", {\"class\": \"xref\"})\n        if include_anchor:\n            include_table_id = include_anchor[\"href\"].split(\"#\", 1)[-1]\n            if include_table_id in visited_tables:\n                self.logger.warning(\n                    f\"Nesting Level: {table_nesting_level}, Circular reference detected for \"\n                    f\"table {include_table_id}, creating node instead of recursing\"\n                )\n                return False\n        return True\n\n    def _parse_included_table(\n        self,\n        dom: BeautifulSoup,\n        row: Tag,\n        column_to_attr: Dict[int, str],\n        name_attr: str,\n        table_nesting_level: int,\n        include_depth: int,\n        level_nodes: Dict[int, Node],\n        root: Node,\n        visited_tables: set,\n        unformatted_list: Optional[list[bool]] = None\n    ) -&gt; None:\n        \"\"\"Recursively parse Included Table.\"\"\"\n        include_anchor = row.find(\"a\", {\"class\": \"xref\"})\n        if not include_anchor:\n            self.logger.warning(f\"Nesting Level: {table_nesting_level}, Include Table Id not found\")\n            return\n\n        include_table_id = include_anchor[\"href\"].split(\"#\", 1)[-1]\n        self.logger.debug(f\"Nesting Level: {table_nesting_level}, Include Table Id: {include_table_id}\")\n\n        included_table_tree = self.parse_table(\n            dom,\n            include_table_id,\n            column_to_attr=column_to_attr,\n            name_attr=name_attr,\n            table_nesting_level=table_nesting_level,\n            include_depth=include_depth,\n            visited_tables=visited_tables,\n            unformatted_list=unformatted_list\n        )\n        if not included_table_tree:\n            return\n\n        self._nest_included_table(included_table_tree, level_nodes, table_nesting_level, root)\n\n    def _nest_included_table(\n        self,\n        included_table_tree: Node,\n        level_nodes: Dict[int, Node],\n        row_nesting_level: int,\n        root: Node\n    ) -&gt; None:\n        \"\"\"Nest the included table tree under the appropriate parent node.\"\"\"\n        parent_node = level_nodes.get(row_nesting_level - 1, root)\n        for child in included_table_tree.children:\n            child.parent = parent_node\n\n    def _create_node(\n        self,\n        node_name: str,\n        row_data: Dict[str, Any],\n        row_nesting_level: int,\n        level_nodes: Dict[int, Node],\n        root: Node\n    ) -&gt; None:\n        \"\"\"Create a new node and attach it to the appropriate parent.\"\"\"\n        parent_node = level_nodes.get(row_nesting_level - 1, root)\n        self.logger.debug(\n            f\"Nesting Level: {row_nesting_level}, Name: {node_name}, \"\n            f\"Parent: {parent_node.name if parent_node else 'None'}\"\n        )\n        node = Node(node_name, parent=parent_node, **row_data)\n        level_nodes[row_nesting_level] = node\n\n    def _extract_header(\n        self,\n        table: Tag,\n        column_to_attr: Dict[int, str]\n    ) -&gt; list[str]:\n        \"\"\"Extract headers from the table and saves them in the headers attribute.\n\n        Realign the keys in column_to_attr to consecutive indices if the number of columns in the table\n        is less than the maximum key in column_to_attr, to handle cases where the mapping is out of sync\n        with the actual table structure.\n\n        Args:\n            table: The table element from which to extract headers.\n            column_to_attr: Mapping between index of columns to parse and attributes name. \n\n        \"\"\"\n        cells = table.find_all(\"th\")\n        num_columns = len(cells)\n        # If the mapping has non-consecutive keys and the table has fewer columns, realign\n        if max(column_to_attr.keys()) &gt;= num_columns:\n            # Map consecutive indices to the same attribute names, skipping as needed\n            sorted_attrs = [column_to_attr[k] for k in sorted(column_to_attr.keys())]\n            realigned_col_to_attr = dict(enumerate(sorted_attrs))\n            column_to_attr = realigned_col_to_attr\n\n        header = []\n        header.extend(\n            cells[col_idx].get_text(strip=True)\n            for col_idx in column_to_attr\n            if col_idx &lt; len(cells)\n        )\n        self.logger.info(f\"Extracted Header: {header}\")\n        return header\n\n    def _clean_extracted_text(self, text: str) -&gt; str:\n        \"\"\"Clean extracted text using Unicode normalization and regex.\n\n        Args:\n            text (str): The text to be cleaned.\n\n        Returns:\n            str: The cleaned text.\n\n        \"\"\"\n        # Normalize unicode characters to compatibility form\n        cleaned = unicodedata.normalize('NFKC', text)\n\n        # Replace non-breaking spaces and zero-width spaces with regular space\n        cleaned = re.sub(r'[\\u00a0\\u200b]', ' ', cleaned)\n\n        # Replace typographic single quotes with ASCII single quote\n        cleaned = re.sub(r'[\\u2018\\u2019]', \"'\", cleaned)\n        # Replace typographic double quotes with ASCII double quote\n        cleaned = re.sub(r'[\\u201c\\u201d\\u00e2\\u0080\\u009c\\u00e2\\u0080\\u009d]', '\"', cleaned)\n        # Replace em dash and en dash with hyphen\n        cleaned = re.sub(r'[\\u2013\\u2014]', '-', cleaned)\n        # Remove stray \u00c2 character\n        cleaned = cleaned.replace('\\u00c2', '')\n\n        return cleaned.strip()\n\n    def _sanitize_string(self, input_string: str) -&gt; str:\n        \"\"\"Sanitize string to use it as a node attribute name.\n\n        - Convert non-ASCII characters to closest ASCII equivalents\n        - Replace space characters with underscores\n        - Replace parentheses characters with dashes\n\n        Args:\n            input_string (str): The string to be sanitized.\n\n        Returns:\n            str: The sanitized string.\n\n        \"\"\"\n        # Normalize the string to NFC form and transliterate to ASCII\n        normalized_str = unidecode(input_string.lower())\n        return re.sub(\n            r\"[ \\-()']\",\n            lambda match: \"-\" if match.group(0) in \"()\" else \"_\",\n            normalized_str,\n        )\n</code></pre>"},{"location":"api/dom_table_spec_parser/#dcmspec.dom_table_spec_parser.DOMTableSpecParser.__init__","title":"<code>__init__(logger=None)</code>","text":"<p>Initialize the DOMTableSpecParser.</p> <p>Sets up the parser with an optional logger and a DOMUtils instance for DOM navigation.</p> PARAMETER DESCRIPTION <code>logger</code> <p>Logger instance to use. If None, a default logger is created.</p> <p> TYPE: <code>Optional[Logger]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/dcmspec/dom_table_spec_parser.py</code> <pre><code>def __init__(self, logger: Optional[Any] = None):\n    \"\"\"Initialize the DOMTableSpecParser.\n\n    Sets up the parser with an optional logger and a DOMUtils instance for DOM navigation.\n\n    Args:\n        logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n    \"\"\"\n    super().__init__(logger=logger)\n\n    self.dom_utils = DOMUtils(logger=self.logger)\n</code></pre>"},{"location":"api/dom_table_spec_parser/#dcmspec.dom_table_spec_parser.DOMTableSpecParser.get_version","title":"<code>get_version(dom, table_id)</code>","text":"<p>Retrieve the DICOM Standard version from the DOM.</p> PARAMETER DESCRIPTION <code>dom</code> <p>The BeautifulSoup DOM object.</p> <p> TYPE: <code>BeautifulSoup</code> </p> <code>table_id</code> <p>The ID of the table to retrieve.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>info_node</code> <p>The info tree node.</p> <p> TYPE: <code>str</code> </p> Source code in <code>src/dcmspec/dom_table_spec_parser.py</code> <pre><code>def get_version(self, dom: BeautifulSoup, table_id: str) -&gt; str:\n    \"\"\"Retrieve the DICOM Standard version from the DOM.\n\n    Args:\n        dom: The BeautifulSoup DOM object.\n        table_id: The ID of the table to retrieve.\n\n    Returns:\n        info_node: The info tree node.\n\n    \"\"\"\n    version = self._version_from_book(dom) or self._version_from_section(dom)\n    if not version:\n        version = \"\"\n        self.logger.warning(\"DICOM Standard version not found\")\n    return version\n</code></pre>"},{"location":"api/dom_table_spec_parser/#dcmspec.dom_table_spec_parser.DOMTableSpecParser.parse","title":"<code>parse(dom, table_id, column_to_attr, name_attr, include_depth=None, progress_observer=None, skip_columns=None, unformatted=True)</code>","text":"<p>Parse specification metadata and content from tables in the DOM.</p> <p>Parses tables within the DOM of a DICOM document and returns a tuple containing the metadata node and the table content node as structured in-memory representations.</p> PARAMETER DESCRIPTION <code>dom</code> <p>The parsed XHTML DOM object.</p> <p> TYPE: <code>BeautifulSoup</code> </p> <code>table_id</code> <p>The ID of the table to parse.</p> <p> TYPE: <code>str</code> </p> <code>column_to_attr</code> <p>Mapping from column indices to attribute names for tree nodes.</p> <p> TYPE: <code>Dict[int, str]</code> </p> <code>name_attr</code> <p>The attribute name to use for building node names.</p> <p> TYPE: <code>str</code> </p> <code>include_depth</code> <p>The depth to which included tables should be parsed.  None means unlimited.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>progress_observer</code> <p>Optional observer to report download progress.</p> <p> TYPE: <code>Optional[object]</code> DEFAULT: <code>None</code> </p> <code>skip_columns</code> <p>List of column indices to skip if the row is missing a column. This argument is typically set via <code>parser_kwargs</code> when using SpecFactory.</p> <p> TYPE: <code>Optional[list[int]]</code> DEFAULT: <code>None</code> </p> <code>unformatted</code> <p>Whether to extract unformatted (plain text) cell content (default True). Can be a bool (applies to all columns) or a dict mapping column indices to bools. This argument is typically set via <code>parser_kwargs</code> when using SpecFactory.</p> <p> TYPE: <code>Optional[Union[bool, Dict[int, bool]]]</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>tuple[Node, Node]</code> <p>Tuple[Node, Node]: The metadata node and the table content node.</p> Source code in <code>src/dcmspec/dom_table_spec_parser.py</code> <pre><code>def parse(\n    self,\n    dom: BeautifulSoup,\n    table_id: str,\n    column_to_attr: Dict[int, str],\n    name_attr: str,\n    include_depth: Optional[int] = None,  # None means unlimited\n    progress_observer: Optional[Any] = None,\n    skip_columns: Optional[list[int]] = None,\n    unformatted: Optional[Union[bool, Dict[int, bool]]] = True,\n) -&gt; tuple[Node, Node]:\n    \"\"\"Parse specification metadata and content from tables in the DOM.\n\n    Parses tables within the DOM of a DICOM document and returns a tuple containing\n    the metadata node and the table content node as structured in-memory representations.\n\n    Args:\n        dom (BeautifulSoup): The parsed XHTML DOM object.\n        table_id (str): The ID of the table to parse.\n        column_to_attr (Dict[int, str]): Mapping from column indices to attribute names for tree nodes.\n        name_attr (str): The attribute name to use for building node names.\n        include_depth (Optional[int], optional): The depth to which included tables should be parsed. \n            None means unlimited.\n        progress_observer (Optional[object], optional): Optional observer to report download progress.\n        skip_columns (Optional[list[int]]): List of column indices to skip if the row is missing a column.\n            This argument is typically set via `parser_kwargs` when using SpecFactory.\n        unformatted (Optional[Union[bool, Dict[int, bool]]]): \n            Whether to extract unformatted (plain text) cell content (default True).\n            Can be a bool (applies to all columns) or a dict mapping column indices to bools.\n            This argument is typically set via `parser_kwargs` when using SpecFactory.\n\n    Returns:\n        Tuple[Node, Node]: The metadata node and the table content node.\n\n    \"\"\"\n    self._skipped_columns_flag = False\n\n    # Build a list of booleans indicating, for each column, whether to extract its cells as unformatted text.\n    # Default is True (extract as unformatted text) for all columns.\n    num_columns = max(column_to_attr.keys()) + 1\n    if isinstance(unformatted, dict):\n        unformatted_list = [unformatted.get(i, True) for i in range(num_columns)]\n    else:\n        unformatted_list = [unformatted] * num_columns\n\n    content = self.parse_table(\n        dom, \n        table_id, \n        column_to_attr, \n        name_attr, \n        include_depth=include_depth, \n        progress_observer=progress_observer,\n        skip_columns=skip_columns, \n        unformatted_list=unformatted_list\n    )\n\n    # If we ever skipped columns, remove them from metadata.column_to_attr and realign keys\n    if skip_columns and self._skipped_columns_flag:\n        kept_items = [(k, v) for k, v in column_to_attr.items() if k not in skip_columns]\n        filtered_column_to_attr = {i: v for i, (k, v) in enumerate(kept_items)}\n    else:\n        filtered_column_to_attr = column_to_attr\n\n    metadata = self.parse_metadata(dom, table_id, filtered_column_to_attr)\n    metadata.column_to_attr = filtered_column_to_attr\n    metadata.table_id = table_id\n    if include_depth is not None:\n        metadata.include_depth = int(include_depth)\n    return metadata, content\n</code></pre>"},{"location":"api/dom_table_spec_parser/#dcmspec.dom_table_spec_parser.DOMTableSpecParser.parse_metadata","title":"<code>parse_metadata(dom, table_id, column_to_attr)</code>","text":"<p>Parse specification metadata from the document and the table within the DOM of a DICOM document.</p> <p>This method extracts the version of the DICOM standard and the headers of the tables.</p> PARAMETER DESCRIPTION <code>dom</code> <p>The BeautifulSoup DOM object.</p> <p> TYPE: <code>BeautifulSoup</code> </p> <code>table_id</code> <p>The ID of the table to parse.</p> <p> TYPE: <code>str</code> </p> <code>column_to_attr</code> <p>Mapping between index of columns to parse and attributes name.</p> <p> TYPE: <code>Dict[int, str]</code> </p> RETURNS DESCRIPTION <code>metadata_node</code> <p>The root node of the tree representation of the specification metadata.</p> <p> TYPE: <code>Node</code> </p> Source code in <code>src/dcmspec/dom_table_spec_parser.py</code> <pre><code>def parse_metadata(\n    self,\n    dom: BeautifulSoup,\n    table_id: str,\n    column_to_attr: Dict[int, str],\n) -&gt; Node:\n    \"\"\"Parse specification metadata from the document and the table within the DOM of a DICOM document.\n\n    This method extracts the version of the DICOM standard and the headers of the tables.\n\n    Args:\n        dom: The BeautifulSoup DOM object.\n        table_id: The ID of the table to parse.\n        column_to_attr: Mapping between index of columns to parse and attributes name.\n\n    Returns:\n        metadata_node: The root node of the tree representation of the specification metadata.\n\n    \"\"\"\n    table = self.dom_utils.get_table(dom, table_id)\n    if not table:\n        raise ValueError(f\"Table with id '{table_id}' not found.\")\n\n    metadata = Node(\"metadata\")\n    # Parse the DICOM Standard document information\n    version = self.get_version(dom, table_id)\n    metadata.version = version\n    # Parse the Attribute table header\n    header = self._extract_header(table, column_to_attr=column_to_attr)\n    metadata.header = header\n\n    return metadata\n</code></pre>"},{"location":"api/dom_table_spec_parser/#dcmspec.dom_table_spec_parser.DOMTableSpecParser.parse_table","title":"<code>parse_table(dom, table_id, column_to_attr, name_attr, table_nesting_level=0, include_depth=None, progress_observer=None, skip_columns=None, visited_tables=None, unformatted_list=None)</code>","text":"<p>Parse specification content from tables within the DOM of a DICOM document.</p> <p>This method extracts data from each row of the table, handles nested tables indicated by \"Include\" links, and builds a tree-like structure of the DICOM attributes which root node is assigned to the attribute model.</p> PARAMETER DESCRIPTION <code>dom</code> <p>The BeautifulSoup DOM object.</p> <p> TYPE: <code>BeautifulSoup</code> </p> <code>table_id</code> <p>The ID of the table to parse.</p> <p> TYPE: <code>str</code> </p> <code>column_to_attr</code> <p>Mapping between index of columns to parse and tree nodes attributes names</p> <p> TYPE: <code>Dict[int, str]</code> </p> <code>name_attr</code> <p>tree node attribute name to use to build node name</p> <p> TYPE: <code>str</code> </p> <code>table_nesting_level</code> <p>The nesting level of the table (used for recursion call only).</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>include_depth</code> <p>The depth to which included tables should be parsed.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>progress_observer</code> <p>Optional observer to report download progress.</p> <p> TYPE: <code>Optional[object]</code> DEFAULT: <code>None</code> </p> <code>skip_columns</code> <p>List of column indices to skip if the row is missing a column.</p> <p> TYPE: <code>Optional[list[int]]</code> DEFAULT: <code>None</code> </p> <code>visited_tables</code> <p>Set of table IDs that have been visited to prevent infinite recursion.</p> <p> TYPE: <code>Optional[set]</code> DEFAULT: <code>None</code> </p> <code>unformatted_list</code> <p>List of booleans indicating whether to extract each column as  unformatted text.</p> <p> TYPE: <code>Optional[list[bool]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>root</code> <p>The root node of the tree representation of the specification table.</p> <p> TYPE: <code>Node</code> </p> Source code in <code>src/dcmspec/dom_table_spec_parser.py</code> <pre><code>def parse_table(\n    self,\n    dom: BeautifulSoup,\n    table_id: str,\n    column_to_attr: Dict[int, str],\n    name_attr: str,\n    table_nesting_level: int = 0,\n    include_depth: Optional[int] = None,  # None means unlimited\n    progress_observer: Optional[Any] = None,\n    skip_columns: Optional[list[int]] = None,\n    visited_tables: Optional[set] = None,\n    unformatted_list: Optional[list[bool]] = None,\n) -&gt; Node:\n    \"\"\"Parse specification content from tables within the DOM of a DICOM document.\n\n    This method extracts data from each row of the table, handles nested\n    tables indicated by \"Include\" links, and builds a tree-like structure\n    of the DICOM attributes which root node is assigned to the attribute\n    model.\n\n    Args:\n        dom: The BeautifulSoup DOM object.\n        table_id: The ID of the table to parse.\n        column_to_attr: Mapping between index of columns to parse and tree nodes attributes names\n        name_attr: tree node attribute name to use to build node name\n        table_nesting_level: The nesting level of the table (used for recursion call only).\n        include_depth: The depth to which included tables should be parsed.\n        progress_observer (Optional[object], optional): Optional observer to report download progress.\n        skip_columns (Optional[list[int]]): List of column indices to skip if the row is missing a column.\n        visited_tables (Optional[set]): Set of table IDs that have been visited to prevent infinite recursion.\n        unformatted_list (Optional[list[bool]]): List of booleans indicating whether to extract each column as \n            unformatted text.\n\n    Returns:\n        root: The root node of the tree representation of the specification table.\n\n    \"\"\"\n    self.logger.info(f\"Nesting Level: {table_nesting_level}, Parsing table with id {table_id}\")\n\n    if unformatted_list is None:\n        num_columns = max(column_to_attr.keys()) + 1\n        unformatted_list = [True] * num_columns\n\n    self._enforce_unformatted_for_name_attr(column_to_attr, name_attr, unformatted_list)\n\n    # Initialize visited_tables set if not provided (first call)\n    if visited_tables is None:\n        visited_tables = set()\n\n    # Use a context manager to ensure table_id is always added to and removed from\n    # visited_tables, even if an exception occurs.\n    with self._visit_table(table_id, visited_tables):\n        # Maps column indices in the DICOM standard table to corresponding node attribute names\n        # for constructing a tree-like representation of the table's data.\n        # self.column_to_attr = {**{0: \"elem_name\", 1: \"elem_tag\"}, **(column_to_attr or {})}\n\n        table = self.dom_utils.get_table(dom, table_id)\n        if not table:\n            raise ValueError(f\"Table with id '{table_id}' not found.\")\n\n        if not column_to_attr:\n            raise ValueError(\"Columns to node attributes missing.\")\n        else:\n            self.column_to_attr = column_to_attr\n\n        root = Node(\"content\")\n        level_nodes: Dict[int, Node] = {0: root}\n\n\n        self._process_table_rows(\n            table=table,\n            dom=dom,\n            column_to_attr=column_to_attr,\n            name_attr=name_attr,\n            table_nesting_level=table_nesting_level,\n            include_depth=include_depth,\n            skip_columns=skip_columns,\n            visited_tables=visited_tables,\n            unformatted_list=unformatted_list,\n            level_nodes=level_nodes,\n            root=root,\n            progress_observer=progress_observer if table_nesting_level == 0 else None,\n        )\n\n        self.logger.info(f\"Nesting Level: {table_nesting_level}, Table parsed successfully\")\n\n        return root\n</code></pre>"},{"location":"api/dom_utils/","title":"DOMUtils","text":""},{"location":"api/dom_utils/#dcmspec.dom_utils.DOMUtils","title":"<code>dcmspec.dom_utils.DOMUtils</code>","text":"<p>Utility class for DOM navigation and extraction in DICOM XHTML documents.</p> <p>Provides methods for locating tables and table IDs within a parsed BeautifulSoup DOM, with optional logging for warnings and debug information.</p> Typical usage <p>dom_utils = DOMUtils(logger=logger) table = dom_utils.get_table(dom, table_id) table_id = dom_utils.get_table_id_from_section(dom, section_id)</p> Source code in <code>src/dcmspec/dom_utils.py</code> <pre><code>class DOMUtils:\n    \"\"\"Utility class for DOM navigation and extraction in DICOM XHTML documents.\n\n    Provides methods for locating tables and table IDs within a parsed BeautifulSoup DOM,\n    with optional logging for warnings and debug information.\n\n    Typical usage:\n        dom_utils = DOMUtils(logger=logger)\n        table = dom_utils.get_table(dom, table_id)\n        table_id = dom_utils.get_table_id_from_section(dom, section_id)\n    \"\"\"\n\n    def __init__(self, logger: Optional[logging.Logger] = None):\n        \"\"\"Initialize DOMUtils with an optional logger.\n\n        Args:\n            logger (Optional[logging.Logger]): Logger instance to use for warnings and debug messages.\n                If None, a default logger is created.\n\n        \"\"\"\n        if logger is not None and not isinstance(logger, logging.Logger):\n            raise TypeError(\"logger must be an instance of logging.Logger or None\")\n        self.logger = logger or logging.getLogger(self.__class__.__name__)\n\n    def get_table(self, dom: BeautifulSoup, table_id: str) -&gt; Optional[Tag]:\n        \"\"\"Retrieve the table element with the specified ID from the DOM.\n\n        DocBook XML to XHTML conversion stylesheets enclose tables in a\n        &lt;div class=\"table\"&gt; with the table identifier in &lt;a id=\"table_ID\"&gt;&lt;/a&gt;\n\n        Searches for an anchor tag with the given ID and then finds the next\n        table element.\n\n        Args:\n            dom: The BeautifulSoup DOM object.\n            table_id: The ID of the table to retrieve.\n\n        Returns:\n            The table element if found, otherwise None.\n\n        \"\"\"\n        anchor = dom.find(\"a\", {\"id\": table_id})\n        if anchor is None:\n            self.logger.warning(f\"Table Id {table_id} not found.\")\n            return None\n        table_div = anchor.find_parent(\"div\", class_=\"table\")\n        if not table_div:\n            self.logger.warning(f\"Parent &lt;div class='table'&gt; for Table Id {table_id} not found.\")\n            return None\n        table = table_div.find(\"table\")\n        if not table:\n            self.logger.warning(f\"Table for Table Id {table_id} not found inside its &lt;div class='table'&gt;.\")\n            return None\n        return table\n\n    def get_table_id_from_section(self, dom: BeautifulSoup, section_id: str) -&gt; Optional[str]:\n        \"\"\"Get the id of the first table in a section.\n\n        Retrieve the first table_id (anchor id) of a &lt;div class=\"table\"&gt; inside a &lt;div class=\"section\"&gt;\n        that contains an &lt;a&gt; anchor with the given section id.\n\n        Args:\n            dom (BeautifulSoup): The parsed XHTML DOM object.\n            section_id (str): The id of the section to search for the table_id.\n\n        Returns:\n            Optional[str]: The id of the first table anchor found, or None if not found.\n\n        \"\"\"\n        # Find the anchor with the given id\n        anchor = dom.find(\"a\", {\"id\": section_id})\n        if not anchor:\n            self.logger.warning(f\"Section with id '{section_id}' not found.\")\n            return None\n\n        # Find the parent section div\n        section_div = anchor.find_parent(\"div\", class_=\"section\")\n        if not section_div:\n            self.logger.warning(f\"No parent &lt;div class='section'&gt; found for section id '{section_id}'.\")\n            return None\n\n        # Find the first &lt;div class=\"table\"&gt; inside the section\n        table_div = section_div.find(\"div\", class_=\"table\")\n        if not table_div:\n            self.logger.warning(f\"No &lt;div class='table'&gt; found in section for section id '{section_id}'.\")\n            return None\n\n        # Find the first anchor with an id inside the table div (the table id)\n        table_anchor = table_div.find(\"a\", id=True)\n        if table_anchor and table_anchor.get(\"id\"):\n            return table_anchor[\"id\"]\n\n        self.logger.warning(f\"No table id found in &lt;div class='table'&gt; for section id '{section_id}'.\")\n        return None\n</code></pre>"},{"location":"api/dom_utils/#dcmspec.dom_utils.DOMUtils.__init__","title":"<code>__init__(logger=None)</code>","text":"<p>Initialize DOMUtils with an optional logger.</p> PARAMETER DESCRIPTION <code>logger</code> <p>Logger instance to use for warnings and debug messages. If None, a default logger is created.</p> <p> TYPE: <code>Optional[Logger]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/dcmspec/dom_utils.py</code> <pre><code>def __init__(self, logger: Optional[logging.Logger] = None):\n    \"\"\"Initialize DOMUtils with an optional logger.\n\n    Args:\n        logger (Optional[logging.Logger]): Logger instance to use for warnings and debug messages.\n            If None, a default logger is created.\n\n    \"\"\"\n    if logger is not None and not isinstance(logger, logging.Logger):\n        raise TypeError(\"logger must be an instance of logging.Logger or None\")\n    self.logger = logger or logging.getLogger(self.__class__.__name__)\n</code></pre>"},{"location":"api/dom_utils/#dcmspec.dom_utils.DOMUtils.get_table","title":"<code>get_table(dom, table_id)</code>","text":"<p>Retrieve the table element with the specified ID from the DOM.</p> <p>DocBook XML to XHTML conversion stylesheets enclose tables in a</p>  with the table identifier in   Searches for an anchor tag with the given ID and then finds the next table element.    PARAMETER DESCRIPTION <code>dom</code> <p>The BeautifulSoup DOM object.</p> <p> TYPE: <code>BeautifulSoup</code> </p> <code>table_id</code> <p>The ID of the table to retrieve.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[Tag]</code> <p>The table element if found, otherwise None.</p> Source code in <code>src/dcmspec/dom_utils.py</code> <pre><code>def get_table(self, dom: BeautifulSoup, table_id: str) -&gt; Optional[Tag]:\n    \"\"\"Retrieve the table element with the specified ID from the DOM.\n\n    DocBook XML to XHTML conversion stylesheets enclose tables in a\n    &lt;div class=\"table\"&gt; with the table identifier in &lt;a id=\"table_ID\"&gt;&lt;/a&gt;\n\n    Searches for an anchor tag with the given ID and then finds the next\n    table element.\n\n    Args:\n        dom: The BeautifulSoup DOM object.\n        table_id: The ID of the table to retrieve.\n\n    Returns:\n        The table element if found, otherwise None.\n\n    \"\"\"\n    anchor = dom.find(\"a\", {\"id\": table_id})\n    if anchor is None:\n        self.logger.warning(f\"Table Id {table_id} not found.\")\n        return None\n    table_div = anchor.find_parent(\"div\", class_=\"table\")\n    if not table_div:\n        self.logger.warning(f\"Parent &lt;div class='table'&gt; for Table Id {table_id} not found.\")\n        return None\n    table = table_div.find(\"table\")\n    if not table:\n        self.logger.warning(f\"Table for Table Id {table_id} not found inside its &lt;div class='table'&gt;.\")\n        return None\n    return table\n</code></pre>"},{"location":"api/dom_utils/#dcmspec.dom_utils.DOMUtils.get_table_id_from_section","title":"<code>get_table_id_from_section(dom, section_id)</code>","text":"<p>Get the id of the first table in a section.</p> <p>Retrieve the first table_id (anchor id) of a  inside a  that contains an  anchor with the given section id. PARAMETER DESCRIPTION <code>dom</code> <p>The parsed XHTML DOM object.</p> <p> TYPE: <code>BeautifulSoup</code> </p> <code>section_id</code> <p>The id of the section to search for the table_id.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[str]</code> <p>Optional[str]: The id of the first table anchor found, or None if not found.</p> Source code in <code>src/dcmspec/dom_utils.py</code> <pre><code>def get_table_id_from_section(self, dom: BeautifulSoup, section_id: str) -&gt; Optional[str]:\n    \"\"\"Get the id of the first table in a section.\n\n    Retrieve the first table_id (anchor id) of a &lt;div class=\"table\"&gt; inside a &lt;div class=\"section\"&gt;\n    that contains an &lt;a&gt; anchor with the given section id.\n\n    Args:\n        dom (BeautifulSoup): The parsed XHTML DOM object.\n        section_id (str): The id of the section to search for the table_id.\n\n    Returns:\n        Optional[str]: The id of the first table anchor found, or None if not found.\n\n    \"\"\"\n    # Find the anchor with the given id\n    anchor = dom.find(\"a\", {\"id\": section_id})\n    if not anchor:\n        self.logger.warning(f\"Section with id '{section_id}' not found.\")\n        return None\n\n    # Find the parent section div\n    section_div = anchor.find_parent(\"div\", class_=\"section\")\n    if not section_div:\n        self.logger.warning(f\"No parent &lt;div class='section'&gt; found for section id '{section_id}'.\")\n        return None\n\n    # Find the first &lt;div class=\"table\"&gt; inside the section\n    table_div = section_div.find(\"div\", class_=\"table\")\n    if not table_div:\n        self.logger.warning(f\"No &lt;div class='table'&gt; found in section for section id '{section_id}'.\")\n        return None\n\n    # Find the first anchor with an id inside the table div (the table id)\n    table_anchor = table_div.find(\"a\", id=True)\n    if table_anchor and table_anchor.get(\"id\"):\n        return table_anchor[\"id\"]\n\n    self.logger.warning(f\"No table id found in &lt;div class='table'&gt; for section id '{section_id}'.\")\n    return None\n</code></pre>"},{"location":"api/iod_spec_builder/","title":"IODSpecBuilder","text":""},{"location":"api/iod_spec_builder/#dcmspec.iod_spec_builder.IODSpecBuilder","title":"<code>dcmspec.iod_spec_builder.IODSpecBuilder</code>","text":"<p>Orchestrates the construction of a expanded DICOM IOD specification model.</p> <p>The IODSpecBuilder uses a factory to build the IOD Modules model and, for each referenced module, uses a (possibly different) factory to build and cache the Module models. It then assembles a new model with the IOD nodes and their referenced module nodes as children, and caches the expanded model.</p> Source code in <code>src/dcmspec/iod_spec_builder.py</code> <pre><code>class IODSpecBuilder:\n    \"\"\"Orchestrates the construction of a expanded DICOM IOD specification model.\n\n    The IODSpecBuilder uses a factory to build the IOD Modules model and, for each referenced module,\n    uses a (possibly different) factory to build and cache the Module models. It then assembles a new\n    model with the IOD nodes and their referenced module nodes as children, and caches the expanded model.\n    \"\"\"\n\n    def __init__(\n        self,\n        iod_factory: SpecFactory = None,\n        module_factory: SpecFactory = None,\n        logger: logging.Logger = None,\n        ref_attr: str = None,\n    ):\n        \"\"\"Initialize the IODSpecBuilder.\n\n        If no factory is provided, a default SpecFactory is used for both IOD and module models.\n\n        Args:\n            iod_factory (Optional[SpecFactory]): Factory for building the IOD model. If None, uses SpecFactory().\n            module_factory (Optional[SpecFactory]): Factory for building module models. If None, uses iod_factory.\n            logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n            ref_attr (Optional[str]): Attribute name to use for module references. If None, defaults to \"ref\".\n\n        Raises:\n            ValueError: If `ref_attr` is not a non-empty string.\n\n        Note:\n            The builder is initialized with factories for the IOD and module models. By default, the same\n            factory is used for both, but a different factory can be provided for modules if needed.\n\n        \"\"\"\n        self.logger = logger or logging.getLogger(self.__class__.__name__)\n\n        self.iod_factory = iod_factory or SpecFactory(logger=self.logger)\n        self.module_factory = module_factory or self.iod_factory\n        self.dom_utils = DOMUtils(logger=self.logger)\n        self.ref_attr = ref_attr or \"ref\"\n        if not isinstance(self.ref_attr, str) or not self.ref_attr.strip():\n            raise ValueError(\"ref_attr must be a non-empty string.\")\n\n    def build_from_url(\n        self,\n        url: str,\n        cache_file_name: str,\n        table_id: str,\n        force_download: bool = False,\n        progress_observer: 'Optional[ProgressObserver]' = None,\n        # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n        progress_callback: 'Optional[Callable[[int], None]]' = None,\n        # END LEGACY SUPPORT\n        json_file_name: str = None,\n        **kwargs: object,\n    ) -&gt; SpecModel:\n        \"\"\"Build and cache a DICOM IOD specification model from a URL.\n\n        This method orchestrates the full workflow:\n        - Loads or downloads the IOD table and builds/caches the IOD model using the iod_factory.\n        - Finds all nodes in the IOD model with a \"ref\" attribute, indicating a referenced module.\n        - For each referenced module, loads or parses and caches the module model using the module_factory.\n        - Assembles a new expanded model, where each IOD node has its referenced module's content node as a child.\n        - Uses the first module's metadata header and version for the expanded model's metadata.\n        - Caches the expanded model if a json_file_name is provided.\n\n        Args:\n            url (str): The URL to download the input file from.\n            cache_file_name (str): Filename of the cached input file.\n            table_id (str): The ID of the IOD table to parse.\n            force_download (bool): If True, always download the input file and generate the model even if cached.\n            progress_observer (Optional[ProgressObserver]): Optional observer to report download progress.\n                See the Note below for details on the progress events and their properties.\n            progress_callback (Optional[Callable[[int], None]]): [LEGACY, Deprecated] Optional callback to\n                report progress as an integer percent (0-100, or -1 if indeterminate). Use progress_observer\n                instead. Will be removed in a future release.            \n            json_file_name (str, optional): Filename to save the cached expanded JSON model.\n            **kwargs: Additional arguments for model construction.\n\n        Returns:\n            SpecModel: The expanded model with IOD and module content.\n\n        Note:\n            If a progress observer accepting a Progress object is provided, progress events are as follows:\n\n            - **Step 1 (DOWNLOADING_IOD):** Events include `status=DOWNLOADING_IOD`, `step=1`,\n            `total_steps=4`, and a meaningful `percent` value.\n            - **Step 2 (PARSING_IOD_MODULE_LIST):** Events include `status=PARSING_IOD_MODULE_LIST`, `step=2`,\n            `total_steps=4`, and `percent == -1` (indeterminate).\n            - **Step 3 (PARSING_IOD_MODULES):** Events include `status=PARSING_IOD_MODULES`, `step=3`,\n                `total_steps=4`, and a meaningful `percent` value.\n            - **Step 4 (SAVING_IOD_MODEL):** Events include `status=SAVING_IOD_MODEL`, `step=4`,\n                `total_steps=4`, and `percent == -1` (indeterminate).\n\n            For example usage in a client application,\n            see [`ProgressStatus`](progress.md#dcmspec.progress.ProgressStatus).\n\n        \"\"\"\n        # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n        progress_observer = handle_legacy_callback(progress_observer, progress_callback)\n        # END LEGACY SUPPORT\n        # Load from cache if the expanded IOD model is already present\n        cached_model = self._load_expanded_model_from_cache(json_file_name, force_download)\n        if cached_model is not None:\n            cached_model.logger = self.logger\n            return cached_model\n\n        total_steps = 4  # 1=download, 2=parse IOD, 3=build modules, 4=save\n\n        # --- Step 1: Load the DOM from cache file or download and cache DOM in memory ---\n        if progress_observer:\n            @add_progress_step(step=1, total_steps=total_steps, status=ProgressStatus.DOWNLOADING_IOD)\n            def step1_progress_observer(progress):\n                progress_observer(progress)\n        else:\n            step1_progress_observer = None\n        dom = self.iod_factory.load_document(\n            url=url,\n            cache_file_name=cache_file_name,\n            force_download=force_download,\n            progress_observer=step1_progress_observer,\n            # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n            progress_callback=progress_observer,\n            # END LEGACY SUPPORT\n        )\n\n        # --- Step 2: Build the IOD Module List model from the DOM ---\n        if progress_observer:\n            progress_observer(\n                Progress(-1, status=ProgressStatus.PARSING_IOD_MODULE_LIST, step=2, total_steps=total_steps)\n                )\n        iodmodules_model = self.iod_factory.build_model(\n            doc_object=dom,\n            table_id=table_id,\n            url=url,\n            json_file_name=json_file_name,\n        )\n\n        # --- Step 3: Build or load model for each module in the IOD ---\n        if progress_observer:\n            progress_observer(\n                Progress(-1, status=ProgressStatus.PARSING_IOD_MODULES, step=3, total_steps=total_steps)\n            )\n\n        # Find all nodes with a reference attribute in the IOD Modules model\n        nodes_with_ref = [node for node in iodmodules_model.content.children if hasattr(node, self.ref_attr)]\n\n        # Build or load module models for each referenced section\n        module_models = self._build_module_models(\n            nodes_with_ref, dom, url, step=3, total_steps=total_steps, progress_observer=progress_observer\n        )\n        # Fail if no module models were found.\n        if not module_models:\n            raise RuntimeError(\"No module models were found for the referenced modules in the IOD table.\")\n\n        # --- Step 4: Create and store the expanded model with IOD and module content ---\n        if progress_observer:\n            progress_observer(Progress(-1, status=ProgressStatus.SAVING_IOD_MODEL, step=4, total_steps=total_steps))\n\n        # Create the expanded model from the IOD modules and module models\n        iod_model = self._create_expanded_model(iodmodules_model, module_models)\n\n        # Cache the expanded model if a json_file_name was provided\n        if json_file_name:\n            iod_json_file_path = os.path.join(\n                self.iod_factory.config.get_param(\"cache_dir\"), \"model\", json_file_name\n            )\n            try:\n                self.iod_factory.model_store.save(iod_model, iod_json_file_path)\n            except Exception as e:\n                self.logger.warning(f\"Failed to cache expanded model to {iod_json_file_path}: {e}\")\n        else:\n            self.logger.info(\"No json_file_name specified; IOD model not cached.\")\n\n        return iod_model\n\n    def _load_expanded_model_from_cache(self, json_file_name: str, force_download: bool) -&gt; SpecModel | None:\n        \"\"\"Return the cached expanded IOD model if available and not force_download, else None.\"\"\"\n        iod_json_file_path = None\n        if json_file_name:\n            iod_json_file_path = os.path.join(\n                self.iod_factory.config.get_param(\"cache_dir\"), \"model\", json_file_name\n            )\n        if iod_json_file_path and os.path.exists(iod_json_file_path) and not force_download:\n            try:\n                return self.iod_factory.model_store.load(iod_json_file_path)\n            except Exception as e:\n                self.logger.warning(\n                    f\"Failed to load expanded IOD model from cache {iod_json_file_path}: {e}\"\n                )\n        return None\n\n    def _build_module_models(\n        self,\n        nodes_with_ref: List[Any],\n        dom: Any,\n        url: str,\n        step: int,\n        total_steps: int,\n        progress_observer: Optional['ProgressObserver'] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Build or load module models for each referenced section, reporting progress.\"\"\"\n        module_models: Dict[str, Any] = {}\n        total_modules = len(nodes_with_ref)\n        if progress_observer and total_modules &gt; 0:\n            progress_observer(\n                Progress(0, status=ProgressStatus.PARSING_IOD_MODULES, step=step, total_steps=total_steps)\n                )\n        for idx, node in enumerate(nodes_with_ref):\n            ref_value = getattr(node, self.ref_attr, None)\n            section_id = self._get_section_id_from_ref(ref_value)\n            if not section_id:\n                continue\n\n            module_table_id = self.dom_utils.get_table_id_from_section(dom, section_id)\n            self.logger.debug(f\"First Module table_id for section_id={repr(section_id)}: {repr(module_table_id)}\")\n            if not module_table_id:\n                self.logger.warning(f\"No table found for section id {section_id}\")\n                continue\n\n            module_json_file_name = f\"{module_table_id}.json\"\n            module_json_file_path = os.path.join(\n                self.module_factory.config.get_param(\"cache_dir\"), \"model\", module_json_file_name\n            )\n            if os.path.exists(module_json_file_path):\n                try:\n                    module_model = self.module_factory.model_store.load(module_json_file_path)\n                except Exception as e:\n                    self.logger.warning(f\"Failed to load module model from cache {module_json_file_path}: {e}\")\n                    module_model = self.module_factory.build_model(\n                        doc_object=dom,\n                        table_id=module_table_id,\n                        url=url,\n                        json_file_name=module_json_file_name,\n                        progress_observer=progress_observer,\n                    )\n            else:\n                module_model = self.module_factory.build_model(\n                    doc_object=dom,\n                    table_id=module_table_id,\n                    url=url,\n                    json_file_name=module_json_file_name,\n                    progress_observer=progress_observer,\n                )\n            module_models[section_id] = module_model\n            if progress_observer and total_modules &gt; 0:\n                percent = calculate_percent(idx + 1, total_modules)\n                progress_observer(Progress(\n                    percent,\n                    status=ProgressStatus.PARSING_IOD_MODULES,\n                    step=step,\n                    total_steps=total_steps\n                ))\n        return module_models\n\n    def _get_section_id_from_ref(self, ref_value: str) -&gt; Optional[str]:\n        \"\"\"Normalize a ref_value (plain text or HTML anchor) to a section_id.\n\n        For HTML, extract the href after '#'. For plain text, always prepend 'sect_'.\n        Strips whitespace for robust lookup.\n        (Do NOT lowercase: DICOM IDs are mixed case and BeautifulSoup search is case-sensitive.)\n        \"\"\"\n        if not ref_value:\n            return None\n        if \"&lt;a \" not in ref_value:\n            # Always prepend 'sect_' for plain text references, strip only\n            section_id = f\"sect_{ref_value.strip()}\"\n            self.logger.debug(f\"Extracted section_id from plain text reference: {repr(section_id)}\")\n            return section_id\n        soup = BeautifulSoup(ref_value, \"lxml-xml\")\n        # Find the anchor with class \"xref\" (the actual module reference)\n        anchor = soup.find(\"a\", class_=\"xref\")\n        if anchor and anchor.has_attr(\"href\"):\n            href = anchor[\"href\"].strip()\n            section_id = href.split(\"#\", 1)[-1] if \"#\" in href else href\n            section_id = section_id.strip()\n            self.logger.debug(f\"Extracted section_id from HTML reference: {repr(section_id)}\")\n            return section_id\n        else:\n            self.logger.debug(f\"No section_id could be extracted from ref_value={repr(ref_value)}\")\n            return None\n\n    def _create_expanded_model(self, iodmodules_model: SpecModel, module_models: dict) -&gt; SpecModel:\n        \"\"\"Create the expanded model by attaching Module nodes content to IOD nodes.\"\"\"\n        # Use the first module's metadata node for the expanded model\n        first_module = next(iter(module_models.values()))\n        iod_metadata = first_module.metadata\n        iod_metadata.table_id = iodmodules_model.metadata.table_id\n\n        # The content node will have as children the IOD model's nodes,\n        # and for each referenced module, its content's children will be attached directly under the iod node\n        iod_content = Node(\"content\")\n        for iod_node in iodmodules_model.content.children:\n            ref_value = getattr(iod_node, self.ref_attr, None)\n            section_id = self._get_section_id_from_ref(ref_value)\n            if section_id and section_id in module_models:\n                module_content = module_models[section_id].content\n                for child in list(module_content.children):\n                    child.parent = iod_node\n            iod_node.parent = iod_content\n\n        # Create and return the expanded model\n        return SpecModel(metadata=iod_metadata, content=iod_content)\n</code></pre>"},{"location":"api/iod_spec_builder/#dcmspec.iod_spec_builder.IODSpecBuilder.__init__","title":"<code>__init__(iod_factory=None, module_factory=None, logger=None, ref_attr=None)</code>","text":"<p>Initialize the IODSpecBuilder.</p> <p>If no factory is provided, a default SpecFactory is used for both IOD and module models.</p> PARAMETER DESCRIPTION <code>iod_factory</code> <p>Factory for building the IOD model. If None, uses SpecFactory().</p> <p> TYPE: <code>Optional[SpecFactory]</code> DEFAULT: <code>None</code> </p> <code>module_factory</code> <p>Factory for building module models. If None, uses iod_factory.</p> <p> TYPE: <code>Optional[SpecFactory]</code> DEFAULT: <code>None</code> </p> <code>logger</code> <p>Logger instance to use. If None, a default logger is created.</p> <p> TYPE: <code>Optional[Logger]</code> DEFAULT: <code>None</code> </p> <code>ref_attr</code> <p>Attribute name to use for module references. If None, defaults to \"ref\".</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If <code>ref_attr</code> is not a non-empty string.</p> Note <p>The builder is initialized with factories for the IOD and module models. By default, the same factory is used for both, but a different factory can be provided for modules if needed.</p> Source code in <code>src/dcmspec/iod_spec_builder.py</code> <pre><code>def __init__(\n    self,\n    iod_factory: SpecFactory = None,\n    module_factory: SpecFactory = None,\n    logger: logging.Logger = None,\n    ref_attr: str = None,\n):\n    \"\"\"Initialize the IODSpecBuilder.\n\n    If no factory is provided, a default SpecFactory is used for both IOD and module models.\n\n    Args:\n        iod_factory (Optional[SpecFactory]): Factory for building the IOD model. If None, uses SpecFactory().\n        module_factory (Optional[SpecFactory]): Factory for building module models. If None, uses iod_factory.\n        logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n        ref_attr (Optional[str]): Attribute name to use for module references. If None, defaults to \"ref\".\n\n    Raises:\n        ValueError: If `ref_attr` is not a non-empty string.\n\n    Note:\n        The builder is initialized with factories for the IOD and module models. By default, the same\n        factory is used for both, but a different factory can be provided for modules if needed.\n\n    \"\"\"\n    self.logger = logger or logging.getLogger(self.__class__.__name__)\n\n    self.iod_factory = iod_factory or SpecFactory(logger=self.logger)\n    self.module_factory = module_factory or self.iod_factory\n    self.dom_utils = DOMUtils(logger=self.logger)\n    self.ref_attr = ref_attr or \"ref\"\n    if not isinstance(self.ref_attr, str) or not self.ref_attr.strip():\n        raise ValueError(\"ref_attr must be a non-empty string.\")\n</code></pre>"},{"location":"api/iod_spec_builder/#dcmspec.iod_spec_builder.IODSpecBuilder.build_from_url","title":"<code>build_from_url(url, cache_file_name, table_id, force_download=False, progress_observer=None, progress_callback=None, json_file_name=None, **kwargs)</code>","text":"<p>Build and cache a DICOM IOD specification model from a URL.</p> <p>This method orchestrates the full workflow: - Loads or downloads the IOD table and builds/caches the IOD model using the iod_factory. - Finds all nodes in the IOD model with a \"ref\" attribute, indicating a referenced module. - For each referenced module, loads or parses and caches the module model using the module_factory. - Assembles a new expanded model, where each IOD node has its referenced module's content node as a child. - Uses the first module's metadata header and version for the expanded model's metadata. - Caches the expanded model if a json_file_name is provided.</p> PARAMETER DESCRIPTION <code>url</code> <p>The URL to download the input file from.</p> <p> TYPE: <code>str</code> </p> <code>cache_file_name</code> <p>Filename of the cached input file.</p> <p> TYPE: <code>str</code> </p> <code>table_id</code> <p>The ID of the IOD table to parse.</p> <p> TYPE: <code>str</code> </p> <code>force_download</code> <p>If True, always download the input file and generate the model even if cached.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>progress_observer</code> <p>Optional observer to report download progress. See the Note below for details on the progress events and their properties.</p> <p> TYPE: <code>Optional[ProgressObserver]</code> DEFAULT: <code>None</code> </p> <code>progress_callback</code> <p>[LEGACY, Deprecated] Optional callback to report progress as an integer percent (0-100, or -1 if indeterminate). Use progress_observer instead. Will be removed in a future release.            </p> <p> TYPE: <code>Optional[Callable[[int], None]]</code> DEFAULT: <code>None</code> </p> <code>json_file_name</code> <p>Filename to save the cached expanded JSON model.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>Additional arguments for model construction.</p> <p> TYPE: <code>object</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>SpecModel</code> <p>The expanded model with IOD and module content.</p> <p> TYPE: <code>SpecModel</code> </p> Note <p>If a progress observer accepting a Progress object is provided, progress events are as follows:</p> <ul> <li>Step 1 (DOWNLOADING_IOD): Events include <code>status=DOWNLOADING_IOD</code>, <code>step=1</code>, <code>total_steps=4</code>, and a meaningful <code>percent</code> value.</li> <li>Step 2 (PARSING_IOD_MODULE_LIST): Events include <code>status=PARSING_IOD_MODULE_LIST</code>, <code>step=2</code>, <code>total_steps=4</code>, and <code>percent == -1</code> (indeterminate).</li> <li>Step 3 (PARSING_IOD_MODULES): Events include <code>status=PARSING_IOD_MODULES</code>, <code>step=3</code>,     <code>total_steps=4</code>, and a meaningful <code>percent</code> value.</li> <li>Step 4 (SAVING_IOD_MODEL): Events include <code>status=SAVING_IOD_MODEL</code>, <code>step=4</code>,     <code>total_steps=4</code>, and <code>percent == -1</code> (indeterminate).</li> </ul> <p>For example usage in a client application, see <code>ProgressStatus</code>.</p> Source code in <code>src/dcmspec/iod_spec_builder.py</code> <pre><code>def build_from_url(\n    self,\n    url: str,\n    cache_file_name: str,\n    table_id: str,\n    force_download: bool = False,\n    progress_observer: 'Optional[ProgressObserver]' = None,\n    # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n    progress_callback: 'Optional[Callable[[int], None]]' = None,\n    # END LEGACY SUPPORT\n    json_file_name: str = None,\n    **kwargs: object,\n) -&gt; SpecModel:\n    \"\"\"Build and cache a DICOM IOD specification model from a URL.\n\n    This method orchestrates the full workflow:\n    - Loads or downloads the IOD table and builds/caches the IOD model using the iod_factory.\n    - Finds all nodes in the IOD model with a \"ref\" attribute, indicating a referenced module.\n    - For each referenced module, loads or parses and caches the module model using the module_factory.\n    - Assembles a new expanded model, where each IOD node has its referenced module's content node as a child.\n    - Uses the first module's metadata header and version for the expanded model's metadata.\n    - Caches the expanded model if a json_file_name is provided.\n\n    Args:\n        url (str): The URL to download the input file from.\n        cache_file_name (str): Filename of the cached input file.\n        table_id (str): The ID of the IOD table to parse.\n        force_download (bool): If True, always download the input file and generate the model even if cached.\n        progress_observer (Optional[ProgressObserver]): Optional observer to report download progress.\n            See the Note below for details on the progress events and their properties.\n        progress_callback (Optional[Callable[[int], None]]): [LEGACY, Deprecated] Optional callback to\n            report progress as an integer percent (0-100, or -1 if indeterminate). Use progress_observer\n            instead. Will be removed in a future release.            \n        json_file_name (str, optional): Filename to save the cached expanded JSON model.\n        **kwargs: Additional arguments for model construction.\n\n    Returns:\n        SpecModel: The expanded model with IOD and module content.\n\n    Note:\n        If a progress observer accepting a Progress object is provided, progress events are as follows:\n\n        - **Step 1 (DOWNLOADING_IOD):** Events include `status=DOWNLOADING_IOD`, `step=1`,\n        `total_steps=4`, and a meaningful `percent` value.\n        - **Step 2 (PARSING_IOD_MODULE_LIST):** Events include `status=PARSING_IOD_MODULE_LIST`, `step=2`,\n        `total_steps=4`, and `percent == -1` (indeterminate).\n        - **Step 3 (PARSING_IOD_MODULES):** Events include `status=PARSING_IOD_MODULES`, `step=3`,\n            `total_steps=4`, and a meaningful `percent` value.\n        - **Step 4 (SAVING_IOD_MODEL):** Events include `status=SAVING_IOD_MODEL`, `step=4`,\n            `total_steps=4`, and `percent == -1` (indeterminate).\n\n        For example usage in a client application,\n        see [`ProgressStatus`](progress.md#dcmspec.progress.ProgressStatus).\n\n    \"\"\"\n    # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n    progress_observer = handle_legacy_callback(progress_observer, progress_callback)\n    # END LEGACY SUPPORT\n    # Load from cache if the expanded IOD model is already present\n    cached_model = self._load_expanded_model_from_cache(json_file_name, force_download)\n    if cached_model is not None:\n        cached_model.logger = self.logger\n        return cached_model\n\n    total_steps = 4  # 1=download, 2=parse IOD, 3=build modules, 4=save\n\n    # --- Step 1: Load the DOM from cache file or download and cache DOM in memory ---\n    if progress_observer:\n        @add_progress_step(step=1, total_steps=total_steps, status=ProgressStatus.DOWNLOADING_IOD)\n        def step1_progress_observer(progress):\n            progress_observer(progress)\n    else:\n        step1_progress_observer = None\n    dom = self.iod_factory.load_document(\n        url=url,\n        cache_file_name=cache_file_name,\n        force_download=force_download,\n        progress_observer=step1_progress_observer,\n        # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n        progress_callback=progress_observer,\n        # END LEGACY SUPPORT\n    )\n\n    # --- Step 2: Build the IOD Module List model from the DOM ---\n    if progress_observer:\n        progress_observer(\n            Progress(-1, status=ProgressStatus.PARSING_IOD_MODULE_LIST, step=2, total_steps=total_steps)\n            )\n    iodmodules_model = self.iod_factory.build_model(\n        doc_object=dom,\n        table_id=table_id,\n        url=url,\n        json_file_name=json_file_name,\n    )\n\n    # --- Step 3: Build or load model for each module in the IOD ---\n    if progress_observer:\n        progress_observer(\n            Progress(-1, status=ProgressStatus.PARSING_IOD_MODULES, step=3, total_steps=total_steps)\n        )\n\n    # Find all nodes with a reference attribute in the IOD Modules model\n    nodes_with_ref = [node for node in iodmodules_model.content.children if hasattr(node, self.ref_attr)]\n\n    # Build or load module models for each referenced section\n    module_models = self._build_module_models(\n        nodes_with_ref, dom, url, step=3, total_steps=total_steps, progress_observer=progress_observer\n    )\n    # Fail if no module models were found.\n    if not module_models:\n        raise RuntimeError(\"No module models were found for the referenced modules in the IOD table.\")\n\n    # --- Step 4: Create and store the expanded model with IOD and module content ---\n    if progress_observer:\n        progress_observer(Progress(-1, status=ProgressStatus.SAVING_IOD_MODEL, step=4, total_steps=total_steps))\n\n    # Create the expanded model from the IOD modules and module models\n    iod_model = self._create_expanded_model(iodmodules_model, module_models)\n\n    # Cache the expanded model if a json_file_name was provided\n    if json_file_name:\n        iod_json_file_path = os.path.join(\n            self.iod_factory.config.get_param(\"cache_dir\"), \"model\", json_file_name\n        )\n        try:\n            self.iod_factory.model_store.save(iod_model, iod_json_file_path)\n        except Exception as e:\n            self.logger.warning(f\"Failed to cache expanded model to {iod_json_file_path}: {e}\")\n    else:\n        self.logger.info(\"No json_file_name specified; IOD model not cached.\")\n\n    return iod_model\n</code></pre>"},{"location":"api/iod_spec_printer/","title":"IODSpecPrinter","text":""},{"location":"api/iod_spec_printer/#dcmspec.iod_spec_printer.IODSpecPrinter","title":"<code>dcmspec.iod_spec_printer.IODSpecPrinter</code>","text":"<p>               Bases: <code>SpecPrinter</code></p> <p>Printer for DICOM IOD specification models with mixed node types.</p> <p>Overrides print_table to display IOD Modules nodes as a one-cell title row (spanning all columns) The table columns are those of the Module Attributes nodes.</p> Source code in <code>src/dcmspec/iod_spec_printer.py</code> <pre><code>class IODSpecPrinter(SpecPrinter):\n    \"\"\"Printer for DICOM IOD specification models with mixed node types.\n\n    Overrides print_table to display IOD Modules nodes as a one-cell title row (spanning all columns)\n    The table columns are those of the Module Attributes nodes.\n    \"\"\"\n\n    def print_table(self, colorize: bool = False):\n        \"\"\"Print the specification model as a flat table with module title rows.\n\n        Args:\n            colorize (bool): Whether to colorize the output by node depth.\n\n        \"\"\"\n        table = Table(show_header=True, header_style=\"bold magenta\", show_lines=True, box=box.ASCII_DOUBLE_HEAD)\n\n        attr_headers = list(self.model.metadata.header)\n        for header in attr_headers:\n            table.add_column(header, width=20)\n\n        # Traverse the tree in PreOrder (as in the base class)\n        for node in PreOrderIter(self.model.content):\n            # skip the root node\n            if node.name == \"content\":\n                continue            # Print IOD module nodes as a title row (one cell, spanning all columns)\n            if hasattr(node, \"module\"):\n                iod_title = getattr(node, \"module\", getattr(node, \"name\", \"\"))\n                iod_usage = getattr(node, \"usage\", \"\")\n                iod_title_text = f\"{iod_title} Module ({iod_usage})\" if iod_usage else iod_title\n                # Set title style\n                row_style = (\n                    \"magenta\" if colorize else None\n                )\n                table.add_row(iod_title_text, *[\"\"] * (len(attr_headers) - 1), style=row_style)\n            # Print module attribute nodes as regular rows\n            else:\n                row = [getattr(node, attr, \"\") for attr in self.model.metadata.column_to_attr.values()]\n                row_style = None\n                if colorize:\n                    row_style = (\n                        \"yellow\"\n                        if self.model._is_include(node)\n                        else \"magenta\"\n                        if self.model._is_title(node)\n                        else LEVEL_COLORS[(node.depth - 1) % len(LEVEL_COLORS)]\n                    )\n                table.add_row(*row, style=row_style)\n\n        self.console.print(table)\n</code></pre>"},{"location":"api/iod_spec_printer/#dcmspec.iod_spec_printer.IODSpecPrinter.print_table","title":"<code>print_table(colorize=False)</code>","text":"<p>Print the specification model as a flat table with module title rows.</p> PARAMETER DESCRIPTION <code>colorize</code> <p>Whether to colorize the output by node depth.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/dcmspec/iod_spec_printer.py</code> <pre><code>def print_table(self, colorize: bool = False):\n    \"\"\"Print the specification model as a flat table with module title rows.\n\n    Args:\n        colorize (bool): Whether to colorize the output by node depth.\n\n    \"\"\"\n    table = Table(show_header=True, header_style=\"bold magenta\", show_lines=True, box=box.ASCII_DOUBLE_HEAD)\n\n    attr_headers = list(self.model.metadata.header)\n    for header in attr_headers:\n        table.add_column(header, width=20)\n\n    # Traverse the tree in PreOrder (as in the base class)\n    for node in PreOrderIter(self.model.content):\n        # skip the root node\n        if node.name == \"content\":\n            continue            # Print IOD module nodes as a title row (one cell, spanning all columns)\n        if hasattr(node, \"module\"):\n            iod_title = getattr(node, \"module\", getattr(node, \"name\", \"\"))\n            iod_usage = getattr(node, \"usage\", \"\")\n            iod_title_text = f\"{iod_title} Module ({iod_usage})\" if iod_usage else iod_title\n            # Set title style\n            row_style = (\n                \"magenta\" if colorize else None\n            )\n            table.add_row(iod_title_text, *[\"\"] * (len(attr_headers) - 1), style=row_style)\n        # Print module attribute nodes as regular rows\n        else:\n            row = [getattr(node, attr, \"\") for attr in self.model.metadata.column_to_attr.values()]\n            row_style = None\n            if colorize:\n                row_style = (\n                    \"yellow\"\n                    if self.model._is_include(node)\n                    else \"magenta\"\n                    if self.model._is_title(node)\n                    else LEVEL_COLORS[(node.depth - 1) % len(LEVEL_COLORS)]\n                )\n            table.add_row(*row, style=row_style)\n\n    self.console.print(table)\n</code></pre>"},{"location":"api/json_spec_store/","title":"JSONSpecStore","text":""},{"location":"api/json_spec_store/#dcmspec.json_spec_store.JSONSpecStore","title":"<code>dcmspec.json_spec_store.JSONSpecStore</code>","text":"<p>               Bases: <code>SpecStore</code></p> <p>Model store class for DICOM specification models storage in JSON format.</p> <p>Provides methods to load and save DICOM specification models to and from JSON files. Inherits logging from SpecStore.</p> Source code in <code>src/dcmspec/json_spec_store.py</code> <pre><code>class JSONSpecStore(SpecStore):\n    \"\"\"Model store class for DICOM specification models storage in JSON format.\n\n    Provides methods to load and save DICOM specification models to and from JSON files.\n    Inherits logging from SpecStore.\n    \"\"\"\n\n    def load(self, path: str) -&gt; SpecModel:\n        \"\"\"Load a specification model from a JSON file.\n\n        Args:\n            path (str): The path to the JSON file to load.\n\n        Returns:\n            SpecModel: The specification model containing both metadata and content nodes.\n\n        Raises:\n            RuntimeError: If the file cannot be read, parsed, or has an invalid structure.\n\n        \"\"\"\n        try:\n            importer = JsonImporter()\n            with open(path, \"r\", encoding=\"utf-8\") as json_file:\n                root = importer.read(json_file)\n\n            # Check that the root node is named \"dcmspec\"\n            if root.name != \"dcmspec\":\n                raise RuntimeError(f\"Invalid model structure in JSON file {path}: root node must be 'dcmspec'.\")\n\n            # Search for metadata and content nodes directly under the root\n            metadata = next((node for node in root.children if node.name == \"metadata\"), None)\n            content = next((node for node in root.children if node.name == \"content\"), None)\n\n            if metadata is None or content is None:\n                raise RuntimeError(\n                    f\"Invalid model structure in JSON file {path}: \"\n                    f\"Both 'metadata' and 'content' nodes must be present as children of 'dcmspec'.\"\n                )\n\n            # Detach the model nodes from the file root node\n            metadata.parent = None\n            content.parent = None\n\n            # Convert keys of column_to_attr back to integers if present in metadata\n            if \"column_to_attr\" in metadata.__dict__:\n                metadata.column_to_attr = {int(k): v for k, v in metadata.column_to_attr.items()}\n\n            return SpecModel(metadata=metadata, content=content)\n        except OSError as e:\n            raise RuntimeError(f\"Failed to read model data from JSON file {path}: {e}\") from e\n        except json.JSONDecodeError as e:\n            raise RuntimeError(f\"Failed to parse JSON file {path}: {e}\") from e\n\n    def save(self, model: SpecModel, path: str) -&gt; None:\n        \"\"\"Save a specification model to a JSON file.\n\n        Args:\n            model (SpecModel): The model object (an instance of SpecModel or a derived class)\n                containing metadata and content nodes to save.\n            path (str): The path to the JSON file to write.\n\n        Returns:\n            None\n\n        Raises:\n            RuntimeError: If the file cannot be written.\n\n        \"\"\"\n        # Create the destination folder if it does not exist\n        dir_name = os.path.dirname(path)\n        if dir_name:\n            os.makedirs(dir_name, exist_ok=True)\n\n        # Create a new root node \"dcmspec\"\n        root_node = Node(\"dcmspec\")\n\n        # Temporarily add the model's metadata and content as children of the new root node\n        model.metadata.parent = root_node\n        model.content.parent = root_node\n\n        try:\n            exporter = JsonExporter(indent=4, sort_keys=False)\n            with open(path, \"w\", encoding=\"utf-8\") as json_file:\n                exporter.write(root_node, json_file)\n            self.logger.info(f\"Attribute model saved as JSON to {path}\")\n\n        except OSError as e:\n            raise RuntimeError(f\"Failed to write JSON file {path}: {e}\") from e\n\n        # Detach the temporary children to leave the model unchanged\n        model.metadata.parent = None\n        model.content.parent = None\n</code></pre>"},{"location":"api/json_spec_store/#dcmspec.json_spec_store.JSONSpecStore.load","title":"<code>load(path)</code>","text":"<p>Load a specification model from a JSON file.</p> PARAMETER DESCRIPTION <code>path</code> <p>The path to the JSON file to load.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>SpecModel</code> <p>The specification model containing both metadata and content nodes.</p> <p> TYPE: <code>SpecModel</code> </p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If the file cannot be read, parsed, or has an invalid structure.</p> Source code in <code>src/dcmspec/json_spec_store.py</code> <pre><code>def load(self, path: str) -&gt; SpecModel:\n    \"\"\"Load a specification model from a JSON file.\n\n    Args:\n        path (str): The path to the JSON file to load.\n\n    Returns:\n        SpecModel: The specification model containing both metadata and content nodes.\n\n    Raises:\n        RuntimeError: If the file cannot be read, parsed, or has an invalid structure.\n\n    \"\"\"\n    try:\n        importer = JsonImporter()\n        with open(path, \"r\", encoding=\"utf-8\") as json_file:\n            root = importer.read(json_file)\n\n        # Check that the root node is named \"dcmspec\"\n        if root.name != \"dcmspec\":\n            raise RuntimeError(f\"Invalid model structure in JSON file {path}: root node must be 'dcmspec'.\")\n\n        # Search for metadata and content nodes directly under the root\n        metadata = next((node for node in root.children if node.name == \"metadata\"), None)\n        content = next((node for node in root.children if node.name == \"content\"), None)\n\n        if metadata is None or content is None:\n            raise RuntimeError(\n                f\"Invalid model structure in JSON file {path}: \"\n                f\"Both 'metadata' and 'content' nodes must be present as children of 'dcmspec'.\"\n            )\n\n        # Detach the model nodes from the file root node\n        metadata.parent = None\n        content.parent = None\n\n        # Convert keys of column_to_attr back to integers if present in metadata\n        if \"column_to_attr\" in metadata.__dict__:\n            metadata.column_to_attr = {int(k): v for k, v in metadata.column_to_attr.items()}\n\n        return SpecModel(metadata=metadata, content=content)\n    except OSError as e:\n        raise RuntimeError(f\"Failed to read model data from JSON file {path}: {e}\") from e\n    except json.JSONDecodeError as e:\n        raise RuntimeError(f\"Failed to parse JSON file {path}: {e}\") from e\n</code></pre>"},{"location":"api/json_spec_store/#dcmspec.json_spec_store.JSONSpecStore.save","title":"<code>save(model, path)</code>","text":"<p>Save a specification model to a JSON file.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model object (an instance of SpecModel or a derived class) containing metadata and content nodes to save.</p> <p> TYPE: <code>SpecModel</code> </p> <code>path</code> <p>The path to the JSON file to write.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>None</code> <p>None</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If the file cannot be written.</p> Source code in <code>src/dcmspec/json_spec_store.py</code> <pre><code>def save(self, model: SpecModel, path: str) -&gt; None:\n    \"\"\"Save a specification model to a JSON file.\n\n    Args:\n        model (SpecModel): The model object (an instance of SpecModel or a derived class)\n            containing metadata and content nodes to save.\n        path (str): The path to the JSON file to write.\n\n    Returns:\n        None\n\n    Raises:\n        RuntimeError: If the file cannot be written.\n\n    \"\"\"\n    # Create the destination folder if it does not exist\n    dir_name = os.path.dirname(path)\n    if dir_name:\n        os.makedirs(dir_name, exist_ok=True)\n\n    # Create a new root node \"dcmspec\"\n    root_node = Node(\"dcmspec\")\n\n    # Temporarily add the model's metadata and content as children of the new root node\n    model.metadata.parent = root_node\n    model.content.parent = root_node\n\n    try:\n        exporter = JsonExporter(indent=4, sort_keys=False)\n        with open(path, \"w\", encoding=\"utf-8\") as json_file:\n            exporter.write(root_node, json_file)\n        self.logger.info(f\"Attribute model saved as JSON to {path}\")\n\n    except OSError as e:\n        raise RuntimeError(f\"Failed to write JSON file {path}: {e}\") from e\n\n    # Detach the temporary children to leave the model unchanged\n    model.metadata.parent = None\n    model.content.parent = None\n</code></pre>"},{"location":"api/pdf_doc_handler/","title":"PDFDocHandler","text":""},{"location":"api/pdf_doc_handler/#dcmspec.pdf_doc_handler.PDFDocHandler","title":"<code>dcmspec.pdf_doc_handler.PDFDocHandler</code>","text":"<p>               Bases: <code>DocHandler</code></p> <p>Handler class for extracting tables from PDF documents.</p> <p>Provides methods to download, cache, and extract tables as CSV data from PDF files.</p> Source code in <code>src/dcmspec/pdf_doc_handler.py</code> <pre><code>class PDFDocHandler(DocHandler):\n    \"\"\"Handler class for extracting tables from PDF documents.\n\n    Provides methods to download, cache, and extract tables as CSV data from PDF files.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Optional[Config] = None,\n        logger: Optional[logging.Logger] = None,\n        extractor: str = \"pdfplumber\"\n    ):\n        \"\"\"Initialize the PDF document handler.\n\n        Sets up the handler with an optional configuration and logger.\n\n        Args:\n            config (Optional[Config]): Configuration object for cache and other settings.\n            logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n            extractor (str): Table extraction library to use. \n                `pdfplumber` (default) uses pdfplumber for extraction.\n                `camelot` uses Camelot (lattice flavor) for extraction.\n                `pdfplumber` detects tables by analyzing lines and whitespace in the PDF's vector content,\n                while `camelot` detects tables by processing the rendered page image to find drawn lines.\n\n        \"\"\"\n        super().__init__(config=config, logger=logger)\n        self.extractor = extractor\n        self.logger.debug(f\"PDFDocHandler initialized with extractor {self.extractor} and logger {self.logger.name} \"\n                          f\"at level {logging.getLevelName(self.logger.level)}\")\n\n        self.cache_file_name = None\n\n    def load_document(\n        self,\n        cache_file_name: str,\n        url: Optional[str] = None,\n        force_download: bool = False,\n        progress_observer: 'Optional[ProgressObserver]' = None,\n        # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n        progress_callback: 'Optional[Callable[[int], None]]' = None,\n        # END LEGACY SUPPORT\n        page_numbers: Optional[list] = None,\n        table_indices: Optional[list] = None,\n        table_header_rowspan: Optional[dict] = None,\n        table_id: Optional[str] = None,\n    ) -&gt; dict:\n        \"\"\"Download, cache, and extract the logical CSV table from the PDF.\n\n        Args:\n            cache_file_name (str): Path to the local cached PDF file.\n            url (str, optional): URL to download the file from if not cached or if force_download is True.\n            force_download (bool): If True, do not use cache and download the file from the URL.\n            progress_observer (Optional[ProgressObserver]): Optional observer to report download progress.\n            progress_callback (Optional[Callable[[int], None]]): [LEGACY, Deprecated] Optional callback to\n                report progress as an integer percent (0-100, or -1 if indeterminate). Use progress_observer\n                instead. Will be removed in a future release.\n            page_numbers (list, optional): List of page numbers to extract tables from.\n            table_indices (list, optional): List of (page, index) tuples specifying which tables to concatenate.\n            table_header_rowspan (dict, optional): Number of header rows (rowspan) for each table in table_indices.\n            table_id (str, optional): An identifier for the concatenated table.\n\n        Returns:\n            dict: The specification table dict with keys 'header', 'data', and optionally 'table_id'.\n\n        Example:\n            ```python\n            handler = PDFDocHandler()\n            spec_table = handler.load_document(\n                cache_file_name=\"myfile.pdf\",\n                url=\"https://example.com/myfile.pdf\",\n                page_numbers=[10, 11],\n                table_indices=[(10, 0), (11, 1)],\n                table_header_rowspan={\n                    (10, 0): 2,  # Table starts on page 10, index 0 and has 2 header rows\n                    (11, 1): 2,  # Table ends on page 11, index 1 and has 2 header rows\n                },\n                table_id=\"my_spec_table\"\n            )\n            ```\n\n        \"\"\"\n        # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n        progress_observer = handle_legacy_callback(progress_observer, progress_callback)\n        # END LEGACY SUPPORT\n        self.cache_file_name = cache_file_name\n        cache_file_path = os.path.join(self.config.get_param(\"cache_dir\"), \"standard\", cache_file_name)\n        need_download = force_download or (not os.path.exists(cache_file_path))\n        if need_download:\n            if not url:\n                self.logger.error(\"URL must be provided to download the file.\")\n                raise ValueError(\"URL must be provided to download the file.\")\n            self.logger.info(f\"Downloading PDF from {url} to {cache_file_path}\")\n            cache_file_path = self.download(url, cache_file_name, progress_observer=progress_observer)\n        else:\n            self.logger.info(f\"Loading PDF from cache file {cache_file_path}\")\n\n        if page_numbers is None or table_indices is None:\n            self.logger.error(\"page_numbers and table_indices must be provided to extract the logical table.\")\n            raise ValueError(\"page_numbers and table_indices must be provided to extract the logical table.\")\n\n        self.logger.debug(f\"Extracting tables from pages: {page_numbers}\")\n        if self.extractor == \"pdfplumber\":\n            pdf = pdfplumber.open(cache_file_path)\n            all_tables = self.extract_tables_pdfplumber(pdf, page_numbers)\n            self.logger.debug(f\"Extracted {len(all_tables)} tables from PDF using pdfplumber.\")\n            pdf.close()\n        elif self.extractor == \"camelot\":\n            all_tables = self.extract_tables_camelot(cache_file_path, page_numbers)\n            self.logger.debug(f\"Extracted {len(all_tables)} tables from PDF using Camelot.\")\n        else:\n            raise ValueError(f\"Unknown extractor: {self.extractor}\")\n\n        if self.logger.isEnabledFor(logging.DEBUG):\n            for idx, table in enumerate(all_tables):\n                self.logger.debug(f\"\\nTable {idx}:\")\n                for row in table[\"data\"]:\n                    self.logger.debug(row)\n\n        self.logger.debug(f\"Selecting tables with indices: {table_indices}\")\n        selected_tables = self.select_tables(all_tables, table_indices, table_header_rowspan)\n        self.logger.debug(f\"Concatenating selected tables with table_id: {table_id}\")\n        spec_table = self.concat_tables(selected_tables, table_id=table_id)\n        self.logger.debug(f\"Returning spec_table with header: {spec_table.get('header', [])}\")\n        self.logger.debug(f\"Returning spec_table with data: {spec_table.get('data', [])}\")\n\n        return spec_table\n\n    def download(self,\n                url: str,\n                cache_file_name: str,\n                progress_observer: 'Optional[ProgressObserver]' = None,\n                # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n                progress_callback: 'Optional[Callable[[int], None]]' = None\n                # END LEGACY SUPPORT\n                ) -&gt; str:\n        \"\"\"Download and cache a PDF file from a URL using the base class download method.\n\n        Args:\n            url: The URL of the PDF document to download.\n            cache_file_name: The filename of the cached document.\n            progress_observer (Optional[ProgressObserver]): Optional observer to report download progress.\n            progress_callback (Optional[Callable[[int], None]]): [LEGACY, Deprecated] Optional callback to\n                report progress as an integer percent (0-100, or -1 if indeterminate). Use progress_observer\n                instead. Will be removed in a future release.\n\n        Returns:\n            The file path where the document was saved.\n\n        Raises:\n            RuntimeError: If the download or save fails.\n\n        \"\"\"\n        # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n        progress_observer = handle_legacy_callback(progress_observer, progress_callback)\n        # END LEGACY SUPPORT\n        file_path = os.path.join(self.config.get_param(\"cache_dir\"), \"standard\", cache_file_name)\n        return super().download(url, file_path, binary=True, progress_observer=progress_observer)\n\n    def extract_tables_pdfplumber(self, pdf: pdfplumber.PDF, page_numbers: List[int]) -&gt; List[dict]:\n        \"\"\"Extract and return all tables from the specified PDF pages using pdfplumber.\n\n        Uses pdfplumber to extract tables from the PDF by analyzing lines and whitespace in the PDF's vector content.\n\n        Args:\n            pdf (pdfplumber.PDF): The PDF object.\n            page_numbers (List[int]): List of page numbers (1-indexed) to extract tables from.\n\n        Returns:\n            List[dict]: List of dicts, each with keys 'page', 'index', and 'data' (table as list of rows).\n\n        Raises:\n            IndexError: If a page number is out of range for the PDF.\n\n        \"\"\"\n        all_tables = []\n        num_pages = len(pdf.pages)\n        for page_num in page_numbers:\n            if not (1 &lt;= page_num &lt;= num_pages):\n                raise IndexError(\n                    f\"Page number {page_num} is out of range for this PDF (valid range: 1 to {num_pages})\"\n                )\n            page = pdf.pages[page_num - 1]\n            tables = page.extract_tables(\n                table_settings={\n                    \"vertical_strategy\": \"lines\", \n                    \"horizontal_strategy\": \"lines\",\n                    \"snap_tolerance\": 8,\n                }\n            )\n\n            if not tables:\n                continue\n            all_tables.extend(\n                {\n                    \"page\": page_num,\n                    \"index\": idx,\n                    \"data\": table,\n                }\n                for idx, table in enumerate(tables)\n            )\n        return all_tables\n\n    def extract_tables_camelot(self, file_path: str, page_numbers: List[int]) -&gt; List[dict]:\n        \"\"\"Extract and return all tables from the specified PDF pages using Camelot.\n\n        Uses Camelot in \"lattice\" mode, which detects tables by analyzing the rendered page image for drawn lines.\n\n        Args:\n            file_path (str): Path to the PDF file.\n            page_numbers (List[int]): List of page numbers (1-indexed) to extract tables from.\n\n        Returns:\n            List[dict]: List of dicts, each with keys 'page', 'index', and 'data' (table as list of rows).\n\n        \"\"\"\n        all_tables = []\n        for page_num in page_numbers:\n            tables = camelot.read_pdf(\n                file_path,\n                pages=str(page_num),\n                flavor=\"lattice\",\n                line_scale=40\n            )\n            all_tables.extend(\n                {\n                    \"page\": page_num,\n                    \"index\": idx,\n                    \"data\": table.df.values.tolist(),\n                }\n                for idx, table in enumerate(tables)\n            )\n        return all_tables\n\n    def select_tables(\n        self,\n        tables: List[dict],\n        table_indices: List[tuple],\n        table_header_rowspan: Optional[dict] = None,\n    ) -&gt; List[dict]:\n        \"\"\"Select tables referenced by table_indices and split each table into header and data.\n\n        This method processes a list of extracted tables, selects those specified by table_indices,\n        and splits each selected table into a header and data rows. If a table has multiple header rows\n        (as specified by table_header_rowspan), these rows are merged column-wise to form a single header row.\n\n\n        Args:\n            tables (List[dict]): List of table dicts, each with 'page', 'index', and 'data' (raw table rows).\n            table_indices (List[tuple]): List of (page, index) tuples specifying which tables to select and process.\n            table_header_rowspan (dict, optional): Number of header rows (rowspan) for each table in table_indices,\n                keyed by (page, index). If not specified, defaults to 1 header row per table.\n\n        Returns:\n            List[dict]: List of dicts, each with keys:\n                - 'page': page number of the table\n                - 'index': index of the table on the page\n                - 'header': merged header row (list of column names)\n                - 'data': list of data rows (list of cell values)\n\n        Example:\n            ```python\n            selected_tables = handler.select_tables(\n                tables,\n                table_indices=[(10, 0), (11, 1)],\n                table_header_rowspan={(10, 0): 2, (11, 1): 2}\n            )\n            for table in selected_tables:\n                print(table[\"header\"], table[\"data\"])\n            ```\n\n        \"\"\"\n        def merge_multirow_header(header_rows):\n            n_cols = max(len(row) for row in header_rows)\n            merged = []\n            for col in range(n_cols):\n                merged_cell = \" \".join(\n                    str(row[col]).strip() for row in header_rows\n                    if col &lt; len(row) and row[col] not in (None, \"\")\n                ).strip()\n                merged.append(merged_cell)\n            return merged\n\n        selected_tables = []\n        for page, idx in table_indices:\n            for table in tables:\n                if table[\"page\"] == page and table[\"index\"] == idx:\n                    table_rows = table[\"data\"]\n                    n_header_rows = 1\n                    if table_header_rowspan and (page, idx) in table_header_rowspan:\n                        n_header_rows = table_header_rowspan[(page, idx)]\n                    header_rows = table_rows[:n_header_rows]\n                    data_rows = table_rows[n_header_rows:]\n                    # Merge header rows if needed\n                    if len(header_rows) == 1:\n                        header_ = header_rows[0]\n                    else:\n                        header_ = merge_multirow_header(header_rows)\n                    selected_tables.append({\n                        \"page\": page,\n                        \"index\": idx,\n                        \"header\": header_,\n                        \"data\": data_rows,\n                    })\n        return selected_tables\n\n    def concat_tables(\n        self,\n        tables: List[dict],\n        table_id: str = None,\n    ) -&gt; dict:\n        \"\"\"Concatenate selected tables (across pages or by specification) into a single logical table.\n\n        Args:\n            tables (List[dict]): List of table dicts, each with 'page', 'index', 'header', and 'data'.\n            table_id (str, optional): An identifier for the concatenated table.\n\n        Returns:\n            dict: A dict with keys 'table_id' (if provided), 'header' (from the first table), \n            and 'data' (the concatenated table as a list of rows).\n\n        \"\"\"\n        grouped_table = []\n        header = []\n        first = True\n        for table in tables:\n            header_ = table.get(\"header\", [])\n            if first:\n                header = header_\n                first = False\n            elif header and header_ != header:\n                self.logger.warning(\n                    f\"Header mismatch in concatenated tables: {header} != {header_} \"\n                    f\"(page {table['page']}, index {table['index']})\"\n                )\n            n_columns = len(header)\n            for row in table[\"data\"]:\n                # Always pad/truncate to header length\n                row = (row + [\"\"] * (n_columns - len(row)))[:n_columns]\n                grouped_table.append(row)\n\n        # Realign columns: shift non-empty header cells and data cells left to fill gaps ---\n        def shift_row_left(row):\n            new_row = [cell for cell in row if cell not in (None, \"\")]\n            new_row += [\"\"] * (len(row) - len(new_row))\n            return new_row\n\n        header = shift_row_left(header)\n        data = [shift_row_left(row) for row in grouped_table]\n\n        # Remove columns where the header is empty\n        columns_to_keep = [i for i, cell in enumerate(header) if cell not in (None, \"\")]\n        header = [header[i] for i in columns_to_keep]\n        data = [[row[i] for i in columns_to_keep] for row in data]\n\n        result = {\"header\": header, \"data\": data}\n        if table_id is not None:\n            result[\"table_id\"] = table_id\n        return result\n\n    def extract_notes(\n        self,\n        pdf: pdfplumber.PDF,\n        page_numbers: List[int],\n        table_id: str = None,\n        note_pattern: str = r\"^\\d*\\s*Note\\s\\d+:\",\n        header_footer_pattern: str = r\"^\\s*(IHE|_{3,}|Rev\\.|Copyright|Template|Page\\s\\d+|\\(TDW-II\\))\",\n        line_number_pattern: str = r\"^\\d+\\s\",\n        end_note_pattern: str = r\".*7\\.5\\.1\\.1\\.2\",\n    ) -&gt; dict:\n        \"\"\"Extract notes referenced in tables from the specified PDF pages.\n\n        Args:\n            pdf (pdfplumber.PDF): The PDF object.\n            page_numbers (List[int]): List of page numbers (1-indexed) to extract notes from.\n            table_id (str, optional): The table_id to associate with these notes.\n            note_pattern (str): Regex pattern to identify note lines.\n            header_footer_pattern (str): Regex pattern to skip header/footer lines.\n            line_number_pattern (str): Regex pattern to remove line numbers.\n            end_note_pattern (str): Regex pattern to identify the end of notes section.\n\n        Returns:\n            dict: Mapping from note key (e.g., \"Note 1:\") to a dict with 'text' and 'table_id' (if provided).\n\n        Example return:\n            {\n                \"Note 1:\": {\"text\": \"...\", \"table_id\": \"T-7.5-1\"},\n                \"Note 2:\": {\"text\": \"...\", \"table_id\": \"T-7.5-1\"},\n            }\n\n        \"\"\"\n        import re\n\n        notes = {}\n        note_re = re.compile(note_pattern)\n        header_footer_re = re.compile(header_footer_pattern)\n        line_number_re = re.compile(line_number_pattern)\n        end_note_re = re.compile(end_note_pattern)\n        current_note = None\n\n        for page_num in page_numbers:\n            page = pdf.pages[page_num - 1]\n            text = page.extract_text()\n            if text:\n                lines = text.split(\"\\n\")\n                for line in lines:\n                    # Always skip header/footer lines, even in note continuation\n                    if header_footer_re.search(line):\n                        continue\n                    if end_note_re.search(line):\n                        current_note = None\n                        break\n                    match = note_re.search(line)\n                    if match:\n                        note_number = match.group().strip()\n                        note_number = re.sub(r\"^\\d*\\s*\", \"\", note_number)\n                        note_text = line[match.end():].strip()\n                        notes[note_number] = {\n                            \"text\": note_text,\n                            \"table_id\": table_id\n                        } if table_id else {\"text\": note_text}\n                        current_note = note_number\n                    elif current_note:\n                        line = line_number_re.sub(\"\", line).strip()\n                        notes[current_note][\"text\"] += f\" {line}\"\n        if notes:\n            self.logger.debug(f\"Extracted notes: {list(notes.keys())}\")\n        return notes\n</code></pre>"},{"location":"api/pdf_doc_handler/#dcmspec.pdf_doc_handler.PDFDocHandler.__init__","title":"<code>__init__(config=None, logger=None, extractor='pdfplumber')</code>","text":"<p>Initialize the PDF document handler.</p> <p>Sets up the handler with an optional configuration and logger.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object for cache and other settings.</p> <p> TYPE: <code>Optional[Config]</code> DEFAULT: <code>None</code> </p> <code>logger</code> <p>Logger instance to use. If None, a default logger is created.</p> <p> TYPE: <code>Optional[Logger]</code> DEFAULT: <code>None</code> </p> <code>extractor</code> <p>Table extraction library to use.  <code>pdfplumber</code> (default) uses pdfplumber for extraction. <code>camelot</code> uses Camelot (lattice flavor) for extraction. <code>pdfplumber</code> detects tables by analyzing lines and whitespace in the PDF's vector content, while <code>camelot</code> detects tables by processing the rendered page image to find drawn lines.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'pdfplumber'</code> </p> Source code in <code>src/dcmspec/pdf_doc_handler.py</code> <pre><code>def __init__(\n    self,\n    config: Optional[Config] = None,\n    logger: Optional[logging.Logger] = None,\n    extractor: str = \"pdfplumber\"\n):\n    \"\"\"Initialize the PDF document handler.\n\n    Sets up the handler with an optional configuration and logger.\n\n    Args:\n        config (Optional[Config]): Configuration object for cache and other settings.\n        logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n        extractor (str): Table extraction library to use. \n            `pdfplumber` (default) uses pdfplumber for extraction.\n            `camelot` uses Camelot (lattice flavor) for extraction.\n            `pdfplumber` detects tables by analyzing lines and whitespace in the PDF's vector content,\n            while `camelot` detects tables by processing the rendered page image to find drawn lines.\n\n    \"\"\"\n    super().__init__(config=config, logger=logger)\n    self.extractor = extractor\n    self.logger.debug(f\"PDFDocHandler initialized with extractor {self.extractor} and logger {self.logger.name} \"\n                      f\"at level {logging.getLevelName(self.logger.level)}\")\n\n    self.cache_file_name = None\n</code></pre>"},{"location":"api/pdf_doc_handler/#dcmspec.pdf_doc_handler.PDFDocHandler.concat_tables","title":"<code>concat_tables(tables, table_id=None)</code>","text":"<p>Concatenate selected tables (across pages or by specification) into a single logical table.</p> PARAMETER DESCRIPTION <code>tables</code> <p>List of table dicts, each with 'page', 'index', 'header', and 'data'.</p> <p> TYPE: <code>List[dict]</code> </p> <code>table_id</code> <p>An identifier for the concatenated table.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>A dict with keys 'table_id' (if provided), 'header' (from the first table), </p> <p> TYPE: <code>dict</code> </p> <code>dict</code> <p>and 'data' (the concatenated table as a list of rows).</p> Source code in <code>src/dcmspec/pdf_doc_handler.py</code> <pre><code>def concat_tables(\n    self,\n    tables: List[dict],\n    table_id: str = None,\n) -&gt; dict:\n    \"\"\"Concatenate selected tables (across pages or by specification) into a single logical table.\n\n    Args:\n        tables (List[dict]): List of table dicts, each with 'page', 'index', 'header', and 'data'.\n        table_id (str, optional): An identifier for the concatenated table.\n\n    Returns:\n        dict: A dict with keys 'table_id' (if provided), 'header' (from the first table), \n        and 'data' (the concatenated table as a list of rows).\n\n    \"\"\"\n    grouped_table = []\n    header = []\n    first = True\n    for table in tables:\n        header_ = table.get(\"header\", [])\n        if first:\n            header = header_\n            first = False\n        elif header and header_ != header:\n            self.logger.warning(\n                f\"Header mismatch in concatenated tables: {header} != {header_} \"\n                f\"(page {table['page']}, index {table['index']})\"\n            )\n        n_columns = len(header)\n        for row in table[\"data\"]:\n            # Always pad/truncate to header length\n            row = (row + [\"\"] * (n_columns - len(row)))[:n_columns]\n            grouped_table.append(row)\n\n    # Realign columns: shift non-empty header cells and data cells left to fill gaps ---\n    def shift_row_left(row):\n        new_row = [cell for cell in row if cell not in (None, \"\")]\n        new_row += [\"\"] * (len(row) - len(new_row))\n        return new_row\n\n    header = shift_row_left(header)\n    data = [shift_row_left(row) for row in grouped_table]\n\n    # Remove columns where the header is empty\n    columns_to_keep = [i for i, cell in enumerate(header) if cell not in (None, \"\")]\n    header = [header[i] for i in columns_to_keep]\n    data = [[row[i] for i in columns_to_keep] for row in data]\n\n    result = {\"header\": header, \"data\": data}\n    if table_id is not None:\n        result[\"table_id\"] = table_id\n    return result\n</code></pre>"},{"location":"api/pdf_doc_handler/#dcmspec.pdf_doc_handler.PDFDocHandler.download","title":"<code>download(url, cache_file_name, progress_observer=None, progress_callback=None)</code>","text":"<p>Download and cache a PDF file from a URL using the base class download method.</p> PARAMETER DESCRIPTION <code>url</code> <p>The URL of the PDF document to download.</p> <p> TYPE: <code>str</code> </p> <code>cache_file_name</code> <p>The filename of the cached document.</p> <p> TYPE: <code>str</code> </p> <code>progress_observer</code> <p>Optional observer to report download progress.</p> <p> TYPE: <code>Optional[ProgressObserver]</code> DEFAULT: <code>None</code> </p> <code>progress_callback</code> <p>[LEGACY, Deprecated] Optional callback to report progress as an integer percent (0-100, or -1 if indeterminate). Use progress_observer instead. Will be removed in a future release.</p> <p> TYPE: <code>Optional[Callable[[int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The file path where the document was saved.</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If the download or save fails.</p> Source code in <code>src/dcmspec/pdf_doc_handler.py</code> <pre><code>def download(self,\n            url: str,\n            cache_file_name: str,\n            progress_observer: 'Optional[ProgressObserver]' = None,\n            # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n            progress_callback: 'Optional[Callable[[int], None]]' = None\n            # END LEGACY SUPPORT\n            ) -&gt; str:\n    \"\"\"Download and cache a PDF file from a URL using the base class download method.\n\n    Args:\n        url: The URL of the PDF document to download.\n        cache_file_name: The filename of the cached document.\n        progress_observer (Optional[ProgressObserver]): Optional observer to report download progress.\n        progress_callback (Optional[Callable[[int], None]]): [LEGACY, Deprecated] Optional callback to\n            report progress as an integer percent (0-100, or -1 if indeterminate). Use progress_observer\n            instead. Will be removed in a future release.\n\n    Returns:\n        The file path where the document was saved.\n\n    Raises:\n        RuntimeError: If the download or save fails.\n\n    \"\"\"\n    # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n    progress_observer = handle_legacy_callback(progress_observer, progress_callback)\n    # END LEGACY SUPPORT\n    file_path = os.path.join(self.config.get_param(\"cache_dir\"), \"standard\", cache_file_name)\n    return super().download(url, file_path, binary=True, progress_observer=progress_observer)\n</code></pre>"},{"location":"api/pdf_doc_handler/#dcmspec.pdf_doc_handler.PDFDocHandler.extract_notes","title":"<code>extract_notes(pdf, page_numbers, table_id=None, note_pattern='^\\\\d*\\\\s*Note\\\\s\\\\d+:', header_footer_pattern='^\\\\s*(IHE|_{3,}|Rev\\\\.|Copyright|Template|Page\\\\s\\\\d+|\\\\(TDW-II\\\\))', line_number_pattern='^\\\\d+\\\\s', end_note_pattern='.*7\\\\.5\\\\.1\\\\.1\\\\.2')</code>","text":"<p>Extract notes referenced in tables from the specified PDF pages.</p> PARAMETER DESCRIPTION <code>pdf</code> <p>The PDF object.</p> <p> TYPE: <code>PDF</code> </p> <code>page_numbers</code> <p>List of page numbers (1-indexed) to extract notes from.</p> <p> TYPE: <code>List[int]</code> </p> <code>table_id</code> <p>The table_id to associate with these notes.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>note_pattern</code> <p>Regex pattern to identify note lines.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'^\\\\d*\\\\s*Note\\\\s\\\\d+:'</code> </p> <code>header_footer_pattern</code> <p>Regex pattern to skip header/footer lines.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'^\\\\s*(IHE|_{3,}|Rev\\\\.|Copyright|Template|Page\\\\s\\\\d+|\\\\(TDW-II\\\\))'</code> </p> <code>line_number_pattern</code> <p>Regex pattern to remove line numbers.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'^\\\\d+\\\\s'</code> </p> <code>end_note_pattern</code> <p>Regex pattern to identify the end of notes section.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'.*7\\\\.5\\\\.1\\\\.1\\\\.2'</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>Mapping from note key (e.g., \"Note 1:\") to a dict with 'text' and 'table_id' (if provided).</p> <p> TYPE: <code>dict</code> </p> Example return <p>{     \"Note 1:\": {\"text\": \"...\", \"table_id\": \"T-7.5-1\"},     \"Note 2:\": {\"text\": \"...\", \"table_id\": \"T-7.5-1\"}, }</p> Source code in <code>src/dcmspec/pdf_doc_handler.py</code> <pre><code>def extract_notes(\n    self,\n    pdf: pdfplumber.PDF,\n    page_numbers: List[int],\n    table_id: str = None,\n    note_pattern: str = r\"^\\d*\\s*Note\\s\\d+:\",\n    header_footer_pattern: str = r\"^\\s*(IHE|_{3,}|Rev\\.|Copyright|Template|Page\\s\\d+|\\(TDW-II\\))\",\n    line_number_pattern: str = r\"^\\d+\\s\",\n    end_note_pattern: str = r\".*7\\.5\\.1\\.1\\.2\",\n) -&gt; dict:\n    \"\"\"Extract notes referenced in tables from the specified PDF pages.\n\n    Args:\n        pdf (pdfplumber.PDF): The PDF object.\n        page_numbers (List[int]): List of page numbers (1-indexed) to extract notes from.\n        table_id (str, optional): The table_id to associate with these notes.\n        note_pattern (str): Regex pattern to identify note lines.\n        header_footer_pattern (str): Regex pattern to skip header/footer lines.\n        line_number_pattern (str): Regex pattern to remove line numbers.\n        end_note_pattern (str): Regex pattern to identify the end of notes section.\n\n    Returns:\n        dict: Mapping from note key (e.g., \"Note 1:\") to a dict with 'text' and 'table_id' (if provided).\n\n    Example return:\n        {\n            \"Note 1:\": {\"text\": \"...\", \"table_id\": \"T-7.5-1\"},\n            \"Note 2:\": {\"text\": \"...\", \"table_id\": \"T-7.5-1\"},\n        }\n\n    \"\"\"\n    import re\n\n    notes = {}\n    note_re = re.compile(note_pattern)\n    header_footer_re = re.compile(header_footer_pattern)\n    line_number_re = re.compile(line_number_pattern)\n    end_note_re = re.compile(end_note_pattern)\n    current_note = None\n\n    for page_num in page_numbers:\n        page = pdf.pages[page_num - 1]\n        text = page.extract_text()\n        if text:\n            lines = text.split(\"\\n\")\n            for line in lines:\n                # Always skip header/footer lines, even in note continuation\n                if header_footer_re.search(line):\n                    continue\n                if end_note_re.search(line):\n                    current_note = None\n                    break\n                match = note_re.search(line)\n                if match:\n                    note_number = match.group().strip()\n                    note_number = re.sub(r\"^\\d*\\s*\", \"\", note_number)\n                    note_text = line[match.end():].strip()\n                    notes[note_number] = {\n                        \"text\": note_text,\n                        \"table_id\": table_id\n                    } if table_id else {\"text\": note_text}\n                    current_note = note_number\n                elif current_note:\n                    line = line_number_re.sub(\"\", line).strip()\n                    notes[current_note][\"text\"] += f\" {line}\"\n    if notes:\n        self.logger.debug(f\"Extracted notes: {list(notes.keys())}\")\n    return notes\n</code></pre>"},{"location":"api/pdf_doc_handler/#dcmspec.pdf_doc_handler.PDFDocHandler.extract_tables_camelot","title":"<code>extract_tables_camelot(file_path, page_numbers)</code>","text":"<p>Extract and return all tables from the specified PDF pages using Camelot.</p> <p>Uses Camelot in \"lattice\" mode, which detects tables by analyzing the rendered page image for drawn lines.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to the PDF file.</p> <p> TYPE: <code>str</code> </p> <code>page_numbers</code> <p>List of page numbers (1-indexed) to extract tables from.</p> <p> TYPE: <code>List[int]</code> </p> RETURNS DESCRIPTION <code>List[dict]</code> <p>List[dict]: List of dicts, each with keys 'page', 'index', and 'data' (table as list of rows).</p> Source code in <code>src/dcmspec/pdf_doc_handler.py</code> <pre><code>def extract_tables_camelot(self, file_path: str, page_numbers: List[int]) -&gt; List[dict]:\n    \"\"\"Extract and return all tables from the specified PDF pages using Camelot.\n\n    Uses Camelot in \"lattice\" mode, which detects tables by analyzing the rendered page image for drawn lines.\n\n    Args:\n        file_path (str): Path to the PDF file.\n        page_numbers (List[int]): List of page numbers (1-indexed) to extract tables from.\n\n    Returns:\n        List[dict]: List of dicts, each with keys 'page', 'index', and 'data' (table as list of rows).\n\n    \"\"\"\n    all_tables = []\n    for page_num in page_numbers:\n        tables = camelot.read_pdf(\n            file_path,\n            pages=str(page_num),\n            flavor=\"lattice\",\n            line_scale=40\n        )\n        all_tables.extend(\n            {\n                \"page\": page_num,\n                \"index\": idx,\n                \"data\": table.df.values.tolist(),\n            }\n            for idx, table in enumerate(tables)\n        )\n    return all_tables\n</code></pre>"},{"location":"api/pdf_doc_handler/#dcmspec.pdf_doc_handler.PDFDocHandler.extract_tables_pdfplumber","title":"<code>extract_tables_pdfplumber(pdf, page_numbers)</code>","text":"<p>Extract and return all tables from the specified PDF pages using pdfplumber.</p> <p>Uses pdfplumber to extract tables from the PDF by analyzing lines and whitespace in the PDF's vector content.</p> PARAMETER DESCRIPTION <code>pdf</code> <p>The PDF object.</p> <p> TYPE: <code>PDF</code> </p> <code>page_numbers</code> <p>List of page numbers (1-indexed) to extract tables from.</p> <p> TYPE: <code>List[int]</code> </p> RETURNS DESCRIPTION <code>List[dict]</code> <p>List[dict]: List of dicts, each with keys 'page', 'index', and 'data' (table as list of rows).</p> RAISES DESCRIPTION <code>IndexError</code> <p>If a page number is out of range for the PDF.</p> Source code in <code>src/dcmspec/pdf_doc_handler.py</code> <pre><code>def extract_tables_pdfplumber(self, pdf: pdfplumber.PDF, page_numbers: List[int]) -&gt; List[dict]:\n    \"\"\"Extract and return all tables from the specified PDF pages using pdfplumber.\n\n    Uses pdfplumber to extract tables from the PDF by analyzing lines and whitespace in the PDF's vector content.\n\n    Args:\n        pdf (pdfplumber.PDF): The PDF object.\n        page_numbers (List[int]): List of page numbers (1-indexed) to extract tables from.\n\n    Returns:\n        List[dict]: List of dicts, each with keys 'page', 'index', and 'data' (table as list of rows).\n\n    Raises:\n        IndexError: If a page number is out of range for the PDF.\n\n    \"\"\"\n    all_tables = []\n    num_pages = len(pdf.pages)\n    for page_num in page_numbers:\n        if not (1 &lt;= page_num &lt;= num_pages):\n            raise IndexError(\n                f\"Page number {page_num} is out of range for this PDF (valid range: 1 to {num_pages})\"\n            )\n        page = pdf.pages[page_num - 1]\n        tables = page.extract_tables(\n            table_settings={\n                \"vertical_strategy\": \"lines\", \n                \"horizontal_strategy\": \"lines\",\n                \"snap_tolerance\": 8,\n            }\n        )\n\n        if not tables:\n            continue\n        all_tables.extend(\n            {\n                \"page\": page_num,\n                \"index\": idx,\n                \"data\": table,\n            }\n            for idx, table in enumerate(tables)\n        )\n    return all_tables\n</code></pre>"},{"location":"api/pdf_doc_handler/#dcmspec.pdf_doc_handler.PDFDocHandler.load_document","title":"<code>load_document(cache_file_name, url=None, force_download=False, progress_observer=None, progress_callback=None, page_numbers=None, table_indices=None, table_header_rowspan=None, table_id=None)</code>","text":"<p>Download, cache, and extract the logical CSV table from the PDF.</p> PARAMETER DESCRIPTION <code>cache_file_name</code> <p>Path to the local cached PDF file.</p> <p> TYPE: <code>str</code> </p> <code>url</code> <p>URL to download the file from if not cached or if force_download is True.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>force_download</code> <p>If True, do not use cache and download the file from the URL.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>progress_observer</code> <p>Optional observer to report download progress.</p> <p> TYPE: <code>Optional[ProgressObserver]</code> DEFAULT: <code>None</code> </p> <code>progress_callback</code> <p>[LEGACY, Deprecated] Optional callback to report progress as an integer percent (0-100, or -1 if indeterminate). Use progress_observer instead. Will be removed in a future release.</p> <p> TYPE: <code>Optional[Callable[[int], None]]</code> DEFAULT: <code>None</code> </p> <code>page_numbers</code> <p>List of page numbers to extract tables from.</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>table_indices</code> <p>List of (page, index) tuples specifying which tables to concatenate.</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>table_header_rowspan</code> <p>Number of header rows (rowspan) for each table in table_indices.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>table_id</code> <p>An identifier for the concatenated table.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>The specification table dict with keys 'header', 'data', and optionally 'table_id'.</p> <p> TYPE: <code>dict</code> </p> Example <pre><code>handler = PDFDocHandler()\nspec_table = handler.load_document(\n    cache_file_name=\"myfile.pdf\",\n    url=\"https://example.com/myfile.pdf\",\n    page_numbers=[10, 11],\n    table_indices=[(10, 0), (11, 1)],\n    table_header_rowspan={\n        (10, 0): 2,  # Table starts on page 10, index 0 and has 2 header rows\n        (11, 1): 2,  # Table ends on page 11, index 1 and has 2 header rows\n    },\n    table_id=\"my_spec_table\"\n)\n</code></pre> Source code in <code>src/dcmspec/pdf_doc_handler.py</code> <pre><code>def load_document(\n    self,\n    cache_file_name: str,\n    url: Optional[str] = None,\n    force_download: bool = False,\n    progress_observer: 'Optional[ProgressObserver]' = None,\n    # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n    progress_callback: 'Optional[Callable[[int], None]]' = None,\n    # END LEGACY SUPPORT\n    page_numbers: Optional[list] = None,\n    table_indices: Optional[list] = None,\n    table_header_rowspan: Optional[dict] = None,\n    table_id: Optional[str] = None,\n) -&gt; dict:\n    \"\"\"Download, cache, and extract the logical CSV table from the PDF.\n\n    Args:\n        cache_file_name (str): Path to the local cached PDF file.\n        url (str, optional): URL to download the file from if not cached or if force_download is True.\n        force_download (bool): If True, do not use cache and download the file from the URL.\n        progress_observer (Optional[ProgressObserver]): Optional observer to report download progress.\n        progress_callback (Optional[Callable[[int], None]]): [LEGACY, Deprecated] Optional callback to\n            report progress as an integer percent (0-100, or -1 if indeterminate). Use progress_observer\n            instead. Will be removed in a future release.\n        page_numbers (list, optional): List of page numbers to extract tables from.\n        table_indices (list, optional): List of (page, index) tuples specifying which tables to concatenate.\n        table_header_rowspan (dict, optional): Number of header rows (rowspan) for each table in table_indices.\n        table_id (str, optional): An identifier for the concatenated table.\n\n    Returns:\n        dict: The specification table dict with keys 'header', 'data', and optionally 'table_id'.\n\n    Example:\n        ```python\n        handler = PDFDocHandler()\n        spec_table = handler.load_document(\n            cache_file_name=\"myfile.pdf\",\n            url=\"https://example.com/myfile.pdf\",\n            page_numbers=[10, 11],\n            table_indices=[(10, 0), (11, 1)],\n            table_header_rowspan={\n                (10, 0): 2,  # Table starts on page 10, index 0 and has 2 header rows\n                (11, 1): 2,  # Table ends on page 11, index 1 and has 2 header rows\n            },\n            table_id=\"my_spec_table\"\n        )\n        ```\n\n    \"\"\"\n    # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n    progress_observer = handle_legacy_callback(progress_observer, progress_callback)\n    # END LEGACY SUPPORT\n    self.cache_file_name = cache_file_name\n    cache_file_path = os.path.join(self.config.get_param(\"cache_dir\"), \"standard\", cache_file_name)\n    need_download = force_download or (not os.path.exists(cache_file_path))\n    if need_download:\n        if not url:\n            self.logger.error(\"URL must be provided to download the file.\")\n            raise ValueError(\"URL must be provided to download the file.\")\n        self.logger.info(f\"Downloading PDF from {url} to {cache_file_path}\")\n        cache_file_path = self.download(url, cache_file_name, progress_observer=progress_observer)\n    else:\n        self.logger.info(f\"Loading PDF from cache file {cache_file_path}\")\n\n    if page_numbers is None or table_indices is None:\n        self.logger.error(\"page_numbers and table_indices must be provided to extract the logical table.\")\n        raise ValueError(\"page_numbers and table_indices must be provided to extract the logical table.\")\n\n    self.logger.debug(f\"Extracting tables from pages: {page_numbers}\")\n    if self.extractor == \"pdfplumber\":\n        pdf = pdfplumber.open(cache_file_path)\n        all_tables = self.extract_tables_pdfplumber(pdf, page_numbers)\n        self.logger.debug(f\"Extracted {len(all_tables)} tables from PDF using pdfplumber.\")\n        pdf.close()\n    elif self.extractor == \"camelot\":\n        all_tables = self.extract_tables_camelot(cache_file_path, page_numbers)\n        self.logger.debug(f\"Extracted {len(all_tables)} tables from PDF using Camelot.\")\n    else:\n        raise ValueError(f\"Unknown extractor: {self.extractor}\")\n\n    if self.logger.isEnabledFor(logging.DEBUG):\n        for idx, table in enumerate(all_tables):\n            self.logger.debug(f\"\\nTable {idx}:\")\n            for row in table[\"data\"]:\n                self.logger.debug(row)\n\n    self.logger.debug(f\"Selecting tables with indices: {table_indices}\")\n    selected_tables = self.select_tables(all_tables, table_indices, table_header_rowspan)\n    self.logger.debug(f\"Concatenating selected tables with table_id: {table_id}\")\n    spec_table = self.concat_tables(selected_tables, table_id=table_id)\n    self.logger.debug(f\"Returning spec_table with header: {spec_table.get('header', [])}\")\n    self.logger.debug(f\"Returning spec_table with data: {spec_table.get('data', [])}\")\n\n    return spec_table\n</code></pre>"},{"location":"api/pdf_doc_handler/#dcmspec.pdf_doc_handler.PDFDocHandler.select_tables","title":"<code>select_tables(tables, table_indices, table_header_rowspan=None)</code>","text":"<p>Select tables referenced by table_indices and split each table into header and data.</p> <p>This method processes a list of extracted tables, selects those specified by table_indices, and splits each selected table into a header and data rows. If a table has multiple header rows (as specified by table_header_rowspan), these rows are merged column-wise to form a single header row.</p> PARAMETER DESCRIPTION <code>tables</code> <p>List of table dicts, each with 'page', 'index', and 'data' (raw table rows).</p> <p> TYPE: <code>List[dict]</code> </p> <code>table_indices</code> <p>List of (page, index) tuples specifying which tables to select and process.</p> <p> TYPE: <code>List[tuple]</code> </p> <code>table_header_rowspan</code> <p>Number of header rows (rowspan) for each table in table_indices, keyed by (page, index). If not specified, defaults to 1 header row per table.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[dict]</code> <p>List[dict]: List of dicts, each with keys: - 'page': page number of the table - 'index': index of the table on the page - 'header': merged header row (list of column names) - 'data': list of data rows (list of cell values)</p> Example <pre><code>selected_tables = handler.select_tables(\n    tables,\n    table_indices=[(10, 0), (11, 1)],\n    table_header_rowspan={(10, 0): 2, (11, 1): 2}\n)\nfor table in selected_tables:\n    print(table[\"header\"], table[\"data\"])\n</code></pre> Source code in <code>src/dcmspec/pdf_doc_handler.py</code> <pre><code>def select_tables(\n    self,\n    tables: List[dict],\n    table_indices: List[tuple],\n    table_header_rowspan: Optional[dict] = None,\n) -&gt; List[dict]:\n    \"\"\"Select tables referenced by table_indices and split each table into header and data.\n\n    This method processes a list of extracted tables, selects those specified by table_indices,\n    and splits each selected table into a header and data rows. If a table has multiple header rows\n    (as specified by table_header_rowspan), these rows are merged column-wise to form a single header row.\n\n\n    Args:\n        tables (List[dict]): List of table dicts, each with 'page', 'index', and 'data' (raw table rows).\n        table_indices (List[tuple]): List of (page, index) tuples specifying which tables to select and process.\n        table_header_rowspan (dict, optional): Number of header rows (rowspan) for each table in table_indices,\n            keyed by (page, index). If not specified, defaults to 1 header row per table.\n\n    Returns:\n        List[dict]: List of dicts, each with keys:\n            - 'page': page number of the table\n            - 'index': index of the table on the page\n            - 'header': merged header row (list of column names)\n            - 'data': list of data rows (list of cell values)\n\n    Example:\n        ```python\n        selected_tables = handler.select_tables(\n            tables,\n            table_indices=[(10, 0), (11, 1)],\n            table_header_rowspan={(10, 0): 2, (11, 1): 2}\n        )\n        for table in selected_tables:\n            print(table[\"header\"], table[\"data\"])\n        ```\n\n    \"\"\"\n    def merge_multirow_header(header_rows):\n        n_cols = max(len(row) for row in header_rows)\n        merged = []\n        for col in range(n_cols):\n            merged_cell = \" \".join(\n                str(row[col]).strip() for row in header_rows\n                if col &lt; len(row) and row[col] not in (None, \"\")\n            ).strip()\n            merged.append(merged_cell)\n        return merged\n\n    selected_tables = []\n    for page, idx in table_indices:\n        for table in tables:\n            if table[\"page\"] == page and table[\"index\"] == idx:\n                table_rows = table[\"data\"]\n                n_header_rows = 1\n                if table_header_rowspan and (page, idx) in table_header_rowspan:\n                    n_header_rows = table_header_rowspan[(page, idx)]\n                header_rows = table_rows[:n_header_rows]\n                data_rows = table_rows[n_header_rows:]\n                # Merge header rows if needed\n                if len(header_rows) == 1:\n                    header_ = header_rows[0]\n                else:\n                    header_ = merge_multirow_header(header_rows)\n                selected_tables.append({\n                    \"page\": page,\n                    \"index\": idx,\n                    \"header\": header_,\n                    \"data\": data_rows,\n                })\n    return selected_tables\n</code></pre>"},{"location":"api/progress/","title":"Progress","text":""},{"location":"api/progress/#dcmspec.progress","title":"<code>dcmspec.progress</code>","text":"<p>Progress tracking classes for monitoring long-running operations in dcmspec.</p>"},{"location":"api/progress/#dcmspec.progress.Progress","title":"<code>Progress</code>","text":"<p>Represent the progress of a long-running operation.</p> PARAMETER DESCRIPTION <code>percent</code> <p>The progress percentage (0-100).</p> <p> TYPE: <code>int</code> </p> <code>status</code> <p>A machine-readable status code (see ProgressStatus enum). Clients are responsible for mapping this code to a user-facing string or UI element.</p> <p> TYPE: <code>ProgressStatus</code> DEFAULT: <code>None</code> </p> <code>step</code> <p>The current step number in a multi-step process (1-based).</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>total_steps</code> <p>The total number of steps in the process.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> Source code in <code>src/dcmspec/progress.py</code> <pre><code>class Progress:\n    \"\"\"Represent the progress of a long-running operation.\n\n    Args:\n        percent (int): The progress percentage (0-100).\n        status (ProgressStatus, optional): A machine-readable status code (see ProgressStatus enum).\n            Clients are responsible for mapping this code to a user-facing string or UI element.\n        step (int, optional): The current step number in a multi-step process (1-based).\n        total_steps (int, optional): The total number of steps in the process.\n\n    \"\"\"\n\n    def __init__(self, percent: int, status: 'ProgressStatus' = None, step: int = None, total_steps: int = None):\n        \"\"\"Initialize the progress private attributes.\n\n        This class is immutable: the percent value is set at initialization and should not be changed.\n        To report new progress, create a new Progress instance.\n\n        Args:\n            percent (int): The progress percentage (0-100).\n            status (ProgressStatus, optional): A status code indicating the current operation.\n            step (int, optional): The current step number in a multi-step process (1-based).\n            total_steps (int, optional): The total number of steps in the process.\n\n        \"\"\"\n        self._percent = percent\n        self._status = status\n        self._step = step\n        self._total_steps = total_steps\n\n    @property\n    def percent(self) -&gt; int:\n        \"\"\"Get the progress percentage.\"\"\"\n        return self._percent\n\n    @property\n    def status(self) -&gt; Optional['ProgressStatus']:\n        \"\"\"Get the progress status.\"\"\"\n        return self._status\n\n    @property\n    def step(self) -&gt; Optional[int]:\n        \"\"\"Get the current step number.\"\"\"\n        return self._step\n\n    @property\n    def total_steps(self) -&gt; Optional[int]:\n        \"\"\"Get the total number of steps.\"\"\"\n        return self._total_steps\n\n    def __setattr__(self, name, value):\n        \"\"\"Prevent modification of attributes after initialization.\"\"\"\n        if hasattr(self, name):\n            raise AttributeError(f\"{self.__class__.__name__} is immutable. Cannot modify '{name}'.\")\n        super().__setattr__(name, value)\n</code></pre>"},{"location":"api/progress/#dcmspec.progress.Progress.percent","title":"<code>percent</code>  <code>property</code>","text":"<p>Get the progress percentage.</p>"},{"location":"api/progress/#dcmspec.progress.Progress.status","title":"<code>status</code>  <code>property</code>","text":"<p>Get the progress status.</p>"},{"location":"api/progress/#dcmspec.progress.Progress.step","title":"<code>step</code>  <code>property</code>","text":"<p>Get the current step number.</p>"},{"location":"api/progress/#dcmspec.progress.Progress.total_steps","title":"<code>total_steps</code>  <code>property</code>","text":"<p>Get the total number of steps.</p>"},{"location":"api/progress/#dcmspec.progress.Progress.__init__","title":"<code>__init__(percent, status=None, step=None, total_steps=None)</code>","text":"<p>Initialize the progress private attributes.</p> <p>This class is immutable: the percent value is set at initialization and should not be changed. To report new progress, create a new Progress instance.</p> PARAMETER DESCRIPTION <code>percent</code> <p>The progress percentage (0-100).</p> <p> TYPE: <code>int</code> </p> <code>status</code> <p>A status code indicating the current operation.</p> <p> TYPE: <code>ProgressStatus</code> DEFAULT: <code>None</code> </p> <code>step</code> <p>The current step number in a multi-step process (1-based).</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>total_steps</code> <p>The total number of steps in the process.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> Source code in <code>src/dcmspec/progress.py</code> <pre><code>def __init__(self, percent: int, status: 'ProgressStatus' = None, step: int = None, total_steps: int = None):\n    \"\"\"Initialize the progress private attributes.\n\n    This class is immutable: the percent value is set at initialization and should not be changed.\n    To report new progress, create a new Progress instance.\n\n    Args:\n        percent (int): The progress percentage (0-100).\n        status (ProgressStatus, optional): A status code indicating the current operation.\n        step (int, optional): The current step number in a multi-step process (1-based).\n        total_steps (int, optional): The total number of steps in the process.\n\n    \"\"\"\n    self._percent = percent\n    self._status = status\n    self._step = step\n    self._total_steps = total_steps\n</code></pre>"},{"location":"api/progress/#dcmspec.progress.Progress.__setattr__","title":"<code>__setattr__(name, value)</code>","text":"<p>Prevent modification of attributes after initialization.</p> Source code in <code>src/dcmspec/progress.py</code> <pre><code>def __setattr__(self, name, value):\n    \"\"\"Prevent modification of attributes after initialization.\"\"\"\n    if hasattr(self, name):\n        raise AttributeError(f\"{self.__class__.__name__} is immutable. Cannot modify '{name}'.\")\n    super().__setattr__(name, value)\n</code></pre>"},{"location":"api/progress/#dcmspec.progress.ProgressObserver","title":"<code>ProgressObserver</code>","text":"<p>Observer for monitoring progress updates.</p> Source code in <code>src/dcmspec/progress.py</code> <pre><code>class ProgressObserver:\n    \"\"\"Observer for monitoring progress updates.\"\"\"\n\n    def __call__(self, progress: Progress) -&gt; None:\n        \"\"\"Handle progress updates.\n\n        Args:\n            progress (Progress): The current progress state.\n\n        \"\"\"\n        # Override in client code or pass a function as observer\n        pass\n</code></pre>"},{"location":"api/progress/#dcmspec.progress.ProgressObserver.__call__","title":"<code>__call__(progress)</code>","text":"<p>Handle progress updates.</p> PARAMETER DESCRIPTION <code>progress</code> <p>The current progress state.</p> <p> TYPE: <code>Progress</code> </p> Source code in <code>src/dcmspec/progress.py</code> <pre><code>def __call__(self, progress: Progress) -&gt; None:\n    \"\"\"Handle progress updates.\n\n    Args:\n        progress (Progress): The current progress state.\n\n    \"\"\"\n    # Override in client code or pass a function as observer\n    pass\n</code></pre>"},{"location":"api/progress/#dcmspec.progress.ProgressStatus","title":"<code>ProgressStatus</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of progress statuses.</p> <p>This enum defines the various states that a long-running operation can be in.</p> Name Value Description DOWNLOADING auto() Generic download (e.g., a document) DOWNLOADING_IOD auto() Downloading the IOD specification document (Part 3) PARSING_TABLE auto() Parsing a DICOM table PARSING_IOD_MODULE_LIST auto() Parsing the list of modules in the IOD PARSING_IOD_MODULES auto() Parsing the IOD modules SAVING_MODEL auto() Saving a specification model to disk SAVING_IOD_MODEL auto() Saving the IOD model to disk Example <p>In your application, you can use ProgressStatus to present progress information to users:</p> <pre><code>from dcmspec.progress import ProgressStatus\n\ndef progress_observer(progress):\n    if progress.status == ProgressStatus.DOWNLOADING_IOD:\n        print(f\"Downloading IOD... {progress.percent}%\")\n</code></pre> Source code in <code>src/dcmspec/progress.py</code> <pre><code>class ProgressStatus(Enum):\n    \"\"\"Enumeration of progress statuses.\n\n    This enum defines the various states that a long-running operation can be in.\n\n    | Name                    | Value | Description                                            |\n    |-------------------------|--------|-------------------------------------------------------|\n    | DOWNLOADING             | auto() | Generic download (e.g., a document)                   |\n    | DOWNLOADING_IOD         | auto() | Downloading the IOD specification document (Part 3)   |\n    | PARSING_TABLE           | auto() | Parsing a DICOM table                                 |\n    | PARSING_IOD_MODULE_LIST | auto() | Parsing the list of modules in the IOD                |\n    | PARSING_IOD_MODULES     | auto() | Parsing the IOD modules                               |\n    | SAVING_MODEL            | auto() | Saving a specification model to disk                  |\n    | SAVING_IOD_MODEL        | auto() | Saving the IOD model to disk                          \n\n    Example:\n        In your application, you can use ProgressStatus to present progress information to users:\n\n        ```python\n        from dcmspec.progress import ProgressStatus\n\n        def progress_observer(progress):\n            if progress.status == ProgressStatus.DOWNLOADING_IOD:\n                print(f\"Downloading IOD... {progress.percent}%\")\n        ```\n\n    \"\"\"\n\n    DOWNLOADING = auto()  # Generic download (e.g., a document)\n    DOWNLOADING_IOD = auto()  # Downloading the IOD specification document (Part 3)\n    PARSING_TABLE = auto()  # Parsing a DICOM table\n    PARSING_IOD_MODULE_LIST = auto()  # Parsing the list of modules in the IOD\n    PARSING_IOD_MODULES = auto()  # Parsing the IOD modules\n    SAVING_MODEL = auto()  # Saving a specification model to disk\n    SAVING_IOD_MODEL = auto()  # Saving the IOD model to disk\n</code></pre>"},{"location":"api/progress/#dcmspec.progress.adapt_progress_observer","title":"<code>adapt_progress_observer(observer)</code>","text":"<p>Wrap a progress observer or callback so it can accept either a Progress object or an int percent.</p> <p>This function provides backward compatibility for legacy progress callbacks that expect an integer percent value. If the observer is a plain function that takes a single argument (typed as <code>int</code> or untyped), it will be wrapped so that it receives <code>progress.percent</code> instead of the Progress object. A DeprecationWarning is issued when this legacy usage occurs.</p> <p>Only plain functions are wrapped; class instances or callables are left unchanged to avoid interfering with class-based observers that expect a Progress object.</p> PARAMETER DESCRIPTION <code>observer</code> <p>The progress observer or callback.</p> <p> TYPE: <code>callable or None</code> </p> RETURNS DESCRIPTION <code>Optional[Callable[[Progress], None]]</code> <p>callable or None: An observer that always accepts a Progress object, or a wrapper that calls the</p> <code>Optional[Callable[[Progress], None]]</code> <p>original callback with progress.percent if it expects an int.</p> Example Source code in <code>src/dcmspec/progress.py</code> <pre><code>def adapt_progress_observer(observer: Optional[Callable]) -&gt; Optional[Callable[[Progress], None]]:\n    \"\"\"Wrap a progress observer or callback so it can accept either a Progress object or an int percent.\n\n    This function provides backward compatibility for legacy progress callbacks that expect\n    an integer percent value. If the observer is a plain function that takes a single argument\n    (typed as `int` or untyped), it will be wrapped so that it receives `progress.percent`\n    instead of the Progress object. A DeprecationWarning is issued when this legacy usage occurs.\n\n    Only plain functions are wrapped; class instances or callables are left unchanged to avoid\n    interfering with class-based observers that expect a Progress object.\n\n    Args:\n        observer (callable or None): The progress observer or callback.\n\n    Returns:\n        callable or None: An observer that always accepts a Progress object, or a wrapper that calls the\n        original callback with progress.percent if it expects an int.\n\n    Example:\n        # Legacy callback (int)\n        def my_callback(percent):\n            print(f\"Progress: {percent}%\")\n\n        # New-style callback (Progress)\n        def my_observer(progress):\n            print(f\"Progress: {progress.percent}%\")\n\n    \"\"\"\n    if observer is None:\n        return None\n    if isinstance(observer, types.FunctionType):\n        sig = inspect.signature(observer)\n        params = list(sig.parameters.values())\n        if len(params) == 1:\n            param = params[0]\n            if param.annotation in (int, inspect._empty):\n                warned = {\"emitted\": False}\n                def wrapper(progress):\n                    \"\"\"Create a closure that adapts a legacy int progress callback to a Progress observer.\n\n                    The original observer function is captured in this closure \u2014 a nested function that remembers\n                    variables from the outer scope \u2014 so the returned wrapper will always call the correct callback.\n                    This allows us to adapt a legacy int callback to a Progress observer without using a class.\n                    \"\"\"\n                    if not warned[\"emitted\"]:\n                        warnings.warn(\n                            \"Passing a progress callback that accepts an int is deprecated. \"\n                            \"Update your callback to accept a Progress object.\",\n                            DeprecationWarning,\n                            stacklevel=2\n                        )\n                        warned[\"emitted\"] = True\n                    return observer(progress.percent)\n                return wrapper\n    return observer\n</code></pre>"},{"location":"api/progress/#dcmspec.progress.adapt_progress_observer--legacy-callback-int","title":"Legacy callback (int)","text":"<p>def my_callback(percent):     print(f\"Progress: {percent}%\")</p>"},{"location":"api/progress/#dcmspec.progress.adapt_progress_observer--new-style-callback-progress","title":"New-style callback (Progress)","text":"<p>def my_observer(progress):     print(f\"Progress: {progress.percent}%\")</p>"},{"location":"api/progress/#dcmspec.progress.add_progress_step","title":"<code>add_progress_step(step, total_steps, status=None)</code>","text":"<p>Define a decorator to enrich progress events with step, total_steps, and optionally status.</p> Source code in <code>src/dcmspec/progress.py</code> <pre><code>def add_progress_step(\n    step: int, total_steps: int, status: Optional[ProgressStatus] = None\n) -&gt; Callable[[Callable[[Progress], None]], Callable[[Progress], None]]:\n    \"\"\"Define a decorator to enrich progress events with step, total_steps, and optionally status.\"\"\"\n    def decorator(observer: Callable[[Progress], None]) -&gt; Callable[[Progress], None]:\n        def wrapper(progress: Progress) -&gt; None:\n            \"\"\"Wrap the observer to include step, total_steps, and optionally status in the Progress object.\"\"\"\n            enriched = Progress(\n                progress.percent,\n                status=status if status is not None else progress.status,\n                step=step,\n                total_steps=total_steps\n            )\n            observer(enriched)\n        return wrapper\n    return decorator\n</code></pre>"},{"location":"api/progress/#dcmspec.progress.calculate_percent","title":"<code>calculate_percent(downloaded, total)</code>","text":"<p>Calculate percent complete, rounded, or -1 if total is unknown/invalid.</p> Source code in <code>src/dcmspec/progress.py</code> <pre><code>def calculate_percent(downloaded: int, total: int) -&gt; int:\n    \"\"\"Calculate percent complete, rounded, or -1 if total is unknown/invalid.\"\"\"\n    if not total or total &lt;= 0:\n        return -1\n    return min(round(downloaded * 100 / total), 100)\n</code></pre>"},{"location":"api/progress/#dcmspec.progress.handle_legacy_callback","title":"<code>handle_legacy_callback(progress_observer=None, progress_callback=None)</code>","text":"<p>Resolve and return a progress_observer, handling legacy progress_callback and warning if both are provided.</p> <p>If both are provided, only progress_observer is used and a warning is issued. If only progress_callback is provided, it is adapted to a progress_observer.</p> Source code in <code>src/dcmspec/progress.py</code> <pre><code>def handle_legacy_callback(\n    progress_observer: Optional[Callable[[Progress], None]] = None,\n    progress_callback: Optional[Callable[[int], None]] = None,\n) -&gt; Optional[Callable[[Progress], None]]:\n    \"\"\"Resolve and return a progress_observer, handling legacy progress_callback and warning if both are provided.\n\n    If both are provided, only progress_observer is used and a warning is issued.\n    If only progress_callback is provided, it is adapted to a progress_observer.\n    \"\"\"\n    if progress_observer is not None and progress_callback is not None:\n        warnings.warn(\n            \"Both progress_observer and progress_callback were provided. \"\n            \"This is not supported: only progress_observer will be used and progress_callback will be ignored. \"\n            \"Do not pass both. progress_callback is deprecated and will be removed in a future release.\",\n            DeprecationWarning,\n            stacklevel=2\n        )\n    if progress_observer is None and progress_callback is not None:\n        from dcmspec.progress import adapt_progress_observer\n        return adapt_progress_observer(progress_callback)\n    return progress_observer\n</code></pre>"},{"location":"api/progress/#dcmspec.progress.offset_progress_steps","title":"<code>offset_progress_steps(step_offset, total_steps)</code>","text":"<p>Define a decorator to offset progress steps by a fixed amount.</p> Source code in <code>src/dcmspec/progress.py</code> <pre><code>def offset_progress_steps(\n    step_offset: int, total_steps: int\n) -&gt; Callable[[Callable[[Progress], None]], Callable[[Progress], None]]:\n    \"\"\"Define a decorator to offset progress steps by a fixed amount.\"\"\"\n    def decorator(observer: Callable[[Progress], None]) -&gt; Callable[[Progress], None]:\n        def wrapper(progress: Progress) -&gt; None:\n            # Only update step/total_steps if they are set\n            step = progress.step + step_offset if progress.step is not None else None\n            observer(\n                Progress(\n                    progress.percent,\n                    status=progress.status,\n                    step=step,\n                    total_steps=total_steps\n                )\n            )\n        return wrapper\n    return decorator\n</code></pre>"},{"location":"api/service_attribute_defaults/","title":"Service Attribute Defaults","text":""},{"location":"api/service_attribute_defaults/#dcmspec.service_attribute_defaults","title":"<code>dcmspec.service_attribute_defaults</code>","text":"<p>Default name_attr, column_to_attr and DIMSE mappings for DICOM Services attribute specification tables.</p> <ul> <li>UPS:  For PS3.4 CC.2.5.1 Unified Procedure Step.</li> <li>MPPS: For PS3.4 F.7.2.1 Modality Performed Procedure Step.</li> </ul> <p>Each mapping provides a default <code>column_to_attr</code> dictionary, a corresponding DIMSE mapping dictionary and <code>name_attr</code> string for use with ServiceAttributeModel and related classes.</p> Note <p>These mappings are designed to be used together. If you use custom attribute names, you must adapt both mappings accordingly.</p> Note <p>These constants are intended as read-only defaults. If you need to modify a mapping, make a copy first (e.g., <code>UPS_COLUMNS_MAPPING.copy()</code> or <code>copy.deepcopy(UPS_DIMSE_MAPPING)</code>). Modifying the shared constants directly can lead to unexpected behavior elsewhere in your code. Only the dictionary constants (e.g., <code>*_COLUMNS_MAPPING</code>, <code>*_DIMSE_MAPPING</code>) need to be copied. String constants (e.g., <code>*_NAME_ATTR</code>) are immutable and do not need to be copied.</p> <p>Example usage: <pre><code>from dcmspec.service_attribute_defaults import UPS_DIMSE_MAPPING, UPS_COLUMNS_MAPPING, UPS_NAME_ATTR\nfactory = SpecFactory(\n    model_class=ServiceAttributeModel,\n    input_handler=UPSXHTMLDocHandler(),\n    column_to_attr=UPS_COLUMNS_MAPPING.copy(),\n    name_attr=UPS_NAME_ATTR,\n    config=config\n)\nmodel = factory.create_model(\n    url=url,\n    cache_file_name=cache_file_name,\n    table_id=table_id,\n    force_download=False,\n    model_kwargs={\"dimse_mapping\": copy.deepcopy(UPS_DIMSE_MAPPING)},\n    )\n</code></pre></p>"},{"location":"api/service_attribute_defaults/#dcmspec.service_attribute_defaults.MPPS_COLUMNS_MAPPING","title":"<code>MPPS_COLUMNS_MAPPING = {0: 'elem_name', 1: 'elem_tag', 2: 'dimse_ncreate', 3: 'dimse_nset', 4: 'dimse_final'}</code>  <code>module-attribute</code>","text":"<p>dict: Default column-to-attribute mapping for MPPS attribute tables.</p>"},{"location":"api/service_attribute_defaults/#dcmspec.service_attribute_defaults.MPPS_DIMSE_MAPPING","title":"<code>MPPS_DIMSE_MAPPING = {'ALL_DIMSE': {'attributes': ['dimse_ncreate', 'dimse_nset', 'dimse_final']}, 'N-CREATE': {'attributes': ['dimse_ncreate'], 'req_attributes': ['dimse_ncreate'], 'req_separator': '/'}, 'N-SET': {'attributes': ['dimse_nset'], 'req_attributes': ['dimse_nset'], 'req_separator': '/'}, 'FINAL': {'attributes': ['dimse_final'], 'req_attributes': ['dimse_final']}}</code>  <code>module-attribute</code>","text":"<p>dict: Default DIMSE mapping for MPPS attribute tables.</p>"},{"location":"api/service_attribute_defaults/#dcmspec.service_attribute_defaults.MPPS_NAME_ATTR","title":"<code>MPPS_NAME_ATTR = 'elem_name'</code>  <code>module-attribute</code>","text":"<p>str: Default name_attr for MPPS attribute tables.</p>"},{"location":"api/service_attribute_defaults/#dcmspec.service_attribute_defaults.UPS_COLUMNS_MAPPING","title":"<code>UPS_COLUMNS_MAPPING = {0: 'elem_name', 1: 'elem_tag', 2: 'dimse_ncreate', 3: 'dimse_nset', 4: 'dimse_final', 5: 'dimse_nget', 6: 'key_matching', 7: 'key_return', 8: 'type_remark'}</code>  <code>module-attribute</code>","text":"<p>dict: Default column-to-attribute mapping for UPS attribute tables.</p>"},{"location":"api/service_attribute_defaults/#dcmspec.service_attribute_defaults.UPS_DIMSE_MAPPING","title":"<code>UPS_DIMSE_MAPPING = {'ALL_DIMSE': {'attributes': ['dimse_ncreate', 'dimse_nset', 'dimse_final', 'dimse_nget', 'key_matching', 'key_return', 'type_remark']}, 'N-CREATE': {'attributes': ['dimse_ncreate', 'type_remark'], 'req_attributes': ['dimse_ncreate'], 'req_separator': '/'}, 'N-SET': {'attributes': ['dimse_nset', 'type_remark'], 'req_attributes': ['dimse_nset'], 'req_separator': '/'}, 'N-GET': {'attributes': ['dimse_nget', 'type_remark'], 'req_attributes': ['dimse_nget'], 'req_separator': '/'}, 'C-FIND': {'attributes': ['key_matching', 'key_return', 'type_remark'], 'req_attributes': ['key_matching', 'key_return']}, 'FINAL': {'attributes': ['dimse_final', 'type_remark'], 'req_attributes': ['dimse_final']}}</code>  <code>module-attribute</code>","text":"<p>dict: Default DIMSE mapping for UPS attribute tables.</p>"},{"location":"api/service_attribute_defaults/#dcmspec.service_attribute_defaults.UPS_NAME_ATTR","title":"<code>UPS_NAME_ATTR = 'elem_name'</code>  <code>module-attribute</code>","text":"<p>str: Default name_attr for UPS attribute tables.</p>"},{"location":"api/service_attribute_model/","title":"ServiceAttributeModel","text":""},{"location":"api/service_attribute_model/#dcmspec.service_attribute_model.ServiceAttributeModel","title":"<code>dcmspec.service_attribute_model.ServiceAttributeModel</code>","text":"<p>               Bases: <code>SpecModel</code></p> <p>A model for DICOM Services with mixed DIMSE and role requirements.</p> <p>ServiceAttributeModel extends SpecModel to support selection and filtering of attributes and columns based on DIMSE service and role, using a provided mapping. It enables extraction of service- and role-specific attribute sets from tables where multiple DIMSE Services and Roles are combined.</p> Source code in <code>src/dcmspec/service_attribute_model.py</code> <pre><code>class ServiceAttributeModel(SpecModel):\n    \"\"\"A model for DICOM Services with mixed DIMSE and role requirements.\n\n    ServiceAttributeModel extends SpecModel to support selection and filtering of attributes\n    and columns based on DIMSE service and role, using a provided mapping. It enables\n    extraction of service- and role-specific attribute sets from tables where multiple\n    DIMSE Services and Roles are combined.\n    \"\"\"\n\n    def __init__(\n        self,\n        metadata: Node,\n        content: Node,\n        dimse_mapping: dict,\n        logger: Optional[logging.Logger] = None\n    ) -&gt; None:\n        \"\"\"Initialize the ServiceAttributeModel.\n\n        Sets up the model with metadata, content, and a DIMSE mapping for filtering.\n        Initializes the DIMSE and role selection to None.\n\n        The `dimse_mapping` argument should be a dictionary with the following structure:\n\n        ```python\n        {\n            \"ALL_DIMSE\": {\n                \"attributes\": [&lt;attribute_name&gt;, ...]\n            },\n            \"&lt;DIMSE&gt;\": {\n                \"attributes\": [&lt;attribute_name&gt;, ...],\n                \"req_attributes\": [&lt;attribute_name&gt;, ...],  # optional, for role-based requirements\n                \"req_separator\": \"&lt;separator&gt;\",             # optional, e.g. \"/\"\n            },\n            ...\n        }\n        ```\n\n        Args:\n            metadata (Node): Node holding table and document metadata.\n            content (Node): Node holding the hierarchical content tree of the DICOM specification.\n            dimse_mapping (dict): Dictionary defining DIMSE and role-based attribute requirements.\n            logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n\n        Example:\n            ```python\n            UPS_DIMSE_MAPPING = {\n                \"ALL_DIMSE\": {\n                    \"attributes\": [\n                        \"dimse_ncreate\", \"dimse_nset\", \"dimse_final\", \"dimse_nget\",\n                        \"key_matching\", \"key_return\", \"type_remark\"\n                    ]\n                },\n                \"N-CREATE\": {\n                    \"attributes\": [\"dimse_ncreate\", \"type_remark\"],\n                    \"req_attributes\": [\"dimse_ncreate\"],\n                    \"req_separator\": \"/\"\n                },\n                \"N-SET\": {\n                    \"attributes\": [\"dimse_nset\", \"type_remark\"],\n                    \"req_attributes\": [\"dimse_nset\"],\n                    \"req_separator\": \"/\"\n                },\n                \"N-GET\": {\n                    \"attributes\": [\"dimse_nget\", \"type_remark\"],\n                    \"req_attributes\": [\"dimse_nget\"],\n                    \"req_separator\": \"/\"\n                },\n                \"C-FIND\": {\n                    \"attributes\": [\"key_matching\", \"key_return\", \"type_remark\"],\n                    \"req_attributes\": [\"key_matching\", \"key_return\"]\n                },\n                \"FINAL\": {\n                    \"attributes\": [\"dimse_final\", \"type_remark\"],\n                    \"req_attributes\": [\"dimse_final\"]\n                },\n            }\n            model = ServiceAttributeModel(metadata, content, UPS_DIMSE_MAPPING)\n            ```\n\n        \"\"\"\n        super().__init__(metadata, content, logger=logger)\n        self.DIMSE_MAPPING = dimse_mapping\n        self.dimse = None\n        self.role = None\n\n\n    def select_dimse(self, dimse: str) -&gt; None:\n        \"\"\"Filter the model to only retain attributes relevant to the specified DIMSE SOP Class.\n\n        This method updates the model so that only the attributes required for the selected\n        DIMSE are kept. All other DIMSE-specific attributes are removed from the model,\n        while other attributes not listed in ALL_DIMSE are retained. This enables extraction\n        of a DIMSE-specific attribute set from a combined table. The model's metadata is also\n        updated to reflect the retained attributes.\n\n        Args:\n            dimse (str): The key of DIMSE_MAPPING to select.\n\n        \"\"\"\n        if dimse not in self.DIMSE_MAPPING:\n            self.logger.warning(f\"DIMSE '{dimse}' not found in DIMSE_MAPPING\")\n            return\n        self.dimse = dimse\n\n        dimse_info = self.DIMSE_MAPPING[dimse]\n        all_dimse_info = self.DIMSE_MAPPING[\"ALL_DIMSE\"]\n\n        # Determine which columns/attributes to keep for this DIMSE\n        dimse_attributes = dimse_info.get(\"attributes\", [])\n        all_attributes = all_dimse_info.get(\"attributes\", [])\n\n        self._filter_node_attributes(dimse_attributes, all_attributes)\n        self._update_metadata_for_dimse(dimse_attributes, all_attributes)\n\n\n    def _filter_node_attributes(self, dimse_attributes: Sequence[str], all_attributes: Sequence[str]) -&gt; None:\n        \"\"\"Remove DIMSE attributes that are not belonging to the selected DIMSE.\"\"\"\n        for node in PreOrderIter(self.content):\n            for attr in list(node.__dict__.keys()):\n                # Retaining non-DIMSE attributes (not in ALL_DIMSE)\n                if attr in all_attributes and attr not in dimse_attributes:\n                    delattr(node, attr)\n\n    def _update_metadata_for_dimse(self, dimse_attributes: Sequence[str], all_attributes: Sequence[str]) -&gt; None:\n        if hasattr(self.metadata, \"header\") and hasattr(self.metadata, \"column_to_attr\"):\n            # Build new header and mapping, keeping original indices for column_to_attr\n            new_header = []\n            new_column_to_attr = {}\n            for i, cell in enumerate(self.metadata.header):\n                # Only keep columns that are in the selected DIMSE or not in ALL_DIMSE\n                if i in self.metadata.column_to_attr:\n                    attr = self.metadata.column_to_attr[i]\n                    if (attr in dimse_attributes) or (attr not in all_attributes):\n                        new_header.append(cell)\n                        new_column_to_attr[i] = attr\n            self.metadata.header = new_header\n            self.metadata.column_to_attr = new_column_to_attr\n        elif hasattr(self.metadata, \"column_to_attr\"):\n            # Only update column_to_attr if no header in metadata\n            self.metadata.column_to_attr = {\n                key: value\n                for key, value in self.metadata.column_to_attr.items()\n                if (value in dimse_attributes) or (value not in all_attributes)\n            }\n\n    def select_role(self, role: str) -&gt; None:\n        \"\"\"Filter the model to only retain requirements for a specific role (SCU or SCP) of the selected DIMSE.\n\n        This method updates the model so that, for attributes with role-specific requirements (e.g., \"SCU/SCP\"),\n        only the requirements relevant to the selected role are retained. For example, if a attribute contains\n        \"1/2\", selecting \"SCU\" will keep \"1\" and selecting \"SCP\" will keep \"2\". Any additional comments\n        after a newline are preserved in a separate \"comment\" attribute. The model's metadata is also\n        updated to reflect the changes in attributes.\n\n        Args:\n            role (str): The role to filter for (\"SCU\" or \"SCP\").\n\n        Note:\n            You must call select_dimse() before calling select_role(), or a RuntimeError will be raised.\n\n        Note:\n            For DIMSEs that do not have explicit SCU and SCP requirements (i.e., no \"req_separator\" specified\n            in the mapping), this function may have no effect and will not modify the model.\n\n        Raises:\n            RuntimeError: If select_dimse was not called before select_role.\n\n        \"\"\"\n        if role is None:\n            return\n        if self.dimse is None or self.dimse == \"ALL_DIMSE\":\n            raise RuntimeError(\"select_dimse must be called before select_role.\")\n        self.role = role\n\n        dimse_info = self.DIMSE_MAPPING[self.dimse]\n        req_attributes = dimse_info.get(\"req_attributes\", [])\n        req_separator = dimse_info.get(\"req_separator\", None)\n\n        comment_needed = self._filter_role_attributes(req_attributes, req_separator, role)\n        self._update_metadata_for_role(comment_needed, role)\n\n    def _filter_role_attributes(self, req_attributes: list, req_separator: str, role: str) -&gt; bool:\n        \"\"\"Filter node attributes for the selected role, handle comments, and return if comment column is needed.\"\"\"\n        comment_needed = False\n        for req_attr in req_attributes:\n            attribute_name = req_attr\n            for node in PreOrderIter(self.content):\n                if hasattr(node, attribute_name):\n                    value = getattr(node, attribute_name)\n                    if not isinstance(value, str):\n                        continue\n                    # Split SCU/SCP optionality requirements and any additional comment\n                    parts = value.split(\"\\n\", 1)\n                    optionality = parts[0]\n                    if len(parts) &gt; 1:\n                        setattr(node, attribute_name, optionality)\n                        setattr(node, \"comment\", parts[1])\n                        comment_needed = True\n                    # Split SCU/SCP optionality requirements\n                    if req_separator and req_separator in optionality:\n                        sub_parts = optionality.split(req_separator, 1)\n                        setattr(node, attribute_name, sub_parts[0] if role == \"SCU\" else sub_parts[1])\n        return comment_needed\n\n    def _update_metadata_for_role(self, comment_needed: bool, role: str) -&gt; None:\n        \"\"\"Update metadata (header and column_to_attr) for role-specific requirements and comments.\"\"\"\n        if comment_needed:\n            if hasattr(self.metadata, \"column_to_attr\") and \"comment\" not in self.metadata.column_to_attr.values():\n                next_key = max(self.metadata.column_to_attr.keys(), default=-1) + 1\n                self.metadata.column_to_attr[next_key] = \"comment\"\n            if hasattr(self.metadata, \"header\") and \"Comment\" not in self.metadata.header:\n                self.metadata.header.append(\"Comment\")\n\n        if hasattr(self.metadata, \"header\"):\n            for i, header in enumerate(self.metadata.header):\n                if \"SCU/SCP\" in header:\n                    self.metadata.header[i] = header.replace(\"SCU/SCP\", role)\n</code></pre>"},{"location":"api/service_attribute_model/#dcmspec.service_attribute_model.ServiceAttributeModel.__init__","title":"<code>__init__(metadata, content, dimse_mapping, logger=None)</code>","text":"<p>Initialize the ServiceAttributeModel.</p> <p>Sets up the model with metadata, content, and a DIMSE mapping for filtering. Initializes the DIMSE and role selection to None.</p> <p>The <code>dimse_mapping</code> argument should be a dictionary with the following structure:</p> <pre><code>{\n    \"ALL_DIMSE\": {\n        \"attributes\": [&lt;attribute_name&gt;, ...]\n    },\n    \"&lt;DIMSE&gt;\": {\n        \"attributes\": [&lt;attribute_name&gt;, ...],\n        \"req_attributes\": [&lt;attribute_name&gt;, ...],  # optional, for role-based requirements\n        \"req_separator\": \"&lt;separator&gt;\",             # optional, e.g. \"/\"\n    },\n    ...\n}\n</code></pre> PARAMETER DESCRIPTION <code>metadata</code> <p>Node holding table and document metadata.</p> <p> TYPE: <code>Node</code> </p> <code>content</code> <p>Node holding the hierarchical content tree of the DICOM specification.</p> <p> TYPE: <code>Node</code> </p> <code>dimse_mapping</code> <p>Dictionary defining DIMSE and role-based attribute requirements.</p> <p> TYPE: <code>dict</code> </p> <code>logger</code> <p>Logger instance to use. If None, a default logger is created.</p> <p> TYPE: <code>Optional[Logger]</code> DEFAULT: <code>None</code> </p> Example <pre><code>UPS_DIMSE_MAPPING = {\n    \"ALL_DIMSE\": {\n        \"attributes\": [\n            \"dimse_ncreate\", \"dimse_nset\", \"dimse_final\", \"dimse_nget\",\n            \"key_matching\", \"key_return\", \"type_remark\"\n        ]\n    },\n    \"N-CREATE\": {\n        \"attributes\": [\"dimse_ncreate\", \"type_remark\"],\n        \"req_attributes\": [\"dimse_ncreate\"],\n        \"req_separator\": \"/\"\n    },\n    \"N-SET\": {\n        \"attributes\": [\"dimse_nset\", \"type_remark\"],\n        \"req_attributes\": [\"dimse_nset\"],\n        \"req_separator\": \"/\"\n    },\n    \"N-GET\": {\n        \"attributes\": [\"dimse_nget\", \"type_remark\"],\n        \"req_attributes\": [\"dimse_nget\"],\n        \"req_separator\": \"/\"\n    },\n    \"C-FIND\": {\n        \"attributes\": [\"key_matching\", \"key_return\", \"type_remark\"],\n        \"req_attributes\": [\"key_matching\", \"key_return\"]\n    },\n    \"FINAL\": {\n        \"attributes\": [\"dimse_final\", \"type_remark\"],\n        \"req_attributes\": [\"dimse_final\"]\n    },\n}\nmodel = ServiceAttributeModel(metadata, content, UPS_DIMSE_MAPPING)\n</code></pre> Source code in <code>src/dcmspec/service_attribute_model.py</code> <pre><code>def __init__(\n    self,\n    metadata: Node,\n    content: Node,\n    dimse_mapping: dict,\n    logger: Optional[logging.Logger] = None\n) -&gt; None:\n    \"\"\"Initialize the ServiceAttributeModel.\n\n    Sets up the model with metadata, content, and a DIMSE mapping for filtering.\n    Initializes the DIMSE and role selection to None.\n\n    The `dimse_mapping` argument should be a dictionary with the following structure:\n\n    ```python\n    {\n        \"ALL_DIMSE\": {\n            \"attributes\": [&lt;attribute_name&gt;, ...]\n        },\n        \"&lt;DIMSE&gt;\": {\n            \"attributes\": [&lt;attribute_name&gt;, ...],\n            \"req_attributes\": [&lt;attribute_name&gt;, ...],  # optional, for role-based requirements\n            \"req_separator\": \"&lt;separator&gt;\",             # optional, e.g. \"/\"\n        },\n        ...\n    }\n    ```\n\n    Args:\n        metadata (Node): Node holding table and document metadata.\n        content (Node): Node holding the hierarchical content tree of the DICOM specification.\n        dimse_mapping (dict): Dictionary defining DIMSE and role-based attribute requirements.\n        logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n\n    Example:\n        ```python\n        UPS_DIMSE_MAPPING = {\n            \"ALL_DIMSE\": {\n                \"attributes\": [\n                    \"dimse_ncreate\", \"dimse_nset\", \"dimse_final\", \"dimse_nget\",\n                    \"key_matching\", \"key_return\", \"type_remark\"\n                ]\n            },\n            \"N-CREATE\": {\n                \"attributes\": [\"dimse_ncreate\", \"type_remark\"],\n                \"req_attributes\": [\"dimse_ncreate\"],\n                \"req_separator\": \"/\"\n            },\n            \"N-SET\": {\n                \"attributes\": [\"dimse_nset\", \"type_remark\"],\n                \"req_attributes\": [\"dimse_nset\"],\n                \"req_separator\": \"/\"\n            },\n            \"N-GET\": {\n                \"attributes\": [\"dimse_nget\", \"type_remark\"],\n                \"req_attributes\": [\"dimse_nget\"],\n                \"req_separator\": \"/\"\n            },\n            \"C-FIND\": {\n                \"attributes\": [\"key_matching\", \"key_return\", \"type_remark\"],\n                \"req_attributes\": [\"key_matching\", \"key_return\"]\n            },\n            \"FINAL\": {\n                \"attributes\": [\"dimse_final\", \"type_remark\"],\n                \"req_attributes\": [\"dimse_final\"]\n            },\n        }\n        model = ServiceAttributeModel(metadata, content, UPS_DIMSE_MAPPING)\n        ```\n\n    \"\"\"\n    super().__init__(metadata, content, logger=logger)\n    self.DIMSE_MAPPING = dimse_mapping\n    self.dimse = None\n    self.role = None\n</code></pre>"},{"location":"api/service_attribute_model/#dcmspec.service_attribute_model.ServiceAttributeModel.select_dimse","title":"<code>select_dimse(dimse)</code>","text":"<p>Filter the model to only retain attributes relevant to the specified DIMSE SOP Class.</p> <p>This method updates the model so that only the attributes required for the selected DIMSE are kept. All other DIMSE-specific attributes are removed from the model, while other attributes not listed in ALL_DIMSE are retained. This enables extraction of a DIMSE-specific attribute set from a combined table. The model's metadata is also updated to reflect the retained attributes.</p> PARAMETER DESCRIPTION <code>dimse</code> <p>The key of DIMSE_MAPPING to select.</p> <p> TYPE: <code>str</code> </p> Source code in <code>src/dcmspec/service_attribute_model.py</code> <pre><code>def select_dimse(self, dimse: str) -&gt; None:\n    \"\"\"Filter the model to only retain attributes relevant to the specified DIMSE SOP Class.\n\n    This method updates the model so that only the attributes required for the selected\n    DIMSE are kept. All other DIMSE-specific attributes are removed from the model,\n    while other attributes not listed in ALL_DIMSE are retained. This enables extraction\n    of a DIMSE-specific attribute set from a combined table. The model's metadata is also\n    updated to reflect the retained attributes.\n\n    Args:\n        dimse (str): The key of DIMSE_MAPPING to select.\n\n    \"\"\"\n    if dimse not in self.DIMSE_MAPPING:\n        self.logger.warning(f\"DIMSE '{dimse}' not found in DIMSE_MAPPING\")\n        return\n    self.dimse = dimse\n\n    dimse_info = self.DIMSE_MAPPING[dimse]\n    all_dimse_info = self.DIMSE_MAPPING[\"ALL_DIMSE\"]\n\n    # Determine which columns/attributes to keep for this DIMSE\n    dimse_attributes = dimse_info.get(\"attributes\", [])\n    all_attributes = all_dimse_info.get(\"attributes\", [])\n\n    self._filter_node_attributes(dimse_attributes, all_attributes)\n    self._update_metadata_for_dimse(dimse_attributes, all_attributes)\n</code></pre>"},{"location":"api/service_attribute_model/#dcmspec.service_attribute_model.ServiceAttributeModel.select_role","title":"<code>select_role(role)</code>","text":"<p>Filter the model to only retain requirements for a specific role (SCU or SCP) of the selected DIMSE.</p> <p>This method updates the model so that, for attributes with role-specific requirements (e.g., \"SCU/SCP\"), only the requirements relevant to the selected role are retained. For example, if a attribute contains \"1/2\", selecting \"SCU\" will keep \"1\" and selecting \"SCP\" will keep \"2\". Any additional comments after a newline are preserved in a separate \"comment\" attribute. The model's metadata is also updated to reflect the changes in attributes.</p> PARAMETER DESCRIPTION <code>role</code> <p>The role to filter for (\"SCU\" or \"SCP\").</p> <p> TYPE: <code>str</code> </p> Note <p>You must call select_dimse() before calling select_role(), or a RuntimeError will be raised.</p> Note <p>For DIMSEs that do not have explicit SCU and SCP requirements (i.e., no \"req_separator\" specified in the mapping), this function may have no effect and will not modify the model.</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If select_dimse was not called before select_role.</p> Source code in <code>src/dcmspec/service_attribute_model.py</code> <pre><code>def select_role(self, role: str) -&gt; None:\n    \"\"\"Filter the model to only retain requirements for a specific role (SCU or SCP) of the selected DIMSE.\n\n    This method updates the model so that, for attributes with role-specific requirements (e.g., \"SCU/SCP\"),\n    only the requirements relevant to the selected role are retained. For example, if a attribute contains\n    \"1/2\", selecting \"SCU\" will keep \"1\" and selecting \"SCP\" will keep \"2\". Any additional comments\n    after a newline are preserved in a separate \"comment\" attribute. The model's metadata is also\n    updated to reflect the changes in attributes.\n\n    Args:\n        role (str): The role to filter for (\"SCU\" or \"SCP\").\n\n    Note:\n        You must call select_dimse() before calling select_role(), or a RuntimeError will be raised.\n\n    Note:\n        For DIMSEs that do not have explicit SCU and SCP requirements (i.e., no \"req_separator\" specified\n        in the mapping), this function may have no effect and will not modify the model.\n\n    Raises:\n        RuntimeError: If select_dimse was not called before select_role.\n\n    \"\"\"\n    if role is None:\n        return\n    if self.dimse is None or self.dimse == \"ALL_DIMSE\":\n        raise RuntimeError(\"select_dimse must be called before select_role.\")\n    self.role = role\n\n    dimse_info = self.DIMSE_MAPPING[self.dimse]\n    req_attributes = dimse_info.get(\"req_attributes\", [])\n    req_separator = dimse_info.get(\"req_separator\", None)\n\n    comment_needed = self._filter_role_attributes(req_attributes, req_separator, role)\n    self._update_metadata_for_role(comment_needed, role)\n</code></pre>"},{"location":"api/spec_factory/","title":"SpecFactory","text":""},{"location":"api/spec_factory/#dcmspec.spec_factory.SpecFactory","title":"<code>dcmspec.spec_factory.SpecFactory</code>","text":"<p>Factory for DICOM specification models.</p> <p>Orchestrates the end-to-end process of producing structured SpecModel objects from DICOM specification sources. This includes coordinating input handlers for downloading and caching files, table parsers for extracting structured data into SpecModel objects, and model stores for caching and loading models. Supports flexible configuration and caching strategies.</p> Typical usage <p>factory = SpecFactory(...) model = factory.create_model(...)</p> Source code in <code>src/dcmspec/spec_factory.py</code> <pre><code>class SpecFactory:\n    \"\"\"Factory for DICOM specification models.\n\n    Orchestrates the end-to-end process of producing structured SpecModel objects from DICOM specification sources.\n    This includes coordinating input handlers for downloading and caching files, table parsers for extracting\n    structured data into SpecModel objects, and model stores for caching and loading models.\n    Supports flexible configuration and caching strategies.\n\n    Typical usage:\n        factory = SpecFactory(...)\n        model = factory.create_model(...)\n    \"\"\"\n\n    def __init__(\n        self,\n        input_handler: Optional[DocHandler] = None,\n        model_class: Optional[Type[SpecModel]] = None,\n        model_store: Optional[SpecStore] = None,\n        table_parser: Optional[SpecParser] = None,\n        column_to_attr: Optional[Dict[int, str]] = None,\n        name_attr: Optional[str] = None,\n        config: Optional[Config] = None,\n        logger: Optional[logging.Logger] = None,\n        parser_kwargs: Optional[Dict[str, Any]] = None,\n    ):\n        \"\"\"Initialize the SpecFactory.\n\n        The default values for `column_to_attr` and `name_attr` are designed for parsing\n        DICOM PS3.3 module attribute tables, where columns typically represent element name,\n        tag, type, and description.\n\n        Args:\n            input_handler (Optional[DocHandler]): Handler for downloading and parsing input files.\n                If None, a default XHTMLDocHandler is used.\n            model_class (Optional[Type[SpecModel]]): The class to instantiate for the model.\n                If None, defaults to SpecModel.\n            model_store (Optional[SpecStore]): Store for loading and saving models.\n                If None, a default JSONSpecStore is used.\n            table_parser (Optional[SpecParser]): Parser for extracting tables from documents.\n                If None, a default DOMTableSpecParser is used.\n            column_to_attr (Optional[Dict[int, str]]): Mapping from column indices to names of attributes\n                of model nodes. If None, a default mapping is used.\n            name_attr (Optional[str]): Attribute name to use for node names in the model.\n                If None, defaults to \"elem_name\".\n            config (Optional[Config]): Configuration object. If None, a default Config is created.\n            logger (Optional[logging.Logger]): Logger instance to use.\n                If None, a default logger is created.\n            parser_kwargs (Optional[Dict[str, Any]]): Default keyword arguments to pass to the parser's\n                `parse` method. Use this to supply parser-specific options such as `skip_columns` or `unformatted`.\n\n        Raises:\n            TypeError: If config is not a Config instance or None.\n\n        Note:\n            `column_to_attr` and related flags (such as `unformatted`) are dicts to support non-sequential\n            column mappings.\n\n        \"\"\"\n        import logging\n        if config is not None and not isinstance(config, Config):\n            raise TypeError(\"config must be an instance of Config or None\")\n        self.config = config or Config()\n\n        self.logger = logger or logging.getLogger(self.__class__.__name__)\n\n        self.model_class = model_class or SpecModel\n        self.input_handler = input_handler or XHTMLDocHandler(config=self.config, logger=self.logger)\n        self.model_store = model_store or JSONSpecStore(logger=self.logger)\n        self.table_parser = table_parser or DOMTableSpecParser(logger=self.logger)\n        self.column_to_attr = column_to_attr or {0: \"elem_name\", 1: \"elem_tag\", 2: \"elem_type\", 3: \"elem_description\"}\n        self.name_attr = name_attr or \"elem_name\"\n        self.parser_kwargs = parser_kwargs or {}\n\n    def load_document(self, \n                    url: str, \n                    cache_file_name: str, \n                    force_download: bool = False, \n                    progress_observer: 'Optional[ProgressObserver]' = None,\n                    # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n                    progress_callback: 'Optional[Callable[[int], None]]' = None,\n                    # END LEGACY SUPPORT\n                    ) -&gt; Any:\n        \"\"\"Download, cache, and parse the specification file from a URL, returning the document object.\n\n        Args:\n            url (str): The URL to download the input file from.\n            cache_file_name (str): Filename of the cached input file.\n            force_download (bool): If True, always download the input file even if cached.\n            progress_observer (Optional[ProgressObserver]): Optional observer to report download progress.\n            progress_callback (Optional[Callable[[int], None]]): [LEGACY] Optional callback to report progress\n                Deprecated: use progress_observer instead. Will be removed in a future release.\n\n        Returns:\n            Any: The document object.\n\n        Note:\n            Progress reporting via progress_observer is delegated to the input handler (e.g., XHTMLDocHandler,\n            PDFDocHandler) and typically covers only downloading and caching (writing to disk).\n            Parsing and model building do not emit progress updates.\n\n        \"\"\"\n        # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n        progress_observer = handle_legacy_callback(progress_observer, progress_callback)\n        # END LEGACY SUPPORT\n        # This will download if needed and always parse/return the DOM\n        return self.input_handler.load_document(cache_file_name=cache_file_name,\n                                                url=url,\n                                                force_download=force_download,\n                                                progress_observer=progress_observer\n                                                )\n\n    def try_load_cache(\n        self,\n        json_file_name: Optional[str],\n        include_depth: Optional[int],\n        model_kwargs: Optional[Dict[str, Any]],\n        force_parse: bool = False,\n    ) -&gt; Optional[SpecModel]:\n        \"\"\"Check for and load a model from cache if available and not force_parse.\"\"\"\n        if json_file_name is None:\n            cache_file_name = getattr(self.input_handler, \"cache_file_name\", None)\n            if cache_file_name is None:\n                raise ValueError(\"input_handler.cache_file_name not set\")\n            json_file_name = f\"{os.path.splitext(cache_file_name)[0]}.json\"\n        json_file_path = os.path.join(self.config.get_param(\"cache_dir\"), \"model\", json_file_name)\n        if os.path.exists(json_file_path) and not force_parse:\n            model = self._load_model_from_cache(json_file_path, include_depth, model_kwargs)\n            if model is not None:\n                return model\n        return None\n\n    def build_model(\n        self,\n        doc_object: Any,\n        table_id: Optional[str] = None,\n        url: Optional[str] = None,\n        json_file_name: Optional[str] = None,\n        include_depth: Optional[int] = None,\n        progress_observer: Optional[ProgressObserver] = None,\n        force_parse: bool = False,\n        model_kwargs: Optional[Dict[str, Any]] = None,\n        parser_kwargs: Optional[Dict[str, Any]] = None,\n    ) -&gt; SpecModel:\n        \"\"\"Build and cache a DICOM specification model from a parsed document object.\n\n        Args:\n            doc_object (Any): The parsed document object to be parsed into a model.\n                - For XHTML: a BeautifulSoup DOM object.\n                - For PDF: a grouped table dict (from PDFDocHandler).\n                - For other formats: as defined by the handler/parser.\n            table_id (Optional[str]): Table identifier for model parsing.\n            url (Optional[str]): The URL the document was fetched from (for metadata).\n            json_file_name (Optional[str]): Filename to save the cached JSON model.\n            include_depth (Optional[int]): The depth to which included tables should be parsed.\n            force_parse (bool): If True, always parse and (over)write the JSON cache file.\n            progress_observer (Optional[ProgressObserver]): Optional observer to report download progress.\n                See the Note below for details on the progress events and their properties.\n            model_kwargs (Optional[Dict[str, Any]]): Additional keyword arguments for model construction.\n                Use this to supply extra parameters required by custom SpecModel subclasses.\n                For example, if your model class is `MyModel(metadata, content, foo, bar)`, pass\n                `model_kwargs={\"foo\": foo_value, \"bar\": bar_value}`.\n            parser_kwargs (Optional[Dict[str, Any]]): Additional keyword arguments to pass to the parser's\n                `parse` method. Use this to supply parser-specific options such as `skip_columns`.\n\n        If `json_file_name` is not provided, the factory will attempt to use\n        `self.input_handler.cache_file_name` to generate a default JSON file name.\n        If neither is set, a ValueError is raised.\n\n        Returns:\n            SpecModel: The constructed model.\n\n        Note:\n            The type of `doc_object` depends on the handler/parser used:\n            - For XHTML: a BeautifulSoup DOM object.\n            - For PDF: a grouped table dict as returned by PDFDocHandler.\n            - For other formats: as defined by the handler/parser.\n\n        Note:\n            If a progress observer accepting a Progress object is provided, progress events are as follows:\n\n            - **Step 1 (PARSING_TABLE):** Events include `status=PARSING_TABLE`, `step=1`, `total_steps=2`,\n                and a meaningful `percent` value.\n            - **Step 2 (SAVING_MODEL):** Events include `status=SAVING_MODEL`, `step=2`, `total_steps=2`,\n                and `percent` values of `0` (start) and `100` (completion).\n\n            For example usage in a client application,\n            see [`ProgressStatus`](progress.md#dcmspec.progress.ProgressStatus).\n\n        \"\"\"\n        # Try to load from cache first\n        model = self.try_load_cache(json_file_name, include_depth, model_kwargs, force_parse)\n        if model is not None:\n            return model\n\n        # Enrich the progress observer for parsing step\n        if progress_observer:\n            @add_progress_step(step=1, total_steps=2, status=ProgressStatus.PARSING_TABLE)\n            def parsing_progress_observer(progress):\n                progress_observer(progress)\n        else:\n            parsing_progress_observer = None\n\n        # Parse provided document otherwise\n        merged_parser_kwargs = {**self.parser_kwargs, **(parser_kwargs or {})}\n        model = self._parse_and_build_model(\n            doc_object=doc_object,\n            table_id=table_id,\n            url=url,\n            include_depth=include_depth,\n            model_kwargs=model_kwargs,\n            parser_kwargs=merged_parser_kwargs,\n            progress_observer=parsing_progress_observer,\n        )\n\n        # Cache the newly built model if requested\n        if json_file_name:\n            json_file_path = os.path.join(self.config.get_param(\"cache_dir\"), \"model\", json_file_name)\n            try:\n                if progress_observer:\n                    # Report start of saving\n                    progress_observer(Progress(0, status=ProgressStatus.SAVING_MODEL, step=2, total_steps=2))\n                self.model_store.save(model, json_file_path)\n                if progress_observer:\n                    # Report completion of saving\n                    progress_observer(Progress(100, status=ProgressStatus.SAVING_MODEL, step=2, total_steps=2))\n\n            except Exception as e:\n                self.logger.warning(f\"Failed to cache model to {json_file_path}: {e}\")\n\n        return model\n\n    def create_model(\n        self,\n        url: str,\n        cache_file_name: str,\n        table_id: Optional[str] = None,\n        force_parse: bool = False,\n        force_download: bool = False,\n        json_file_name: Optional[str] = None,\n        include_depth: Optional[int] = None,\n        progress_observer: Optional[ProgressObserver] = None,\n        # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n        progress_callback: Optional[Callable[[int], None]] = None,\n        # END LEGACY SUPPORT\n        handler_kwargs: Optional[Dict[str, Any]] = None,\n        model_kwargs: Optional[Dict[str, Any]] = None,\n        parser_kwargs: Optional[Dict[str, Any]] = None,\n    ) -&gt; SpecModel:\n        \"\"\"Integrated, one-step method to fetch, parse, and build a DICOM specification model from a URL.\n\n        Args:\n            url (str): The URL to download the input file from.\n            cache_file_name (str): Filename of the cached input file.\n            table_id (Optional[str]): Table identifier for model parsing.\n            force_parse (bool): If True, always parse the DOM and generate the JSON model, even if cached.\n            force_download (bool): If True, always download the input file and generate the model even if cached.\n                Note: force_download also implies force_parse.\n            json_file_name (Optional[str]): Filename to save the cached JSON model.\n            include_depth (Optional[int]): The depth to which included tables should be parsed.\n            progress_observer (Optional[ProgressObserver]): Optional observer to report download progress.\n                See the Note below for details on the progress events and their properties.\n            progress_callback (Optional[Callable[[int], None]]): [LEGACY, Deprecated] Optional callback to\n                report progress as an integer percent (0-100, or -1 if indeterminate). Use progress_observer\n                instead. Will be removed in a future release.\n            handler_kwargs (Optional[Dict[str, Any]]): Additional keyword arguments for the input handler's methods.\n            model_kwargs (Optional[Dict[str, Any]]): Additional keyword arguments for model construction.\n                Use this to supply extra parameters required by custom SpecModel subclasses.\n                For example, if your model class is `MyModel(metadata, content, foo, bar)`, pass\n                `model_kwargs={\"foo\": foo_value, \"bar\": bar_value}`.\n            parser_kwargs (Optional[Dict[str, Any]]): Additional keyword arguments to pass to the parser's\n                `parse` method. Use this to supply parser-specific options such as `skip_columns`.\n\n        Returns:\n            SpecModel: The constructed model.\n\n\n        Note:\n            If a progress observer accepting a Progress object is provided, progress events are as follows:\n\n            - **Step 1 (DOWNLOADING):** Events include `status=DOWNLOADING`, `step=1`, `total_steps=3`,\n                and a meaningful `percent` value.\n            - **Step 2 (PARSING_TABLE):** Events include `status=PARSING_TABLE`, `step=2`, `total_steps=3`,\n                and a meaningful `percent` value.\n            - **Step 3 (SAVING_MODEL):** Events include `status=SAVING_MODEL`, `step=3`, `total_steps=3`,\n                and `percent` values of `0` (start) and `100` (completion).\n\n            For example usage in a client application,\n            see [`ProgressStatus`](progress.md#dcmspec.progress.ProgressStatus).\n\n        \"\"\"\n        # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n        progress_observer = handle_legacy_callback(progress_observer, progress_callback)\n        # END LEGACY SUPPORT\n        # Set cache_file_name on the handler before checking cache\n        self.input_handler.cache_file_name = cache_file_name\n\n        # Try to load from cache before loading document object\n        model = self.try_load_cache(json_file_name, include_depth, model_kwargs, force_parse or force_download)\n        if model is not None:\n            return model\n\n        # --- Step 1 (DOWNLOADING)\n\n        # Wrap progress observer for step 1\n        if progress_observer:\n            @add_progress_step(step=1, total_steps=3)\n            def load_progress_observer(progress):\n                progress_observer(progress)\n        else:\n            load_progress_observer = None\n\n        # Pass handler_kwargs to load_document\n        doc_object = self.input_handler.load_document(\n            cache_file_name=cache_file_name,\n            url=url,\n            force_download=force_download,\n            progress_observer=load_progress_observer,\n            # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n            progress_callback=progress_callback,\n            # END LEGACY SUPPORT\n            **(handler_kwargs or {})\n        )\n\n        # --- Step 2 and 3 (PARSING_TABLE and SAVING_MODEL)\n\n        # Wrap progress observer for step 2 and 3\n        if progress_observer:\n            @offset_progress_steps(step_offset=1, total_steps=3)\n            def build_progress_observer(progress):\n                progress_observer(progress)\n        else:\n            build_progress_observer = None\n\n        return self.build_model(\n            doc_object=doc_object,\n            table_id=table_id,\n            url=url,\n            json_file_name=json_file_name,\n            include_depth=include_depth,\n            progress_observer=build_progress_observer,\n            force_parse=force_parse or force_download,\n            model_kwargs=model_kwargs,\n            parser_kwargs=parser_kwargs,\n        )\n\n\n    def _load_model_from_cache(\n        self,\n        json_file_path: str,\n        include_depth: Optional[int],\n        model_kwargs: Optional[Dict[str, Any]],\n    ) -&gt; Optional[SpecModel]:\n        \"\"\"Load model from cache file if include depth is valid.\"\"\"\n        try:\n            # Load the model from cache\n            model = self.model_store.load(json_file_path)\n            self.logger.info(f\"Loaded model from cache {json_file_path}\")\n\n            # Do not use cache if include_depth does not match the cached model's metadata\n            cached_depth = getattr(model.metadata, \"include_depth\", None)\n            if (\n                (include_depth is not None and cached_depth is not None and int(cached_depth) != int(include_depth))\n                or (include_depth is None and cached_depth is not None)\n                or (include_depth is not None and cached_depth is None)\n            ):\n                self.logger.info(\n                    (\n                        f\"Cached model include_depth ({cached_depth}) \"\n                        f\"does not match requested ({include_depth}), reparsing.\"\n                    )\n                )\n                return None\n\n            # Return the cached model, reconstructing it to the required subclass if necessary        \n            if isinstance(model, self.model_class):\n                model.logger = self.logger\n                return model\n            return self.model_class(\n                metadata=model.metadata,\n                content=model.content,\n                logger=self.logger,\n                **(model_kwargs or {}),\n            )\n\n        except Exception as e:\n            self.logger.warning(f\"Failed to load model from cache {json_file_path}: {e}\")\n            return None\n\n    def _parse_and_build_model(\n        self,\n        doc_object: Any,\n        table_id: Optional[str],\n        url: Optional[str],\n        include_depth: Optional[int],\n        model_kwargs: Optional[Dict[str, Any]],\n        parser_kwargs: Optional[Dict[str, Any]] = None,\n        progress_observer: Optional[ProgressObserver] = None,\n    ) -&gt; SpecModel:\n        \"\"\"Parse and Build model from provided parsed document object.\"\"\"\n        # Parse content and some metadata from the parsed document object\n        metadata, content = self.table_parser.parse(\n            doc_object,\n            table_id=table_id,\n            include_depth=include_depth,\n            column_to_attr=self.column_to_attr,\n            progress_observer=progress_observer,\n            name_attr=self.name_attr,\n            **(parser_kwargs or {}),\n        )\n\n        # Add args values to model metadata\n        metadata.url = url\n\n        # Build the model from parsed content and metadata\n        model = self.model_class(\n            metadata=metadata,\n            content=content,\n            logger=self.logger,\n            **(model_kwargs or {}),\n        )\n\n        # Clean up model from title nodes\n        model.exclude_titles()\n\n        return model\n</code></pre>"},{"location":"api/spec_factory/#dcmspec.spec_factory.SpecFactory.__init__","title":"<code>__init__(input_handler=None, model_class=None, model_store=None, table_parser=None, column_to_attr=None, name_attr=None, config=None, logger=None, parser_kwargs=None)</code>","text":"<p>Initialize the SpecFactory.</p> <p>The default values for <code>column_to_attr</code> and <code>name_attr</code> are designed for parsing DICOM PS3.3 module attribute tables, where columns typically represent element name, tag, type, and description.</p> PARAMETER DESCRIPTION <code>input_handler</code> <p>Handler for downloading and parsing input files. If None, a default XHTMLDocHandler is used.</p> <p> TYPE: <code>Optional[DocHandler]</code> DEFAULT: <code>None</code> </p> <code>model_class</code> <p>The class to instantiate for the model. If None, defaults to SpecModel.</p> <p> TYPE: <code>Optional[Type[SpecModel]]</code> DEFAULT: <code>None</code> </p> <code>model_store</code> <p>Store for loading and saving models. If None, a default JSONSpecStore is used.</p> <p> TYPE: <code>Optional[SpecStore]</code> DEFAULT: <code>None</code> </p> <code>table_parser</code> <p>Parser for extracting tables from documents. If None, a default DOMTableSpecParser is used.</p> <p> TYPE: <code>Optional[SpecParser]</code> DEFAULT: <code>None</code> </p> <code>column_to_attr</code> <p>Mapping from column indices to names of attributes of model nodes. If None, a default mapping is used.</p> <p> TYPE: <code>Optional[Dict[int, str]]</code> DEFAULT: <code>None</code> </p> <code>name_attr</code> <p>Attribute name to use for node names in the model. If None, defaults to \"elem_name\".</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>Configuration object. If None, a default Config is created.</p> <p> TYPE: <code>Optional[Config]</code> DEFAULT: <code>None</code> </p> <code>logger</code> <p>Logger instance to use. If None, a default logger is created.</p> <p> TYPE: <code>Optional[Logger]</code> DEFAULT: <code>None</code> </p> <code>parser_kwargs</code> <p>Default keyword arguments to pass to the parser's <code>parse</code> method. Use this to supply parser-specific options such as <code>skip_columns</code> or <code>unformatted</code>.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If config is not a Config instance or None.</p> Note <p><code>column_to_attr</code> and related flags (such as <code>unformatted</code>) are dicts to support non-sequential column mappings.</p> Source code in <code>src/dcmspec/spec_factory.py</code> <pre><code>def __init__(\n    self,\n    input_handler: Optional[DocHandler] = None,\n    model_class: Optional[Type[SpecModel]] = None,\n    model_store: Optional[SpecStore] = None,\n    table_parser: Optional[SpecParser] = None,\n    column_to_attr: Optional[Dict[int, str]] = None,\n    name_attr: Optional[str] = None,\n    config: Optional[Config] = None,\n    logger: Optional[logging.Logger] = None,\n    parser_kwargs: Optional[Dict[str, Any]] = None,\n):\n    \"\"\"Initialize the SpecFactory.\n\n    The default values for `column_to_attr` and `name_attr` are designed for parsing\n    DICOM PS3.3 module attribute tables, where columns typically represent element name,\n    tag, type, and description.\n\n    Args:\n        input_handler (Optional[DocHandler]): Handler for downloading and parsing input files.\n            If None, a default XHTMLDocHandler is used.\n        model_class (Optional[Type[SpecModel]]): The class to instantiate for the model.\n            If None, defaults to SpecModel.\n        model_store (Optional[SpecStore]): Store for loading and saving models.\n            If None, a default JSONSpecStore is used.\n        table_parser (Optional[SpecParser]): Parser for extracting tables from documents.\n            If None, a default DOMTableSpecParser is used.\n        column_to_attr (Optional[Dict[int, str]]): Mapping from column indices to names of attributes\n            of model nodes. If None, a default mapping is used.\n        name_attr (Optional[str]): Attribute name to use for node names in the model.\n            If None, defaults to \"elem_name\".\n        config (Optional[Config]): Configuration object. If None, a default Config is created.\n        logger (Optional[logging.Logger]): Logger instance to use.\n            If None, a default logger is created.\n        parser_kwargs (Optional[Dict[str, Any]]): Default keyword arguments to pass to the parser's\n            `parse` method. Use this to supply parser-specific options such as `skip_columns` or `unformatted`.\n\n    Raises:\n        TypeError: If config is not a Config instance or None.\n\n    Note:\n        `column_to_attr` and related flags (such as `unformatted`) are dicts to support non-sequential\n        column mappings.\n\n    \"\"\"\n    import logging\n    if config is not None and not isinstance(config, Config):\n        raise TypeError(\"config must be an instance of Config or None\")\n    self.config = config or Config()\n\n    self.logger = logger or logging.getLogger(self.__class__.__name__)\n\n    self.model_class = model_class or SpecModel\n    self.input_handler = input_handler or XHTMLDocHandler(config=self.config, logger=self.logger)\n    self.model_store = model_store or JSONSpecStore(logger=self.logger)\n    self.table_parser = table_parser or DOMTableSpecParser(logger=self.logger)\n    self.column_to_attr = column_to_attr or {0: \"elem_name\", 1: \"elem_tag\", 2: \"elem_type\", 3: \"elem_description\"}\n    self.name_attr = name_attr or \"elem_name\"\n    self.parser_kwargs = parser_kwargs or {}\n</code></pre>"},{"location":"api/spec_factory/#dcmspec.spec_factory.SpecFactory.build_model","title":"<code>build_model(doc_object, table_id=None, url=None, json_file_name=None, include_depth=None, progress_observer=None, force_parse=False, model_kwargs=None, parser_kwargs=None)</code>","text":"<p>Build and cache a DICOM specification model from a parsed document object.</p> PARAMETER DESCRIPTION <code>doc_object</code> <p>The parsed document object to be parsed into a model. - For XHTML: a BeautifulSoup DOM object. - For PDF: a grouped table dict (from PDFDocHandler). - For other formats: as defined by the handler/parser.</p> <p> TYPE: <code>Any</code> </p> <code>table_id</code> <p>Table identifier for model parsing.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>url</code> <p>The URL the document was fetched from (for metadata).</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>json_file_name</code> <p>Filename to save the cached JSON model.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>include_depth</code> <p>The depth to which included tables should be parsed.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>force_parse</code> <p>If True, always parse and (over)write the JSON cache file.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>progress_observer</code> <p>Optional observer to report download progress. See the Note below for details on the progress events and their properties.</p> <p> TYPE: <code>Optional[ProgressObserver]</code> DEFAULT: <code>None</code> </p> <code>model_kwargs</code> <p>Additional keyword arguments for model construction. Use this to supply extra parameters required by custom SpecModel subclasses. For example, if your model class is <code>MyModel(metadata, content, foo, bar)</code>, pass <code>model_kwargs={\"foo\": foo_value, \"bar\": bar_value}</code>.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> <code>parser_kwargs</code> <p>Additional keyword arguments to pass to the parser's <code>parse</code> method. Use this to supply parser-specific options such as <code>skip_columns</code>.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> <p>If <code>json_file_name</code> is not provided, the factory will attempt to use <code>self.input_handler.cache_file_name</code> to generate a default JSON file name. If neither is set, a ValueError is raised.</p> RETURNS DESCRIPTION <code>SpecModel</code> <p>The constructed model.</p> <p> TYPE: <code>SpecModel</code> </p> Note <p>The type of <code>doc_object</code> depends on the handler/parser used: - For XHTML: a BeautifulSoup DOM object. - For PDF: a grouped table dict as returned by PDFDocHandler. - For other formats: as defined by the handler/parser.</p> Note <p>If a progress observer accepting a Progress object is provided, progress events are as follows:</p> <ul> <li>Step 1 (PARSING_TABLE): Events include <code>status=PARSING_TABLE</code>, <code>step=1</code>, <code>total_steps=2</code>,     and a meaningful <code>percent</code> value.</li> <li>Step 2 (SAVING_MODEL): Events include <code>status=SAVING_MODEL</code>, <code>step=2</code>, <code>total_steps=2</code>,     and <code>percent</code> values of <code>0</code> (start) and <code>100</code> (completion).</li> </ul> <p>For example usage in a client application, see <code>ProgressStatus</code>.</p> Source code in <code>src/dcmspec/spec_factory.py</code> <pre><code>def build_model(\n    self,\n    doc_object: Any,\n    table_id: Optional[str] = None,\n    url: Optional[str] = None,\n    json_file_name: Optional[str] = None,\n    include_depth: Optional[int] = None,\n    progress_observer: Optional[ProgressObserver] = None,\n    force_parse: bool = False,\n    model_kwargs: Optional[Dict[str, Any]] = None,\n    parser_kwargs: Optional[Dict[str, Any]] = None,\n) -&gt; SpecModel:\n    \"\"\"Build and cache a DICOM specification model from a parsed document object.\n\n    Args:\n        doc_object (Any): The parsed document object to be parsed into a model.\n            - For XHTML: a BeautifulSoup DOM object.\n            - For PDF: a grouped table dict (from PDFDocHandler).\n            - For other formats: as defined by the handler/parser.\n        table_id (Optional[str]): Table identifier for model parsing.\n        url (Optional[str]): The URL the document was fetched from (for metadata).\n        json_file_name (Optional[str]): Filename to save the cached JSON model.\n        include_depth (Optional[int]): The depth to which included tables should be parsed.\n        force_parse (bool): If True, always parse and (over)write the JSON cache file.\n        progress_observer (Optional[ProgressObserver]): Optional observer to report download progress.\n            See the Note below for details on the progress events and their properties.\n        model_kwargs (Optional[Dict[str, Any]]): Additional keyword arguments for model construction.\n            Use this to supply extra parameters required by custom SpecModel subclasses.\n            For example, if your model class is `MyModel(metadata, content, foo, bar)`, pass\n            `model_kwargs={\"foo\": foo_value, \"bar\": bar_value}`.\n        parser_kwargs (Optional[Dict[str, Any]]): Additional keyword arguments to pass to the parser's\n            `parse` method. Use this to supply parser-specific options such as `skip_columns`.\n\n    If `json_file_name` is not provided, the factory will attempt to use\n    `self.input_handler.cache_file_name` to generate a default JSON file name.\n    If neither is set, a ValueError is raised.\n\n    Returns:\n        SpecModel: The constructed model.\n\n    Note:\n        The type of `doc_object` depends on the handler/parser used:\n        - For XHTML: a BeautifulSoup DOM object.\n        - For PDF: a grouped table dict as returned by PDFDocHandler.\n        - For other formats: as defined by the handler/parser.\n\n    Note:\n        If a progress observer accepting a Progress object is provided, progress events are as follows:\n\n        - **Step 1 (PARSING_TABLE):** Events include `status=PARSING_TABLE`, `step=1`, `total_steps=2`,\n            and a meaningful `percent` value.\n        - **Step 2 (SAVING_MODEL):** Events include `status=SAVING_MODEL`, `step=2`, `total_steps=2`,\n            and `percent` values of `0` (start) and `100` (completion).\n\n        For example usage in a client application,\n        see [`ProgressStatus`](progress.md#dcmspec.progress.ProgressStatus).\n\n    \"\"\"\n    # Try to load from cache first\n    model = self.try_load_cache(json_file_name, include_depth, model_kwargs, force_parse)\n    if model is not None:\n        return model\n\n    # Enrich the progress observer for parsing step\n    if progress_observer:\n        @add_progress_step(step=1, total_steps=2, status=ProgressStatus.PARSING_TABLE)\n        def parsing_progress_observer(progress):\n            progress_observer(progress)\n    else:\n        parsing_progress_observer = None\n\n    # Parse provided document otherwise\n    merged_parser_kwargs = {**self.parser_kwargs, **(parser_kwargs or {})}\n    model = self._parse_and_build_model(\n        doc_object=doc_object,\n        table_id=table_id,\n        url=url,\n        include_depth=include_depth,\n        model_kwargs=model_kwargs,\n        parser_kwargs=merged_parser_kwargs,\n        progress_observer=parsing_progress_observer,\n    )\n\n    # Cache the newly built model if requested\n    if json_file_name:\n        json_file_path = os.path.join(self.config.get_param(\"cache_dir\"), \"model\", json_file_name)\n        try:\n            if progress_observer:\n                # Report start of saving\n                progress_observer(Progress(0, status=ProgressStatus.SAVING_MODEL, step=2, total_steps=2))\n            self.model_store.save(model, json_file_path)\n            if progress_observer:\n                # Report completion of saving\n                progress_observer(Progress(100, status=ProgressStatus.SAVING_MODEL, step=2, total_steps=2))\n\n        except Exception as e:\n            self.logger.warning(f\"Failed to cache model to {json_file_path}: {e}\")\n\n    return model\n</code></pre>"},{"location":"api/spec_factory/#dcmspec.spec_factory.SpecFactory.create_model","title":"<code>create_model(url, cache_file_name, table_id=None, force_parse=False, force_download=False, json_file_name=None, include_depth=None, progress_observer=None, progress_callback=None, handler_kwargs=None, model_kwargs=None, parser_kwargs=None)</code>","text":"<p>Integrated, one-step method to fetch, parse, and build a DICOM specification model from a URL.</p> PARAMETER DESCRIPTION <code>url</code> <p>The URL to download the input file from.</p> <p> TYPE: <code>str</code> </p> <code>cache_file_name</code> <p>Filename of the cached input file.</p> <p> TYPE: <code>str</code> </p> <code>table_id</code> <p>Table identifier for model parsing.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>force_parse</code> <p>If True, always parse the DOM and generate the JSON model, even if cached.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>force_download</code> <p>If True, always download the input file and generate the model even if cached. Note: force_download also implies force_parse.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>json_file_name</code> <p>Filename to save the cached JSON model.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>include_depth</code> <p>The depth to which included tables should be parsed.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>progress_observer</code> <p>Optional observer to report download progress. See the Note below for details on the progress events and their properties.</p> <p> TYPE: <code>Optional[ProgressObserver]</code> DEFAULT: <code>None</code> </p> <code>progress_callback</code> <p>[LEGACY, Deprecated] Optional callback to report progress as an integer percent (0-100, or -1 if indeterminate). Use progress_observer instead. Will be removed in a future release.</p> <p> TYPE: <code>Optional[Callable[[int], None]]</code> DEFAULT: <code>None</code> </p> <code>handler_kwargs</code> <p>Additional keyword arguments for the input handler's methods.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> <code>model_kwargs</code> <p>Additional keyword arguments for model construction. Use this to supply extra parameters required by custom SpecModel subclasses. For example, if your model class is <code>MyModel(metadata, content, foo, bar)</code>, pass <code>model_kwargs={\"foo\": foo_value, \"bar\": bar_value}</code>.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> <code>parser_kwargs</code> <p>Additional keyword arguments to pass to the parser's <code>parse</code> method. Use this to supply parser-specific options such as <code>skip_columns</code>.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>SpecModel</code> <p>The constructed model.</p> <p> TYPE: <code>SpecModel</code> </p> Note <p>If a progress observer accepting a Progress object is provided, progress events are as follows:</p> <ul> <li>Step 1 (DOWNLOADING): Events include <code>status=DOWNLOADING</code>, <code>step=1</code>, <code>total_steps=3</code>,     and a meaningful <code>percent</code> value.</li> <li>Step 2 (PARSING_TABLE): Events include <code>status=PARSING_TABLE</code>, <code>step=2</code>, <code>total_steps=3</code>,     and a meaningful <code>percent</code> value.</li> <li>Step 3 (SAVING_MODEL): Events include <code>status=SAVING_MODEL</code>, <code>step=3</code>, <code>total_steps=3</code>,     and <code>percent</code> values of <code>0</code> (start) and <code>100</code> (completion).</li> </ul> <p>For example usage in a client application, see <code>ProgressStatus</code>.</p> Source code in <code>src/dcmspec/spec_factory.py</code> <pre><code>def create_model(\n    self,\n    url: str,\n    cache_file_name: str,\n    table_id: Optional[str] = None,\n    force_parse: bool = False,\n    force_download: bool = False,\n    json_file_name: Optional[str] = None,\n    include_depth: Optional[int] = None,\n    progress_observer: Optional[ProgressObserver] = None,\n    # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n    progress_callback: Optional[Callable[[int], None]] = None,\n    # END LEGACY SUPPORT\n    handler_kwargs: Optional[Dict[str, Any]] = None,\n    model_kwargs: Optional[Dict[str, Any]] = None,\n    parser_kwargs: Optional[Dict[str, Any]] = None,\n) -&gt; SpecModel:\n    \"\"\"Integrated, one-step method to fetch, parse, and build a DICOM specification model from a URL.\n\n    Args:\n        url (str): The URL to download the input file from.\n        cache_file_name (str): Filename of the cached input file.\n        table_id (Optional[str]): Table identifier for model parsing.\n        force_parse (bool): If True, always parse the DOM and generate the JSON model, even if cached.\n        force_download (bool): If True, always download the input file and generate the model even if cached.\n            Note: force_download also implies force_parse.\n        json_file_name (Optional[str]): Filename to save the cached JSON model.\n        include_depth (Optional[int]): The depth to which included tables should be parsed.\n        progress_observer (Optional[ProgressObserver]): Optional observer to report download progress.\n            See the Note below for details on the progress events and their properties.\n        progress_callback (Optional[Callable[[int], None]]): [LEGACY, Deprecated] Optional callback to\n            report progress as an integer percent (0-100, or -1 if indeterminate). Use progress_observer\n            instead. Will be removed in a future release.\n        handler_kwargs (Optional[Dict[str, Any]]): Additional keyword arguments for the input handler's methods.\n        model_kwargs (Optional[Dict[str, Any]]): Additional keyword arguments for model construction.\n            Use this to supply extra parameters required by custom SpecModel subclasses.\n            For example, if your model class is `MyModel(metadata, content, foo, bar)`, pass\n            `model_kwargs={\"foo\": foo_value, \"bar\": bar_value}`.\n        parser_kwargs (Optional[Dict[str, Any]]): Additional keyword arguments to pass to the parser's\n            `parse` method. Use this to supply parser-specific options such as `skip_columns`.\n\n    Returns:\n        SpecModel: The constructed model.\n\n\n    Note:\n        If a progress observer accepting a Progress object is provided, progress events are as follows:\n\n        - **Step 1 (DOWNLOADING):** Events include `status=DOWNLOADING`, `step=1`, `total_steps=3`,\n            and a meaningful `percent` value.\n        - **Step 2 (PARSING_TABLE):** Events include `status=PARSING_TABLE`, `step=2`, `total_steps=3`,\n            and a meaningful `percent` value.\n        - **Step 3 (SAVING_MODEL):** Events include `status=SAVING_MODEL`, `step=3`, `total_steps=3`,\n            and `percent` values of `0` (start) and `100` (completion).\n\n        For example usage in a client application,\n        see [`ProgressStatus`](progress.md#dcmspec.progress.ProgressStatus).\n\n    \"\"\"\n    # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n    progress_observer = handle_legacy_callback(progress_observer, progress_callback)\n    # END LEGACY SUPPORT\n    # Set cache_file_name on the handler before checking cache\n    self.input_handler.cache_file_name = cache_file_name\n\n    # Try to load from cache before loading document object\n    model = self.try_load_cache(json_file_name, include_depth, model_kwargs, force_parse or force_download)\n    if model is not None:\n        return model\n\n    # --- Step 1 (DOWNLOADING)\n\n    # Wrap progress observer for step 1\n    if progress_observer:\n        @add_progress_step(step=1, total_steps=3)\n        def load_progress_observer(progress):\n            progress_observer(progress)\n    else:\n        load_progress_observer = None\n\n    # Pass handler_kwargs to load_document\n    doc_object = self.input_handler.load_document(\n        cache_file_name=cache_file_name,\n        url=url,\n        force_download=force_download,\n        progress_observer=load_progress_observer,\n        # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n        progress_callback=progress_callback,\n        # END LEGACY SUPPORT\n        **(handler_kwargs or {})\n    )\n\n    # --- Step 2 and 3 (PARSING_TABLE and SAVING_MODEL)\n\n    # Wrap progress observer for step 2 and 3\n    if progress_observer:\n        @offset_progress_steps(step_offset=1, total_steps=3)\n        def build_progress_observer(progress):\n            progress_observer(progress)\n    else:\n        build_progress_observer = None\n\n    return self.build_model(\n        doc_object=doc_object,\n        table_id=table_id,\n        url=url,\n        json_file_name=json_file_name,\n        include_depth=include_depth,\n        progress_observer=build_progress_observer,\n        force_parse=force_parse or force_download,\n        model_kwargs=model_kwargs,\n        parser_kwargs=parser_kwargs,\n    )\n</code></pre>"},{"location":"api/spec_factory/#dcmspec.spec_factory.SpecFactory.load_document","title":"<code>load_document(url, cache_file_name, force_download=False, progress_observer=None, progress_callback=None)</code>","text":"<p>Download, cache, and parse the specification file from a URL, returning the document object.</p> PARAMETER DESCRIPTION <code>url</code> <p>The URL to download the input file from.</p> <p> TYPE: <code>str</code> </p> <code>cache_file_name</code> <p>Filename of the cached input file.</p> <p> TYPE: <code>str</code> </p> <code>force_download</code> <p>If True, always download the input file even if cached.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>progress_observer</code> <p>Optional observer to report download progress.</p> <p> TYPE: <code>Optional[ProgressObserver]</code> DEFAULT: <code>None</code> </p> <code>progress_callback</code> <p>[LEGACY] Optional callback to report progress Deprecated: use progress_observer instead. Will be removed in a future release.</p> <p> TYPE: <code>Optional[Callable[[int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The document object.</p> <p> TYPE: <code>Any</code> </p> Note <p>Progress reporting via progress_observer is delegated to the input handler (e.g., XHTMLDocHandler, PDFDocHandler) and typically covers only downloading and caching (writing to disk). Parsing and model building do not emit progress updates.</p> Source code in <code>src/dcmspec/spec_factory.py</code> <pre><code>def load_document(self, \n                url: str, \n                cache_file_name: str, \n                force_download: bool = False, \n                progress_observer: 'Optional[ProgressObserver]' = None,\n                # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n                progress_callback: 'Optional[Callable[[int], None]]' = None,\n                # END LEGACY SUPPORT\n                ) -&gt; Any:\n    \"\"\"Download, cache, and parse the specification file from a URL, returning the document object.\n\n    Args:\n        url (str): The URL to download the input file from.\n        cache_file_name (str): Filename of the cached input file.\n        force_download (bool): If True, always download the input file even if cached.\n        progress_observer (Optional[ProgressObserver]): Optional observer to report download progress.\n        progress_callback (Optional[Callable[[int], None]]): [LEGACY] Optional callback to report progress\n            Deprecated: use progress_observer instead. Will be removed in a future release.\n\n    Returns:\n        Any: The document object.\n\n    Note:\n        Progress reporting via progress_observer is delegated to the input handler (e.g., XHTMLDocHandler,\n        PDFDocHandler) and typically covers only downloading and caching (writing to disk).\n        Parsing and model building do not emit progress updates.\n\n    \"\"\"\n    # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n    progress_observer = handle_legacy_callback(progress_observer, progress_callback)\n    # END LEGACY SUPPORT\n    # This will download if needed and always parse/return the DOM\n    return self.input_handler.load_document(cache_file_name=cache_file_name,\n                                            url=url,\n                                            force_download=force_download,\n                                            progress_observer=progress_observer\n                                            )\n</code></pre>"},{"location":"api/spec_factory/#dcmspec.spec_factory.SpecFactory.try_load_cache","title":"<code>try_load_cache(json_file_name, include_depth, model_kwargs, force_parse=False)</code>","text":"<p>Check for and load a model from cache if available and not force_parse.</p> Source code in <code>src/dcmspec/spec_factory.py</code> <pre><code>def try_load_cache(\n    self,\n    json_file_name: Optional[str],\n    include_depth: Optional[int],\n    model_kwargs: Optional[Dict[str, Any]],\n    force_parse: bool = False,\n) -&gt; Optional[SpecModel]:\n    \"\"\"Check for and load a model from cache if available and not force_parse.\"\"\"\n    if json_file_name is None:\n        cache_file_name = getattr(self.input_handler, \"cache_file_name\", None)\n        if cache_file_name is None:\n            raise ValueError(\"input_handler.cache_file_name not set\")\n        json_file_name = f\"{os.path.splitext(cache_file_name)[0]}.json\"\n    json_file_path = os.path.join(self.config.get_param(\"cache_dir\"), \"model\", json_file_name)\n    if os.path.exists(json_file_path) and not force_parse:\n        model = self._load_model_from_cache(json_file_path, include_depth, model_kwargs)\n        if model is not None:\n            return model\n    return None\n</code></pre>"},{"location":"api/spec_merger/","title":"SpecMerger","text":""},{"location":"api/spec_merger/#dcmspec.spec_merger.SpecMerger","title":"<code>dcmspec.spec_merger.SpecMerger</code>","text":"<p>Merges multiple DICOM specification models.</p> <p>The SpecMerger class provides methods to combine and enrich DICOM SpecModel objects, supporting both path-based and node-based merging strategies. This is useful for workflows where you need to sequentially merge two or more models, such as enriching PS3.3 module attributes models with definitions from the PS3.6 data elements dictionary, or combining a PS3.3 specification with a PS3.4 SOP class and then enriching with an  IHE profile specification.</p> Source code in <code>src/dcmspec/spec_merger.py</code> <pre><code>class SpecMerger:\n    \"\"\"Merges multiple DICOM specification models.\n\n    The SpecMerger class provides methods to combine and enrich DICOM SpecModel objects,\n    supporting both path-based and node-based merging strategies. This is useful for\n    workflows where you need to sequentially merge two or more models, such as enriching\n    PS3.3 module attributes models with definitions from the PS3.6 data elements dictionary,\n    or combining a PS3.3 specification with a PS3.4 SOP class and then enriching with an \n    IHE profile specification.\n    \"\"\"\n\n    def __init__(self, config: Config = None, model_store: SpecStore = None, logger: logging.Logger = None):\n        \"\"\"Initialize the SpecMerger.\n\n        Sets up the logger for the merger. If no logger is provided, a default logger is created.\n        If no model_store is provided, defaults to JSONSpecStore.\n\n        Args:\n            config (Optional[Config]): Configuration object. If None, a default Config is created.\n            model_store (Optional[SpecStore]): Store for loading and saving models. Defaults to JSONSpecStore.\n            logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n        \"\"\"\n        self.logger = logger or logging.getLogger(self.__class__.__name__)\n        self.config = config or Config()\n        self.model_store = model_store or JSONSpecStore(logger=self.logger)\n\n    def merge_node(\n        self,\n        model1: SpecModel,\n        model2: SpecModel,\n        match_by: str = \"name\",\n        attribute_name: str = None,\n        merge_attrs: list[str] = None,\n        json_file_name: str = None,\n        force_update: bool = False,\n    ) -&gt; SpecModel:\n        \"\"\"Merge two DICOM SpecModel objects using the node merge method, with optional caching.\n\n        This is a convenience method that calls merge_many with two models.\n\n        Args:\n            model1 (SpecModel): The first model.\n            model2 (SpecModel): The second model to merge with the first.\n            match_by (str, optional): \"name\" to match by node name, \"attribute\" to match by a specific attribute.\n            attribute_name (str, optional): The attribute name to use for matching.\n            merge_attrs (list[str], optional): List of attribute names to merge from the other model's node.\n            json_file_name (str, optional): If provided, cache/load the merged model to/from this file.\n            force_update (bool, optional): If True, always perform the merge and overwrite the cache.\n\n        Returns:\n            SpecModel: The merged SpecModel instance.\n\n        \"\"\"\n        return self.merge_many(\n            [model1, model2],\n            method = \"matching_node\",\n            match_by=match_by,\n            attribute_names=[attribute_name],\n            merge_attrs_list=[merge_attrs],\n            json_file_name=json_file_name,\n            force_update=force_update,\n        )\n\n    def merge_path(\n        self,\n        model1: SpecModel,\n        model2: SpecModel,\n        match_by: str = \"attribute\",\n        attribute_name: str = \"elem_tag\",\n        merge_attrs: list[str] = None,\n        json_file_name: str = None,\n        force_update: bool = False,\n        ignore_module_level: bool = False,\n    ) -&gt; SpecModel:\n        \"\"\"Merge two DICOM SpecModel objects using the path merge method, with optional caching.\n\n        This is a convenience method that calls merge_many with two models.\n\n        By default, this method matches nodes by their DICOM tag (attribute_name=\"elem_tag\") using\n        path-based merging (match_by=\"attribute\"). This is the recommended and robust approach for\n        DICOM attribute-level merging, as DICOM tags are unique and consistent identifiers.\n\n        Args:\n            model1 (SpecModel): The first model.\n            model2 (SpecModel): The second model to merge with the first.\n            match_by (str, optional): \"attribute\" (default, recommended) to match by a specific attribute (DICOM tag),\n                or \"name\" to match by node name.\n            attribute_name (str, optional): The attribute name to use for matching (default: \"elem_tag\").\n            merge_attrs (list[str], optional): List of attribute names to merge from the other model's node.\n            json_file_name (str, optional): If provided, cache/load the merged model to/from this file.\n            force_update (bool, optional): If True, always perform the merge and overwrite the cache.\n            ignore_module_level (bool, optional): If True, skip the module level when matching paths.\n\n        Returns:\n            SpecModel: The merged SpecModel instance.\n\n        Note:\n            For DICOM attribute-level merging, the default (match_by=\"attribute\", attribute_name=\"elem_tag\")\n            is strongly recommended. Only use match_by=\"name\" for special cases where tag-based matching is\n            not possible.\n\n        \"\"\"\n        return self.merge_many(\n            [model1, model2],\n            method = \"matching_path\",\n            match_by=match_by,\n            attribute_names=[attribute_name],\n            merge_attrs_list=[merge_attrs],\n            json_file_name=json_file_name,\n            force_update=force_update,\n            ignore_module_level=ignore_module_level,\n        )\n\n    def merge_path_with_default(\n        self,\n        model1: SpecModel,\n        model2: SpecModel,\n        match_by: str = \"name\",\n        attribute_name: str = None,\n        merge_attrs: list[str] = None,\n        default_attr: str = \"elem_type\",\n        default_value: str = \"3\",\n        default_value_func: callable = None,\n        json_file_name: str = None,\n        force_update: bool = False,\n        ignore_module_level: bool = False,\n\n    ) -&gt; SpecModel:\n        \"\"\"Merge two DICOM SpecModel objects by path, and set a default value for missing attributes.\n\n        This method merges two models using the path-based merge strategy (matching nodes by their\n        hierarchical path and by DICOM tag, i.e., match_by=\"attribute\", attribute_name=\"elem_tag\" by default),\n        and then sets `default_attr` to `default_value` for any node in the merged model that does not already\n        have that attribute.\n\n        This is especially useful for workflows where you want to enrich a normalized IOD model\n        (e.g., from DICOM PS3.3) with a service attribute model (e.g., from DICOM PS3.4 or an IHE\n        profile), and you want to ensure that all nodes in the merged model have a value for the\n        Type attribute.\n\n        Use case:\n            - Merging a DICOM PS3.3 normalized IOD attributes specification (e.g., built with IODSpecBuilder)\n              with a DICOM PS3.4 DIMSE SCU or SCP attributes specification (e.g., built with ServiceAttributeModel\n              and select_dimse/select_role). After merging, any node present in the normalized IOD model but\n              missing from the service attribute model will have its \"Type\" (or other specified attribute)\n              set to the default value (e.g., \"3\"), ensuring the merged model is complete and ready for\n              further processing or export.\n\n        Args:\n            model1 (SpecModel): The first model (e.g., normalized IOD).\n            model2 (SpecModel): The second model (e.g., service attribute model).\n            match_by (str, optional): \"attribute\" (default, recommended) to match by a specific attribute (DICOM tag),\n                or \"name\" to match by node name.\n            attribute_name (str, optional): The attribute name to use for matching (default: \"elem_tag\").\n            merge_attrs (list[str], optional): List of attribute names to merge from the other model's node.\n            default_attr (str, optional): The attribute to set if missing (default: \"elem_type\").\n            default_value (str, optional): The value to set for missing attributes (default: \"3\").\n            default_value_func (callable, optional): A function to determine the default value for missing attributes.\n                If provided, it will be called as\n                `default_value_func(node, merged_model, service_model, default_attr, default_value)`\n                and should return the value to use for the missing attribute.\n            json_file_name (str, optional): If provided, cache/load the merged model to/from this file.\n            force_update (bool, optional): If True, always perform the merge and overwrite the cache.\n            ignore_module_level (bool, optional): If True, skip the module level when matching paths.\n\n        Returns:\n            SpecModel: The merged SpecModel instance with default values set for missing attributes.\n\n        Note:\n            For DICOM attribute-level merging, the default (match_by=\"attribute\", attribute_name=\"elem_tag\")\n            is strongly recommended. Only use match_by=\"name\" for special cases where tag-based matching is\n            not possible.\n\n        \"\"\"\n        merged = self.merge_path(\n            model1,\n            model2,\n            match_by=match_by,\n            attribute_name=attribute_name,\n            merge_attrs=merge_attrs,\n            ignore_module_level=ignore_module_level,\n            json_file_name=json_file_name,\n            force_update=force_update,\n        )\n\n        for node in merged.content.descendants:\n            # Only set default_attr on nodes that have the match attribute (attribute_name) and are not module nodes\n            if (\n                attribute_name is not None\n                and hasattr(node, attribute_name)\n                and not hasattr(node, default_attr)\n            ):\n                if default_value_func is not None:\n                    value = default_value_func(node, merged, model2, default_attr, default_value)\n                else:\n                    value = default_value\n                setattr(node, default_attr, value)\n        return merged\n\n    def merge_many(\n        self,\n        models: list[SpecModel],\n        method: str,\n        match_by: str,\n        attribute_names: list = None,\n        merge_attrs_list: list = None,\n        json_file_name: str = None,\n        force_update: bool = False,\n        ignore_module_level: bool = False,\n    ) -&gt; SpecModel:\n        \"\"\"Merge a sequence of DICOM SpecModel objects using the specified merge method, with optional caching.\n\n        This method merges a list of models in order, applying either path-based or node-based\n        merging at each step. You can specify different attribute names and lists of attributes\n        to merge for each step, allowing for flexible, multi-stage enrichment of DICOM models.\n        If json_file_name is provided, the merged model will be cached to that file, and loaded from\n        cache if available and force_update is False.\n\n        Args:\n            models (list of SpecModel): The models to merge, in order.\n            method (str): Merge method to use (\"matching_path\" or \"matching_node\").\n            match_by (str): \"name\" to match by node name, \"attribute\" to match by a specific attribute.\n            attribute_names (list, optional): List of attribute names to use for each merge step.\n                Each entry corresponds to a merge operation between two models.\n                Required if match_by=\"attribute\". If match_by=\"name\", can be None.\n            merge_attrs_list (list, optional): List of lists of attribute names to merge for each merge step.\n                Each entry corresponds to a merge operation between two models.\n            json_file_name (str, optional): If provided, cache/load the merged model to/from this file.\n            force_update (bool, optional): If True, always perform the merge and overwrite the cache.\n            ignore_module_level (bool, optional): If True, skip the module level when matching paths (only applies\n                to path-based merging).\n\n        Returns:\n            SpecModel: The final merged SpecModel instance.\n\n        Raises:\n            ValueError: If models is empty, method is unknown, or attribute_names/merge_attrs_list\n                have incorrect length, or if attribute_names is not set when match_by=\"attribute\".\n\n        Note:\n            - For path-based merging of DICOM attributes, it is recommended to use match_by=\"attribute\"\n              and attribute_names=[\"elem_tag\", ...] for robust, tag-based matching.\n            - For node-based merging or special cases, match_by=\"name\" can be used and attribute_names may be None.\n\n        \"\"\"\n        # Check that required arguments are set\n        if method is None or match_by is None:\n            raise ValueError(\n                \"merge_many requires method and match_by to be set explicitly by the caller.\"\n            )\n        if match_by == \"attribute\" and (attribute_names is None or any(a is None for a in attribute_names)):\n            raise ValueError(\n                \"merge_many requires attribute_names to be set when match_by='attribute'.\"\n            )\n        orig_col2attr = None\n        if models and hasattr(models[0].metadata, \"column_to_attr\"):\n            orig_col2attr = models[0].metadata.column_to_attr\n        cached_model = self._load_merged_model_from_cache(json_file_name, force_update, merge_attrs_list, orig_col2attr)\n        if cached_model is not None:\n            return cached_model\n\n        self._validate_merge_args(models, attribute_names, merge_attrs_list)\n        merged = self._merge_models(\n            models,\n            method=method,\n            match_by=match_by,\n            attribute_names=attribute_names,\n            merge_attrs_list=merge_attrs_list,\n            ignore_module_level=ignore_module_level,\n        )\n        self._update_metadata(merged, models, merge_attrs_list)\n        self._save_cache(merged, json_file_name)\n        return merged\n\n    def _validate_merge_args(\n        self,\n        models: list[SpecModel],\n        attribute_names: list,\n        merge_attrs_list: list,\n    ) -&gt; None:\n        \"\"\"Validate and normalize merge arguments for merging models.\n\n        This function ensures that the lists of attribute names and merge attribute lists\n        are the correct length and format for the number of merges to be performed.\n        It also normalizes single values to lists, so that downstream code can always\n        assume lists of the correct length.\n\n        - If attribute_names or merge_attrs_list are None or a single value, they are expanded to lists.\n        - If their length does not match the number of merges (len(models) - 1), a ValueError is raised.\n\n        This normalization allows the merge logic to always use attribute_names[i] and merge_attrs_list[i]\n        for each merge step, regardless of how the arguments were originally provided.\n        \"\"\"\n        if not models:\n            raise ValueError(\"No models to merge\")\n        n_merges = len(models) - 1\n\n        # Normalize attribute_names: ensure it's a list of length n_merges\n        if attribute_names is None:\n            attribute_names = [None] * n_merges\n        elif not isinstance(attribute_names, list):\n            # If a single value is provided, expand it to a list\n            attribute_names = [attribute_names] * n_merges\n\n        # Normalize merge_attrs_list: ensure it's a list of lists of length n_merges\n        if merge_attrs_list is None:\n            merge_attrs_list = [None] * n_merges\n        elif (\n            not isinstance(merge_attrs_list, list)\n            or (\n                merge_attrs_list\n                and not isinstance(merge_attrs_list[0], list)\n            )\n        ):\n            # If a single value or a flat list is provided, expand it to a list of lists\n            merge_attrs_list = [merge_attrs_list] * n_merges\n\n        # Validate lengths\n        if len(attribute_names) != n_merges:\n            raise ValueError(\n                f\"Length of attribute_names ({len(attribute_names)}) \"\n                f\"does not match number of merges ({n_merges})\"\n            )\n        if len(merge_attrs_list) != n_merges:\n            raise ValueError(\n                f\"Length of merge_attrs_list ({len(merge_attrs_list)}) \"\n                f\"does not match number of merges ({n_merges})\"\n                )\n\n    def _merge_models(\n        self,\n        models: list[SpecModel],\n        method: str = \"matching_path\",\n        match_by: str = \"name\",\n        attribute_names: list = None,\n        merge_attrs_list: list = None,\n        ignore_module_level: bool = False,\n    ) -&gt; SpecModel:\n        \"\"\"Perform the actual merging of models using the specified method.\"\"\"\n        merged = models[0]\n        if method not in (\"matching_path\", \"matching_node\"):\n            raise ValueError(f\"Unknown merge method: {method}\")\n\n        for i, model in enumerate(models[1:]):\n            attribute_name = attribute_names[i]\n            merge_attrs = merge_attrs_list[i]\n            if method == \"matching_node\":\n                self.logger.debug(\n                    f\"Merging model {i+1} by node with match_by={match_by}, \"\n                    f\"attribute_name={attribute_name}, merge_attrs={merge_attrs}\"\n                )\n                merged = merged.merge_matching_node(\n                    model, match_by=match_by, attribute_name=attribute_name, merge_attrs=merge_attrs\n                    )\n            elif method == \"matching_path\":\n                self.logger.debug(\n                    f\"Merging model {i+1} by path with match_by={match_by}, \"\n                    f\"attribute_name={attribute_name}, merge_attrs={merge_attrs}, \"\n                    f\"ignore_module_level={ignore_module_level}\"\n                )\n                merged = merged.merge_matching_path(\n                    model,\n                    match_by=match_by,\n                    attribute_name=attribute_name,\n                    merge_attrs=merge_attrs,\n                    ignore_module_level=ignore_module_level\n                )\n                self._add_missing_nodes_from_model(merged, model)\n        return merged\n\n    def _update_metadata(\n        self,\n        merged: SpecModel,\n        models: list[SpecModel],\n        merge_attrs_list: list,\n    ) -&gt; None:\n        \"\"\"Update the metadata of the merged model to reflect merged attributes.\"\"\"\n        # Start with the original metadata\n        meta = merged.metadata\n        orig_header = list(getattr(meta, \"header\", []))\n        orig_col2attr = dict(getattr(meta, \"column_to_attr\", {}))\n\n        # Find the next available column index\n        next_col = max(int(idx) for idx in orig_col2attr) + 1 if orig_col2attr else 0\n        # For each merged-in model, add new merged attributes if not already present\n        for i, model in enumerate(models[1:]):\n            merge_attrs = merge_attrs_list[i]\n            other_meta = getattr(model, \"metadata\", None)\n            if other_meta is not None and merge_attrs:\n                other_header = getattr(other_meta, \"header\", None)\n                other_col2attr = getattr(other_meta, \"column_to_attr\", None)\n                if other_header and other_col2attr:\n                    for idx, attr in other_col2attr.items():\n                        if attr in merge_attrs and attr not in orig_col2attr.values():\n                            # Add new column for this attribute\n                            if isinstance(other_header, list) and int(idx) &lt; len(other_header):\n                                orig_header.append(other_header[int(idx)])\n                            else:\n                                orig_header.append(attr)\n                            orig_col2attr[next_col] = attr\n                            next_col += 1\n\n        if hasattr(meta, \"header\"):\n            meta.header = orig_header\n        if hasattr(meta, \"column_to_attr\"):\n            meta.column_to_attr = orig_col2attr\n\n    def _save_cache(\n        self,\n        merged: SpecModel,\n        json_file_name: str,\n    ) -&gt; None:\n        \"\"\"Save the merged model to cache if a json_file_name is provided.\"\"\"\n        if json_file_name:\n            merged_json_file_path = os.path.join(\n                self.config.get_param(\"cache_dir\"), \"model\", json_file_name\n            )\n            try:\n                self.model_store.save(merged, merged_json_file_path)\n            except Exception as e:\n                self.logger.warning(f\"Failed to cache merged model to {merged_json_file_path}: {e}\")\n        else:\n            self.logger.info(\"No json_file_name specified; merged model not cached.\")\n\n    def _load_merged_model_from_cache(\n        self,\n        json_file_name: str,\n        force_update: bool,\n        merge_attrs_list: list = None,\n        orig_col2attr: dict = None,\n    ) -&gt; SpecModel | None:\n        \"\"\"Return the cached merged model if available, valid, and not force_update, else None.\"\"\"\n        merged_json_file_path = None\n        if json_file_name:\n            merged_json_file_path = os.path.join(\n                self.config.get_param(\"cache_dir\"), \"model\", json_file_name\n            )\n        if merged_json_file_path and os.path.exists(merged_json_file_path) and not force_update:\n            try:\n                model = self.model_store.load(merged_json_file_path)\n                # Check that all requested merge attributes are present in the cached model's metadata\n                if merge_attrs_list:\n                    all_attrs = set()\n                    for attrs in merge_attrs_list:\n                        if attrs:\n                            all_attrs.update(attrs)\n                    col2attr = getattr(model.metadata, \"column_to_attr\", {})\n                    orig_attrs = set(orig_col2attr.values()) if orig_col2attr else set()\n                    # All requested attributes must be present\n                    if any(attr not in col2attr.values() for attr in all_attrs):\n                        self.logger.info(\n                            f\"Cached model at {merged_json_file_path} missing required merged attributes {all_attrs}; \"\n                            f\"ignoring cache.\"\n                        )\n                        return None\n                    # No extra attributes except those in the original model\n                    allowed_attrs = all_attrs | orig_attrs\n                    extra_attrs = set(col2attr.values()) - allowed_attrs\n                    if extra_attrs:\n                        self.logger.info(\n                            f\"Cached model at {merged_json_file_path} contains extra attributes {extra_attrs} \"\n                            f\"not requested; ignoring cache.\"\n                        )\n                        return None\n                self.logger.info(\n                    f\"Loaded model from cache {merged_json_file_path}\"\n                )\n                return model\n            except Exception as e:\n                self.logger.warning(\n                    f\"Failed to load merged model from cache {merged_json_file_path}: {e}\"\n                )\n        return None\n\n    def _add_missing_nodes_from_model(self, merged, model):\n        \"\"\"Add nodes from model that are not present in merged (by path).\"\"\"\n\n        # Use elem_tag path normalized to uppercase for missing node detection\n        def tag_path(node):\n            return tuple(\n                getattr(n, \"elem_tag\", None).upper()\n                for n in node.path\n                if hasattr(n, \"elem_tag\") and getattr(n, \"elem_tag\", None)\n            )\n\n        merged_tag_paths = {tag_path(node) for node in PreOrderIter(merged.content) if getattr(node, \"elem_tag\", None)}\n        added_count = 0\n        for node2 in PreOrderIter(model.content):\n            node2_tag_path = tag_path(node2)\n            if (\n                node2_tag_path\n                and node2_tag_path not in merged_tag_paths\n                and hasattr(node2, \"elem_name\")\n                and hasattr(node2, \"elem_tag\")\n            ):\n                # Find parent by tag path\n                parent_tag_path = node2_tag_path[:-1]\n                parent = None\n                for n in PreOrderIter(merged.content):\n                    if tag_path(n) == parent_tag_path:\n                        parent = n\n                        break\n                elem_name = getattr(node2, \"elem_name\", \"\")\n                if (\n                    parent is not None\n                    and not (\n                        elem_name.startswith(\"All other Attributes\") or elem_name.startswith(\"All Attributes\")\n                    )\n                ):\n                    new_node = copy.deepcopy(node2)\n                    new_node.parent = parent\n                    merged_tag_paths.add(node2_tag_path)\n                    added_count += 1\n                    self.logger.debug(\n                        f\"Added missing node from model: {getattr(new_node, 'name', None)} \"\n                        f\"at tag_path {node2_tag_path}\"\n                    )\n\n        self.logger.info(f\"Total missing nodes added from model: {added_count}\")\n</code></pre>"},{"location":"api/spec_merger/#dcmspec.spec_merger.SpecMerger.__init__","title":"<code>__init__(config=None, model_store=None, logger=None)</code>","text":"<p>Initialize the SpecMerger.</p> <p>Sets up the logger for the merger. If no logger is provided, a default logger is created. If no model_store is provided, defaults to JSONSpecStore.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object. If None, a default Config is created.</p> <p> TYPE: <code>Optional[Config]</code> DEFAULT: <code>None</code> </p> <code>model_store</code> <p>Store for loading and saving models. Defaults to JSONSpecStore.</p> <p> TYPE: <code>Optional[SpecStore]</code> DEFAULT: <code>None</code> </p> <code>logger</code> <p>Logger instance to use. If None, a default logger is created.</p> <p> TYPE: <code>Optional[Logger]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/dcmspec/spec_merger.py</code> <pre><code>def __init__(self, config: Config = None, model_store: SpecStore = None, logger: logging.Logger = None):\n    \"\"\"Initialize the SpecMerger.\n\n    Sets up the logger for the merger. If no logger is provided, a default logger is created.\n    If no model_store is provided, defaults to JSONSpecStore.\n\n    Args:\n        config (Optional[Config]): Configuration object. If None, a default Config is created.\n        model_store (Optional[SpecStore]): Store for loading and saving models. Defaults to JSONSpecStore.\n        logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n    \"\"\"\n    self.logger = logger or logging.getLogger(self.__class__.__name__)\n    self.config = config or Config()\n    self.model_store = model_store or JSONSpecStore(logger=self.logger)\n</code></pre>"},{"location":"api/spec_merger/#dcmspec.spec_merger.SpecMerger.merge_many","title":"<code>merge_many(models, method, match_by, attribute_names=None, merge_attrs_list=None, json_file_name=None, force_update=False, ignore_module_level=False)</code>","text":"<p>Merge a sequence of DICOM SpecModel objects using the specified merge method, with optional caching.</p> <p>This method merges a list of models in order, applying either path-based or node-based merging at each step. You can specify different attribute names and lists of attributes to merge for each step, allowing for flexible, multi-stage enrichment of DICOM models. If json_file_name is provided, the merged model will be cached to that file, and loaded from cache if available and force_update is False.</p> PARAMETER DESCRIPTION <code>models</code> <p>The models to merge, in order.</p> <p> TYPE: <code>list of SpecModel</code> </p> <code>method</code> <p>Merge method to use (\"matching_path\" or \"matching_node\").</p> <p> TYPE: <code>str</code> </p> <code>match_by</code> <p>\"name\" to match by node name, \"attribute\" to match by a specific attribute.</p> <p> TYPE: <code>str</code> </p> <code>attribute_names</code> <p>List of attribute names to use for each merge step. Each entry corresponds to a merge operation between two models. Required if match_by=\"attribute\". If match_by=\"name\", can be None.</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>merge_attrs_list</code> <p>List of lists of attribute names to merge for each merge step. Each entry corresponds to a merge operation between two models.</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>json_file_name</code> <p>If provided, cache/load the merged model to/from this file.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>force_update</code> <p>If True, always perform the merge and overwrite the cache.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_module_level</code> <p>If True, skip the module level when matching paths (only applies to path-based merging).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>SpecModel</code> <p>The final merged SpecModel instance.</p> <p> TYPE: <code>SpecModel</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If models is empty, method is unknown, or attribute_names/merge_attrs_list have incorrect length, or if attribute_names is not set when match_by=\"attribute\".</p> Note <ul> <li>For path-based merging of DICOM attributes, it is recommended to use match_by=\"attribute\"   and attribute_names=[\"elem_tag\", ...] for robust, tag-based matching.</li> <li>For node-based merging or special cases, match_by=\"name\" can be used and attribute_names may be None.</li> </ul> Source code in <code>src/dcmspec/spec_merger.py</code> <pre><code>def merge_many(\n    self,\n    models: list[SpecModel],\n    method: str,\n    match_by: str,\n    attribute_names: list = None,\n    merge_attrs_list: list = None,\n    json_file_name: str = None,\n    force_update: bool = False,\n    ignore_module_level: bool = False,\n) -&gt; SpecModel:\n    \"\"\"Merge a sequence of DICOM SpecModel objects using the specified merge method, with optional caching.\n\n    This method merges a list of models in order, applying either path-based or node-based\n    merging at each step. You can specify different attribute names and lists of attributes\n    to merge for each step, allowing for flexible, multi-stage enrichment of DICOM models.\n    If json_file_name is provided, the merged model will be cached to that file, and loaded from\n    cache if available and force_update is False.\n\n    Args:\n        models (list of SpecModel): The models to merge, in order.\n        method (str): Merge method to use (\"matching_path\" or \"matching_node\").\n        match_by (str): \"name\" to match by node name, \"attribute\" to match by a specific attribute.\n        attribute_names (list, optional): List of attribute names to use for each merge step.\n            Each entry corresponds to a merge operation between two models.\n            Required if match_by=\"attribute\". If match_by=\"name\", can be None.\n        merge_attrs_list (list, optional): List of lists of attribute names to merge for each merge step.\n            Each entry corresponds to a merge operation between two models.\n        json_file_name (str, optional): If provided, cache/load the merged model to/from this file.\n        force_update (bool, optional): If True, always perform the merge and overwrite the cache.\n        ignore_module_level (bool, optional): If True, skip the module level when matching paths (only applies\n            to path-based merging).\n\n    Returns:\n        SpecModel: The final merged SpecModel instance.\n\n    Raises:\n        ValueError: If models is empty, method is unknown, or attribute_names/merge_attrs_list\n            have incorrect length, or if attribute_names is not set when match_by=\"attribute\".\n\n    Note:\n        - For path-based merging of DICOM attributes, it is recommended to use match_by=\"attribute\"\n          and attribute_names=[\"elem_tag\", ...] for robust, tag-based matching.\n        - For node-based merging or special cases, match_by=\"name\" can be used and attribute_names may be None.\n\n    \"\"\"\n    # Check that required arguments are set\n    if method is None or match_by is None:\n        raise ValueError(\n            \"merge_many requires method and match_by to be set explicitly by the caller.\"\n        )\n    if match_by == \"attribute\" and (attribute_names is None or any(a is None for a in attribute_names)):\n        raise ValueError(\n            \"merge_many requires attribute_names to be set when match_by='attribute'.\"\n        )\n    orig_col2attr = None\n    if models and hasattr(models[0].metadata, \"column_to_attr\"):\n        orig_col2attr = models[0].metadata.column_to_attr\n    cached_model = self._load_merged_model_from_cache(json_file_name, force_update, merge_attrs_list, orig_col2attr)\n    if cached_model is not None:\n        return cached_model\n\n    self._validate_merge_args(models, attribute_names, merge_attrs_list)\n    merged = self._merge_models(\n        models,\n        method=method,\n        match_by=match_by,\n        attribute_names=attribute_names,\n        merge_attrs_list=merge_attrs_list,\n        ignore_module_level=ignore_module_level,\n    )\n    self._update_metadata(merged, models, merge_attrs_list)\n    self._save_cache(merged, json_file_name)\n    return merged\n</code></pre>"},{"location":"api/spec_merger/#dcmspec.spec_merger.SpecMerger.merge_node","title":"<code>merge_node(model1, model2, match_by='name', attribute_name=None, merge_attrs=None, json_file_name=None, force_update=False)</code>","text":"<p>Merge two DICOM SpecModel objects using the node merge method, with optional caching.</p> <p>This is a convenience method that calls merge_many with two models.</p> PARAMETER DESCRIPTION <code>model1</code> <p>The first model.</p> <p> TYPE: <code>SpecModel</code> </p> <code>model2</code> <p>The second model to merge with the first.</p> <p> TYPE: <code>SpecModel</code> </p> <code>match_by</code> <p>\"name\" to match by node name, \"attribute\" to match by a specific attribute.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'name'</code> </p> <code>attribute_name</code> <p>The attribute name to use for matching.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>merge_attrs</code> <p>List of attribute names to merge from the other model's node.</p> <p> TYPE: <code>list[str]</code> DEFAULT: <code>None</code> </p> <code>json_file_name</code> <p>If provided, cache/load the merged model to/from this file.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>force_update</code> <p>If True, always perform the merge and overwrite the cache.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>SpecModel</code> <p>The merged SpecModel instance.</p> <p> TYPE: <code>SpecModel</code> </p> Source code in <code>src/dcmspec/spec_merger.py</code> <pre><code>def merge_node(\n    self,\n    model1: SpecModel,\n    model2: SpecModel,\n    match_by: str = \"name\",\n    attribute_name: str = None,\n    merge_attrs: list[str] = None,\n    json_file_name: str = None,\n    force_update: bool = False,\n) -&gt; SpecModel:\n    \"\"\"Merge two DICOM SpecModel objects using the node merge method, with optional caching.\n\n    This is a convenience method that calls merge_many with two models.\n\n    Args:\n        model1 (SpecModel): The first model.\n        model2 (SpecModel): The second model to merge with the first.\n        match_by (str, optional): \"name\" to match by node name, \"attribute\" to match by a specific attribute.\n        attribute_name (str, optional): The attribute name to use for matching.\n        merge_attrs (list[str], optional): List of attribute names to merge from the other model's node.\n        json_file_name (str, optional): If provided, cache/load the merged model to/from this file.\n        force_update (bool, optional): If True, always perform the merge and overwrite the cache.\n\n    Returns:\n        SpecModel: The merged SpecModel instance.\n\n    \"\"\"\n    return self.merge_many(\n        [model1, model2],\n        method = \"matching_node\",\n        match_by=match_by,\n        attribute_names=[attribute_name],\n        merge_attrs_list=[merge_attrs],\n        json_file_name=json_file_name,\n        force_update=force_update,\n    )\n</code></pre>"},{"location":"api/spec_merger/#dcmspec.spec_merger.SpecMerger.merge_path","title":"<code>merge_path(model1, model2, match_by='attribute', attribute_name='elem_tag', merge_attrs=None, json_file_name=None, force_update=False, ignore_module_level=False)</code>","text":"<p>Merge two DICOM SpecModel objects using the path merge method, with optional caching.</p> <p>This is a convenience method that calls merge_many with two models.</p> <p>By default, this method matches nodes by their DICOM tag (attribute_name=\"elem_tag\") using path-based merging (match_by=\"attribute\"). This is the recommended and robust approach for DICOM attribute-level merging, as DICOM tags are unique and consistent identifiers.</p> PARAMETER DESCRIPTION <code>model1</code> <p>The first model.</p> <p> TYPE: <code>SpecModel</code> </p> <code>model2</code> <p>The second model to merge with the first.</p> <p> TYPE: <code>SpecModel</code> </p> <code>match_by</code> <p>\"attribute\" (default, recommended) to match by a specific attribute (DICOM tag), or \"name\" to match by node name.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'attribute'</code> </p> <code>attribute_name</code> <p>The attribute name to use for matching (default: \"elem_tag\").</p> <p> TYPE: <code>str</code> DEFAULT: <code>'elem_tag'</code> </p> <code>merge_attrs</code> <p>List of attribute names to merge from the other model's node.</p> <p> TYPE: <code>list[str]</code> DEFAULT: <code>None</code> </p> <code>json_file_name</code> <p>If provided, cache/load the merged model to/from this file.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>force_update</code> <p>If True, always perform the merge and overwrite the cache.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_module_level</code> <p>If True, skip the module level when matching paths.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>SpecModel</code> <p>The merged SpecModel instance.</p> <p> TYPE: <code>SpecModel</code> </p> Note <p>For DICOM attribute-level merging, the default (match_by=\"attribute\", attribute_name=\"elem_tag\") is strongly recommended. Only use match_by=\"name\" for special cases where tag-based matching is not possible.</p> Source code in <code>src/dcmspec/spec_merger.py</code> <pre><code>def merge_path(\n    self,\n    model1: SpecModel,\n    model2: SpecModel,\n    match_by: str = \"attribute\",\n    attribute_name: str = \"elem_tag\",\n    merge_attrs: list[str] = None,\n    json_file_name: str = None,\n    force_update: bool = False,\n    ignore_module_level: bool = False,\n) -&gt; SpecModel:\n    \"\"\"Merge two DICOM SpecModel objects using the path merge method, with optional caching.\n\n    This is a convenience method that calls merge_many with two models.\n\n    By default, this method matches nodes by their DICOM tag (attribute_name=\"elem_tag\") using\n    path-based merging (match_by=\"attribute\"). This is the recommended and robust approach for\n    DICOM attribute-level merging, as DICOM tags are unique and consistent identifiers.\n\n    Args:\n        model1 (SpecModel): The first model.\n        model2 (SpecModel): The second model to merge with the first.\n        match_by (str, optional): \"attribute\" (default, recommended) to match by a specific attribute (DICOM tag),\n            or \"name\" to match by node name.\n        attribute_name (str, optional): The attribute name to use for matching (default: \"elem_tag\").\n        merge_attrs (list[str], optional): List of attribute names to merge from the other model's node.\n        json_file_name (str, optional): If provided, cache/load the merged model to/from this file.\n        force_update (bool, optional): If True, always perform the merge and overwrite the cache.\n        ignore_module_level (bool, optional): If True, skip the module level when matching paths.\n\n    Returns:\n        SpecModel: The merged SpecModel instance.\n\n    Note:\n        For DICOM attribute-level merging, the default (match_by=\"attribute\", attribute_name=\"elem_tag\")\n        is strongly recommended. Only use match_by=\"name\" for special cases where tag-based matching is\n        not possible.\n\n    \"\"\"\n    return self.merge_many(\n        [model1, model2],\n        method = \"matching_path\",\n        match_by=match_by,\n        attribute_names=[attribute_name],\n        merge_attrs_list=[merge_attrs],\n        json_file_name=json_file_name,\n        force_update=force_update,\n        ignore_module_level=ignore_module_level,\n    )\n</code></pre>"},{"location":"api/spec_merger/#dcmspec.spec_merger.SpecMerger.merge_path_with_default","title":"<code>merge_path_with_default(model1, model2, match_by='name', attribute_name=None, merge_attrs=None, default_attr='elem_type', default_value='3', default_value_func=None, json_file_name=None, force_update=False, ignore_module_level=False)</code>","text":"<p>Merge two DICOM SpecModel objects by path, and set a default value for missing attributes.</p> <p>This method merges two models using the path-based merge strategy (matching nodes by their hierarchical path and by DICOM tag, i.e., match_by=\"attribute\", attribute_name=\"elem_tag\" by default), and then sets <code>default_attr</code> to <code>default_value</code> for any node in the merged model that does not already have that attribute.</p> <p>This is especially useful for workflows where you want to enrich a normalized IOD model (e.g., from DICOM PS3.3) with a service attribute model (e.g., from DICOM PS3.4 or an IHE profile), and you want to ensure that all nodes in the merged model have a value for the Type attribute.</p> Use case <ul> <li>Merging a DICOM PS3.3 normalized IOD attributes specification (e.g., built with IODSpecBuilder)   with a DICOM PS3.4 DIMSE SCU or SCP attributes specification (e.g., built with ServiceAttributeModel   and select_dimse/select_role). After merging, any node present in the normalized IOD model but   missing from the service attribute model will have its \"Type\" (or other specified attribute)   set to the default value (e.g., \"3\"), ensuring the merged model is complete and ready for   further processing or export.</li> </ul> PARAMETER DESCRIPTION <code>model1</code> <p>The first model (e.g., normalized IOD).</p> <p> TYPE: <code>SpecModel</code> </p> <code>model2</code> <p>The second model (e.g., service attribute model).</p> <p> TYPE: <code>SpecModel</code> </p> <code>match_by</code> <p>\"attribute\" (default, recommended) to match by a specific attribute (DICOM tag), or \"name\" to match by node name.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'name'</code> </p> <code>attribute_name</code> <p>The attribute name to use for matching (default: \"elem_tag\").</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>merge_attrs</code> <p>List of attribute names to merge from the other model's node.</p> <p> TYPE: <code>list[str]</code> DEFAULT: <code>None</code> </p> <code>default_attr</code> <p>The attribute to set if missing (default: \"elem_type\").</p> <p> TYPE: <code>str</code> DEFAULT: <code>'elem_type'</code> </p> <code>default_value</code> <p>The value to set for missing attributes (default: \"3\").</p> <p> TYPE: <code>str</code> DEFAULT: <code>'3'</code> </p> <code>default_value_func</code> <p>A function to determine the default value for missing attributes. If provided, it will be called as <code>default_value_func(node, merged_model, service_model, default_attr, default_value)</code> and should return the value to use for the missing attribute.</p> <p> TYPE: <code>callable</code> DEFAULT: <code>None</code> </p> <code>json_file_name</code> <p>If provided, cache/load the merged model to/from this file.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>force_update</code> <p>If True, always perform the merge and overwrite the cache.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_module_level</code> <p>If True, skip the module level when matching paths.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>SpecModel</code> <p>The merged SpecModel instance with default values set for missing attributes.</p> <p> TYPE: <code>SpecModel</code> </p> Note <p>For DICOM attribute-level merging, the default (match_by=\"attribute\", attribute_name=\"elem_tag\") is strongly recommended. Only use match_by=\"name\" for special cases where tag-based matching is not possible.</p> Source code in <code>src/dcmspec/spec_merger.py</code> <pre><code>def merge_path_with_default(\n    self,\n    model1: SpecModel,\n    model2: SpecModel,\n    match_by: str = \"name\",\n    attribute_name: str = None,\n    merge_attrs: list[str] = None,\n    default_attr: str = \"elem_type\",\n    default_value: str = \"3\",\n    default_value_func: callable = None,\n    json_file_name: str = None,\n    force_update: bool = False,\n    ignore_module_level: bool = False,\n\n) -&gt; SpecModel:\n    \"\"\"Merge two DICOM SpecModel objects by path, and set a default value for missing attributes.\n\n    This method merges two models using the path-based merge strategy (matching nodes by their\n    hierarchical path and by DICOM tag, i.e., match_by=\"attribute\", attribute_name=\"elem_tag\" by default),\n    and then sets `default_attr` to `default_value` for any node in the merged model that does not already\n    have that attribute.\n\n    This is especially useful for workflows where you want to enrich a normalized IOD model\n    (e.g., from DICOM PS3.3) with a service attribute model (e.g., from DICOM PS3.4 or an IHE\n    profile), and you want to ensure that all nodes in the merged model have a value for the\n    Type attribute.\n\n    Use case:\n        - Merging a DICOM PS3.3 normalized IOD attributes specification (e.g., built with IODSpecBuilder)\n          with a DICOM PS3.4 DIMSE SCU or SCP attributes specification (e.g., built with ServiceAttributeModel\n          and select_dimse/select_role). After merging, any node present in the normalized IOD model but\n          missing from the service attribute model will have its \"Type\" (or other specified attribute)\n          set to the default value (e.g., \"3\"), ensuring the merged model is complete and ready for\n          further processing or export.\n\n    Args:\n        model1 (SpecModel): The first model (e.g., normalized IOD).\n        model2 (SpecModel): The second model (e.g., service attribute model).\n        match_by (str, optional): \"attribute\" (default, recommended) to match by a specific attribute (DICOM tag),\n            or \"name\" to match by node name.\n        attribute_name (str, optional): The attribute name to use for matching (default: \"elem_tag\").\n        merge_attrs (list[str], optional): List of attribute names to merge from the other model's node.\n        default_attr (str, optional): The attribute to set if missing (default: \"elem_type\").\n        default_value (str, optional): The value to set for missing attributes (default: \"3\").\n        default_value_func (callable, optional): A function to determine the default value for missing attributes.\n            If provided, it will be called as\n            `default_value_func(node, merged_model, service_model, default_attr, default_value)`\n            and should return the value to use for the missing attribute.\n        json_file_name (str, optional): If provided, cache/load the merged model to/from this file.\n        force_update (bool, optional): If True, always perform the merge and overwrite the cache.\n        ignore_module_level (bool, optional): If True, skip the module level when matching paths.\n\n    Returns:\n        SpecModel: The merged SpecModel instance with default values set for missing attributes.\n\n    Note:\n        For DICOM attribute-level merging, the default (match_by=\"attribute\", attribute_name=\"elem_tag\")\n        is strongly recommended. Only use match_by=\"name\" for special cases where tag-based matching is\n        not possible.\n\n    \"\"\"\n    merged = self.merge_path(\n        model1,\n        model2,\n        match_by=match_by,\n        attribute_name=attribute_name,\n        merge_attrs=merge_attrs,\n        ignore_module_level=ignore_module_level,\n        json_file_name=json_file_name,\n        force_update=force_update,\n    )\n\n    for node in merged.content.descendants:\n        # Only set default_attr on nodes that have the match attribute (attribute_name) and are not module nodes\n        if (\n            attribute_name is not None\n            and hasattr(node, attribute_name)\n            and not hasattr(node, default_attr)\n        ):\n            if default_value_func is not None:\n                value = default_value_func(node, merged, model2, default_attr, default_value)\n            else:\n                value = default_value\n            setattr(node, default_attr, value)\n    return merged\n</code></pre>"},{"location":"api/spec_model/","title":"SpecModel","text":""},{"location":"api/spec_model/#dcmspec.spec_model.SpecModel","title":"<code>dcmspec.spec_model.SpecModel</code>","text":"<p>Represent a hierarchical information model from any table of DICOM documents.</p> <p>This class holds the DICOM specification model, structured into a hierarchical tree of DICOM components such as Data Elements, UIDs, Attributes, and others.</p> The model contains two main parts <ul> <li>metadata: a node holding table and document metadata</li> <li>content: a node holding the hierarchical content tree</li> </ul> <p>The model can be filtered.</p> Source code in <code>src/dcmspec/spec_model.py</code> <pre><code>class SpecModel:\n    \"\"\"Represent a hierarchical information model from any table of DICOM documents.\n\n    This class holds the DICOM specification model, structured into a hierarchical tree\n    of DICOM components such as Data Elements, UIDs, Attributes, and others.\n\n    The model contains two main parts:\n        - metadata: a node holding table and document metadata\n        - content: a node holding the hierarchical content tree\n\n    The model can be filtered.\n    \"\"\"\n\n    def __init__(\n        self,\n        metadata: Node,\n        content: Node,\n        logger: logging.Logger = None,\n    ):\n        \"\"\"Initialize the SpecModel.\n\n        Sets up the logger and initializes the specification model.\n\n        Args:\n            metadata (Node): Node holding table and document metadata, such as headers, version, and table ID.\n            content (Node): Node holding the hierarchical content tree of the DICOM specification.\n            logger (logging.Logger, optional): A pre-configured logger instance to use.\n                If None, a default logger will be created.\n\n        \"\"\"\n        self.logger = logger or logging.getLogger(self.__class__.__name__)\n        self.metadata = metadata\n        self.content = content\n\n    def exclude_titles(self) -&gt; None:\n        \"\"\"Remove nodes corresponding to title rows from the content tree.\n\n        Title rows are typically found in some DICOM tables and represent section headers\n        rather than actual data elements (such as Module titles in PS3.4). \n        This method traverses the content tree and removes any node identified as a title,\n        cleaning up the model for further processing.\n\n        The method operates on the content tree and does not affect the metadata node.\n\n        Returns:\n            None\n\n        \"\"\"\n        # Traverse the tree and remove nodes where is_title is True\n        for node in list(PreOrderIter(self.content)):\n            if self._is_title(node):\n                self.logger.debug(f\"Removing title node: {node.name}\")\n                node.parent = None\n\n    def filter_required(\n        self,\n        type_attr_name: str,\n        keep: Optional[list[str]] = None,\n        remove: Optional[list[str]] = None\n    ) -&gt; None:\n        \"\"\"Remove nodes that are considered optional according to DICOM requirements.\n\n        This method traverses the content tree and removes nodes whose requirement\n        (e.g., \"Type\", \"Matching\", or \"Return Key\") indicates that they are optional. \n        Nodes with conditional or required types (e.g., \"1\", \"1C\", \"2\", \"2C\")\n        are retained. The method can be customized by specifying which types to keep or remove.\n\n        Additionally, for nodes representing Sequences (node names containing \"_sequence\"), \n        this method removes all subelements if the sequence itself is not required or can be empty\n        (e.g., type \"3\", \"2\", \"2C\", \"-\", \"O\", or \"Not allowed\").\n\n        Args:\n            type_attr_name (str): Name of the node attribute holding the optionality requirement,\n                for example \"Type\" of an attribute, \"Matching\", or \"Return Key\".\n            keep (Optional[list[str]]): List of type values to keep (default: [\"1\", \"1C\", \"2\", \"2C\"]).\n            remove (Optional[list[str]]): List of type values to remove (default: [\"3\"]).\n\n        Returns:\n            None\n\n        \"\"\"\n        if keep is None:\n            keep = [\"1\", \"1C\", \"2\", \"2C\"]\n        if remove is None:\n            remove = [\"3\"]\n        types_to_keep = keep\n        types_to_remove = remove\n        attribute_name = type_attr_name\n\n        for node in PreOrderIter(self.content):\n            if hasattr(node, attribute_name):\n                dcmtype = getattr(node, attribute_name)\n                if dcmtype in types_to_remove and dcmtype not in types_to_keep:\n                    self.logger.debug(f\"[{dcmtype.rjust(3)}] : Removing {node.name} element\")\n                    node.parent = None\n                # Remove nodes under \"Sequence\" nodes which are not required or which can be empty\n                if \"_sequence\" in node.name and dcmtype in [\"3\", \"2\", \"2C\", \"-\", \"O\", \"Not allowed\"]:\n                    self.logger.debug(f\"[{dcmtype.rjust(3)}] : Removing {node.name} subelements\")\n                    for descendant in node.descendants:\n                        descendant.parent = None\n\n    def merge_matching_path(\n        self,\n        other: \"SpecModel\",\n        match_by: str = \"name\",\n        attribute_name: Optional[str] = None,\n        merge_attrs: Optional[list[str]] = None,\n        ignore_module_level: bool = False,\n    ) -&gt; \"SpecModel\":\n        \"\"\"Merge with another SpecModel, producing a new model with attributes merged for nodes with matching paths.\n\n        The path for matching is constructed at each level using either the node's `name`\n        (if match_by=\"name\") or a specified attribute (if match_by=\"attribute\" and attribute_name is given).\n        Only nodes whose full path matches (by the chosen key) will be merged.\n\n        This method is useful for combining DICOM specification models from different parts of the standard.\n        For example, it can be used to merge a PS3.3 model of a normalized IOD specification with a PS3.4 model of a\n        SOP class specification.\n\n        Args:\n            other (SpecModel): The other model to merge with the current model.\n            match_by (str): \"name\" to match by node.name path, \"attribute\" to match by a specific attribute path.\n            attribute_name (str, optional): The attribute name to use for matching if match_by=\"attribute\".\n            merge_attrs (list[str], optional): List of attribute names to merge from the other model's node.\n            ignore_module_level (bool, optional): If True, skip the module level in the path for matching.\n\n        Returns:\n            SpecModel: A new merged SpecModel.\n\n        \"\"\"        \n        return self._merge_nodes(\n            other,\n            match_by=match_by,\n            attribute_name=attribute_name,\n            merge_attrs=merge_attrs,\n            is_path_based=True,\n            ignore_module_level=ignore_module_level\n        )\n\n    def merge_matching_node(\n        self,\n        other: \"SpecModel\",\n        match_by: str = \"name\",\n        attribute_name: Optional[str] = None,\n        merge_attrs: Optional[list[str]] = None,\n    ) -&gt; \"SpecModel\":\n        \"\"\"Merge two SpecModel trees by matching nodes at any level using a single key (name or attribute).\n\n        For each node in the current model, this method finds a matching node in the other model\n        using either the node's name (if match_by=\"name\") or a specified attribute (if match_by=\"attribute\").\n        If a match is found, the specified attributes from the other model's node are merged into the current node.\n\n        This is useful for enrichment scenarios, such as adding VR/VM/Keyword from the Part 6 dictionary\n        to a Part 3 module, where nodes are matched by a unique attribute like elem_tag.\n\n        - Matching is performed globally (not by path): any node in the current model is matched to any node\n          in the other model with the same key value, regardless of their position in the tree.\n        - It is expected that there is only one matching node per key in the other model.\n        - If multiple nodes in the other model have the same key, a warning is logged and only the last one\n          found in pre-order traversal is used for merging.\n\n        Example use cases:\n            - Enrich a PS3.3 module attribute specification with VR/VM from the PS3.6 data elements dictionary.\n            - Merge any two models where a unique key (name or attribute) can be used for node correspondence.\n\n        Args:\n            other (SpecModel): The other model to merge with the current model.\n            match_by (str): \"name\" to match by node.name (stripped of leading '&gt;' and whitespace),\n                or \"attribute\" to match by a specific attribute value.\n            attribute_name (str, optional): The attribute name to use for matching if match_by=\"attribute\".\n            merge_attrs (list[str], optional): List of attribute names to merge from the other model's node.\n\n        Returns:\n            SpecModel: A new merged SpecModel with attributes from the other model merged in.\n\n        Raises:\n            ValueError: If match_by is invalid or attribute_name is missing when required.\n\n        \"\"\"        \n        return self._merge_nodes(\n            other,\n            match_by=match_by,\n            attribute_name=attribute_name,\n            merge_attrs=merge_attrs,\n            is_path_based=False\n        )\n    def _strip_leading_gt(self, name):\n        \"\"\"Strip leading '&gt;' and whitespace from a node name for matching.\"\"\"\n        return name.lstrip(\"&gt;\").lstrip().rstrip() if isinstance(name, str) else name\n\n    def _is_include(self, node: Node) -&gt; bool:\n        \"\"\"Determine if a node represents an 'Include' of a Macro table.\n\n        Args:\n            node: The node to check.\n\n        Returns:\n            True if the node represents an 'Include' of a Macro table, False otherwise.\n\n        \"\"\"\n        return \"include_table\" in node.name\n\n    def _is_title(self, node: Node) -&gt; bool:\n        \"\"\"Determine if a node is a title.\n\n        Args:\n            node: The node to check.\n\n        Returns:\n            True if the node is a title, False otherwise.\n\n        \"\"\"\n        return (\n            self._has_only_key_0_attr(node, self.metadata.column_to_attr)\n            and not self._is_include(node)\n            and node.name != \"content\"\n        )\n\n    def _has_only_key_0_attr(self, node: Node, column_to_attr: Dict[int, str]) -&gt; bool:\n        # sourcery skip: merge-duplicate-blocks, use-any\n        \"\"\"Check that only the key 0 attribute is present.\n\n        Determines if a node has only the attribute specified by the item with key \"0\"\n        in column_to_attr, corresponding to the first column of the table.\n\n        Args:\n            node: The node to check.\n            column_to_attr: Mapping between column number and attribute name.\n\n        Returns:\n            True if the node has only the key \"0\" attribute, False otherwise.\n\n        \"\"\"\n        # Irrelevant if columns 0 not extracted\n        if 0 not in column_to_attr:\n            return False\n\n        key_0_attr = column_to_attr[0]\n        # key 0 must be present and not None\n        if not hasattr(node, key_0_attr) or getattr(node, key_0_attr) is None:\n            return False\n\n        # all other keys must be absent or None\n        for key, attr_name in column_to_attr.items():\n            if key == 0:\n                continue\n            if hasattr(node, attr_name) and getattr(node, attr_name) is not None:\n                return False\n        return True\n\n\n    @staticmethod\n    def _get_node_path(node: Node, attr: str = \"name\") -&gt; tuple:\n        \"\"\"Return a tuple representing the path of the node using the given attribute.\"\"\"\n        return tuple(getattr(n, attr, None) for n in node.path)\n\n\n    @staticmethod\n    def _get_path_by_name(node: Node) -&gt; tuple:\n        \"\"\"Return the path of the node using node.name at each level.\"\"\"\n        return SpecModel._get_node_path(node, \"name\")\n\n    @staticmethod\n    def _get_path_by_attr(node: Node, attr: str) -&gt; tuple:\n        \"\"\"Return the path of the node using the given attribute at each level.\"\"\"\n        return SpecModel._get_node_path(node, attr)\n\n    def _build_node_map(\n        self,\n        other: \"SpecModel\",\n        match_by: str,\n        attribute_name: Optional[str] = None,\n        is_path_based: bool = False\n    ) -&gt; tuple[dict, callable]:\n        \"\"\"Construct a mapping from keys to nodes in the other model, and a key function for matching.\n\n        This method prepares the data structures needed for merging two SpecModel trees. It builds a mapping\n        from a key (either a node's name, a specified attribute, or a path of such values) to nodes in the\n        `other` model, and returns a function that computes the same key for nodes in the current model.\n\n        Args:\n            other (SpecModel): The other model to merge with.\n            match_by (str): \"name\" to match by node name, or \"attribute\" to match by a specific attribute.\n            attribute_name (str, optional): The attribute name to use for matching if match_by=\"attribute\".\n            is_path_based (bool): If True, use the full path of names/attributes as the key; if False, \n                use only the value.\n\n        Returns:\n            tuple: (node_map, key_func)\n                node_map (dict): Mapping from key to node in the other model.\n                key_func (callable): Function that computes the key for a node in the current model.\n\n        Raises:\n            ValueError: If match_by is invalid or attribute_name is missing when required.\n\n        \"\"\"\n        if match_by == \"name\":\n            self.logger.debug(\"Matching models by node name.\")\n            if is_path_based:\n                node_map = {\n                    self._get_path_by_name(node): node\n                    for node in PreOrderIter(other.content)\n                }\n                def key_func(node):\n                    return self._get_path_by_name(node)\n            else:\n                def key_func(node):\n                    return self._strip_leading_gt(node.name)\n                # Build mapping with handling of duplicates\n                key_to_nodes = defaultdict(list)\n                for node in PreOrderIter(other.content):\n                    key = key_func(node)\n                    key_to_nodes[key].append(node)\n\n                self._warn_multiple_matches(key_to_nodes)\n                node_map = {key: nodes[-1] for key, nodes in key_to_nodes.items()}\n\n        elif match_by == \"attribute\" and attribute_name:\n            self.logger.debug(f\"Matching models by attribute: {attribute_name}\")\n            if is_path_based:\n                node_map = {\n                    self._get_path_by_attr(node, attribute_name): node\n                    for node in PreOrderIter(other.content)\n                }\n                def key_func(node):\n                    return self._get_path_by_attr(node, attribute_name)\n            else:\n                def key_func(node):\n                    return getattr(node, attribute_name, None)\n                # Build mapping with handling of duplicates\n                key_to_nodes = defaultdict(list)\n                for node in PreOrderIter(other.content):\n                    key = key_func(node)\n                    key_to_nodes[key].append(node)\n\n                self._warn_multiple_matches(key_to_nodes)\n                node_map = {key: nodes[-1] for key, nodes in key_to_nodes.items()}\n        else:\n            raise ValueError(\"Invalid match_by or missing attribute_name\")\n\n        return node_map, key_func\n\n    def _warn_multiple_matches(self, key_to_nodes: dict):\n        \"\"\"Log a warning if any key in the mapping corresponds to multiple nodes.\n\n        Args:\n            key_to_nodes (dict): A mapping from key to a list of nodes with that key.\n\n        Returns:\n            None\n\n        \"\"\"\n        for key, nodes in key_to_nodes.items():\n            if key is not None and len(nodes) &gt; 1:\n                self.logger.warning(\n                    f\"Multiple nodes found for key '{key}': \"\n                    f\"{[getattr(n, 'name', None) for n in nodes]}. \"\n                    \"Only the last one will be used for merging.\"\n                )\n\n    def _merge_nodes(\n        self,\n        other: \"SpecModel\",\n        match_by: str,\n        attribute_name: Optional[str] = None,\n        merge_attrs: Optional[list[str]] = None,\n        is_path_based: bool = False,\n        ignore_module_level: bool = False,\n    ) -&gt; \"SpecModel\":\n        \"\"\"Merge this SpecModel with another, enriching nodes by matching keys.\n\n        This is the core logic for merging two SpecModel trees. For each node in the current model,\n        it attempts to find a matching node in the other model using the specified matching strategy.\n        If a match is found, the specified attributes from the other node are copied into the current node.\n\n        Args:\n            other (SpecModel): The other model to merge from.\n            match_by (str): \"name\" to match by node name, or \"attribute\" to match by a specific attribute.\n            attribute_name (str, optional): The attribute name to use for matching if match_by=\"attribute\".\n            merge_attrs (list[str], optional): List of attribute names to copy from the matching node.\n            is_path_based (bool): If True, match nodes by their full path; if False, match globally by key.\n            ignore_module_level (bool): If True, skip the module level in the path for matching.\n\n        Returns:\n            SpecModel: A deep copy of this model, with attributes merged from the other model where matches are found.\n\n        Notes:\n            - If multiple nodes in the other model have the same key, only the last one is used (a warning is logged).\n            - If a node in this model has no match in the other model, it is left unchanged.\n            - The merge is non-destructive: a new SpecModel is returned.\n\n        \"\"\"\n        merged = copy.deepcopy(self)\n        merged.logger = self.logger \n\n        if is_path_based and ignore_module_level:\n            # Build node_map with stripped paths\n            if match_by == \"name\":\n                node_map = {\n                    self._strip_module_level(self._get_path_by_name(node)): node\n                    for node in PreOrderIter(other.content)\n                }\n                def key_func(node):\n                    return self._strip_module_level(self._get_path_by_name(node))\n            elif match_by == \"attribute\" and attribute_name:\n                node_map = {\n                    self._strip_module_level(self._get_path_by_attr(node, attribute_name)): node\n                    for node in PreOrderIter(other.content)\n                }\n                def key_func(node):\n                    return self._strip_module_level(self._get_path_by_attr(node, attribute_name))\n            else:\n                raise ValueError(\"Invalid match_by or missing attribute_name\")\n        else:\n            node_map, key_func = self._build_node_map(\n                other, match_by, attribute_name, is_path_based\n            )\n\n        enriched_count = 0\n        total_nodes = 0\n        for node in PreOrderIter(merged.content):\n            total_nodes += 1\n            key = key_func(node)\n\n            if key in node_map and key is not None:\n                other_node = node_map[key]\n                enriched_this_node = False\n                for attr in (merge_attrs or []):\n                    if attr is not None and hasattr(other_node, attr):\n                        setattr(node, attr, getattr(other_node, attr))\n                        attr_val = getattr(other_node, attr)\n                        self.logger.debug(\n                            f\"Enriched node {getattr(node, 'name', None)} \"\n                            f\"(key={key}) with {attr}={str(attr_val)[:10]}\"\n                        )\n                        enriched_this_node = True\n                if enriched_this_node:\n                    enriched_count += 1\n\n        self.logger.info(f\"Total nodes enriched during merge: {enriched_count} / {total_nodes}\")\n        return merged\n\n    def _strip_module_level(self, path_tuple):\n        # Remove all but the last leading None or the module level for path matching\n        # This ensures (None, None, '(0010,0010)') and (None, '(0010,0010)') both become (None, '(0010,0010)')\n        path = list(path_tuple)\n        while len(path) &gt; 2 and path[0] is None:\n            path.pop(0)\n        return tuple(path)\n</code></pre>"},{"location":"api/spec_model/#dcmspec.spec_model.SpecModel.__init__","title":"<code>__init__(metadata, content, logger=None)</code>","text":"<p>Initialize the SpecModel.</p> <p>Sets up the logger and initializes the specification model.</p> PARAMETER DESCRIPTION <code>metadata</code> <p>Node holding table and document metadata, such as headers, version, and table ID.</p> <p> TYPE: <code>Node</code> </p> <code>content</code> <p>Node holding the hierarchical content tree of the DICOM specification.</p> <p> TYPE: <code>Node</code> </p> <code>logger</code> <p>A pre-configured logger instance to use. If None, a default logger will be created.</p> <p> TYPE: <code>Logger</code> DEFAULT: <code>None</code> </p> Source code in <code>src/dcmspec/spec_model.py</code> <pre><code>def __init__(\n    self,\n    metadata: Node,\n    content: Node,\n    logger: logging.Logger = None,\n):\n    \"\"\"Initialize the SpecModel.\n\n    Sets up the logger and initializes the specification model.\n\n    Args:\n        metadata (Node): Node holding table and document metadata, such as headers, version, and table ID.\n        content (Node): Node holding the hierarchical content tree of the DICOM specification.\n        logger (logging.Logger, optional): A pre-configured logger instance to use.\n            If None, a default logger will be created.\n\n    \"\"\"\n    self.logger = logger or logging.getLogger(self.__class__.__name__)\n    self.metadata = metadata\n    self.content = content\n</code></pre>"},{"location":"api/spec_model/#dcmspec.spec_model.SpecModel.exclude_titles","title":"<code>exclude_titles()</code>","text":"<p>Remove nodes corresponding to title rows from the content tree.</p> <p>Title rows are typically found in some DICOM tables and represent section headers rather than actual data elements (such as Module titles in PS3.4).  This method traverses the content tree and removes any node identified as a title, cleaning up the model for further processing.</p> <p>The method operates on the content tree and does not affect the metadata node.</p> RETURNS DESCRIPTION <code>None</code> <p>None</p> Source code in <code>src/dcmspec/spec_model.py</code> <pre><code>def exclude_titles(self) -&gt; None:\n    \"\"\"Remove nodes corresponding to title rows from the content tree.\n\n    Title rows are typically found in some DICOM tables and represent section headers\n    rather than actual data elements (such as Module titles in PS3.4). \n    This method traverses the content tree and removes any node identified as a title,\n    cleaning up the model for further processing.\n\n    The method operates on the content tree and does not affect the metadata node.\n\n    Returns:\n        None\n\n    \"\"\"\n    # Traverse the tree and remove nodes where is_title is True\n    for node in list(PreOrderIter(self.content)):\n        if self._is_title(node):\n            self.logger.debug(f\"Removing title node: {node.name}\")\n            node.parent = None\n</code></pre>"},{"location":"api/spec_model/#dcmspec.spec_model.SpecModel.filter_required","title":"<code>filter_required(type_attr_name, keep=None, remove=None)</code>","text":"<p>Remove nodes that are considered optional according to DICOM requirements.</p> <p>This method traverses the content tree and removes nodes whose requirement (e.g., \"Type\", \"Matching\", or \"Return Key\") indicates that they are optional.  Nodes with conditional or required types (e.g., \"1\", \"1C\", \"2\", \"2C\") are retained. The method can be customized by specifying which types to keep or remove.</p> <p>Additionally, for nodes representing Sequences (node names containing \"_sequence\"),  this method removes all subelements if the sequence itself is not required or can be empty (e.g., type \"3\", \"2\", \"2C\", \"-\", \"O\", or \"Not allowed\").</p> PARAMETER DESCRIPTION <code>type_attr_name</code> <p>Name of the node attribute holding the optionality requirement, for example \"Type\" of an attribute, \"Matching\", or \"Return Key\".</p> <p> TYPE: <code>str</code> </p> <code>keep</code> <p>List of type values to keep (default: [\"1\", \"1C\", \"2\", \"2C\"]).</p> <p> TYPE: <code>Optional[list[str]]</code> DEFAULT: <code>None</code> </p> <code>remove</code> <p>List of type values to remove (default: [\"3\"]).</p> <p> TYPE: <code>Optional[list[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>None</code> <p>None</p> Source code in <code>src/dcmspec/spec_model.py</code> <pre><code>def filter_required(\n    self,\n    type_attr_name: str,\n    keep: Optional[list[str]] = None,\n    remove: Optional[list[str]] = None\n) -&gt; None:\n    \"\"\"Remove nodes that are considered optional according to DICOM requirements.\n\n    This method traverses the content tree and removes nodes whose requirement\n    (e.g., \"Type\", \"Matching\", or \"Return Key\") indicates that they are optional. \n    Nodes with conditional or required types (e.g., \"1\", \"1C\", \"2\", \"2C\")\n    are retained. The method can be customized by specifying which types to keep or remove.\n\n    Additionally, for nodes representing Sequences (node names containing \"_sequence\"), \n    this method removes all subelements if the sequence itself is not required or can be empty\n    (e.g., type \"3\", \"2\", \"2C\", \"-\", \"O\", or \"Not allowed\").\n\n    Args:\n        type_attr_name (str): Name of the node attribute holding the optionality requirement,\n            for example \"Type\" of an attribute, \"Matching\", or \"Return Key\".\n        keep (Optional[list[str]]): List of type values to keep (default: [\"1\", \"1C\", \"2\", \"2C\"]).\n        remove (Optional[list[str]]): List of type values to remove (default: [\"3\"]).\n\n    Returns:\n        None\n\n    \"\"\"\n    if keep is None:\n        keep = [\"1\", \"1C\", \"2\", \"2C\"]\n    if remove is None:\n        remove = [\"3\"]\n    types_to_keep = keep\n    types_to_remove = remove\n    attribute_name = type_attr_name\n\n    for node in PreOrderIter(self.content):\n        if hasattr(node, attribute_name):\n            dcmtype = getattr(node, attribute_name)\n            if dcmtype in types_to_remove and dcmtype not in types_to_keep:\n                self.logger.debug(f\"[{dcmtype.rjust(3)}] : Removing {node.name} element\")\n                node.parent = None\n            # Remove nodes under \"Sequence\" nodes which are not required or which can be empty\n            if \"_sequence\" in node.name and dcmtype in [\"3\", \"2\", \"2C\", \"-\", \"O\", \"Not allowed\"]:\n                self.logger.debug(f\"[{dcmtype.rjust(3)}] : Removing {node.name} subelements\")\n                for descendant in node.descendants:\n                    descendant.parent = None\n</code></pre>"},{"location":"api/spec_model/#dcmspec.spec_model.SpecModel.merge_matching_node","title":"<code>merge_matching_node(other, match_by='name', attribute_name=None, merge_attrs=None)</code>","text":"<p>Merge two SpecModel trees by matching nodes at any level using a single key (name or attribute).</p> <p>For each node in the current model, this method finds a matching node in the other model using either the node's name (if match_by=\"name\") or a specified attribute (if match_by=\"attribute\"). If a match is found, the specified attributes from the other model's node are merged into the current node.</p> <p>This is useful for enrichment scenarios, such as adding VR/VM/Keyword from the Part 6 dictionary to a Part 3 module, where nodes are matched by a unique attribute like elem_tag.</p> <ul> <li>Matching is performed globally (not by path): any node in the current model is matched to any node   in the other model with the same key value, regardless of their position in the tree.</li> <li>It is expected that there is only one matching node per key in the other model.</li> <li>If multiple nodes in the other model have the same key, a warning is logged and only the last one   found in pre-order traversal is used for merging.</li> </ul> Example use cases <ul> <li>Enrich a PS3.3 module attribute specification with VR/VM from the PS3.6 data elements dictionary.</li> <li>Merge any two models where a unique key (name or attribute) can be used for node correspondence.</li> </ul> PARAMETER DESCRIPTION <code>other</code> <p>The other model to merge with the current model.</p> <p> TYPE: <code>SpecModel</code> </p> <code>match_by</code> <p>\"name\" to match by node.name (stripped of leading '&gt;' and whitespace), or \"attribute\" to match by a specific attribute value.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'name'</code> </p> <code>attribute_name</code> <p>The attribute name to use for matching if match_by=\"attribute\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>merge_attrs</code> <p>List of attribute names to merge from the other model's node.</p> <p> TYPE: <code>list[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>SpecModel</code> <p>A new merged SpecModel with attributes from the other model merged in.</p> <p> TYPE: <code>SpecModel</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If match_by is invalid or attribute_name is missing when required.</p> Source code in <code>src/dcmspec/spec_model.py</code> <pre><code>def merge_matching_node(\n    self,\n    other: \"SpecModel\",\n    match_by: str = \"name\",\n    attribute_name: Optional[str] = None,\n    merge_attrs: Optional[list[str]] = None,\n) -&gt; \"SpecModel\":\n    \"\"\"Merge two SpecModel trees by matching nodes at any level using a single key (name or attribute).\n\n    For each node in the current model, this method finds a matching node in the other model\n    using either the node's name (if match_by=\"name\") or a specified attribute (if match_by=\"attribute\").\n    If a match is found, the specified attributes from the other model's node are merged into the current node.\n\n    This is useful for enrichment scenarios, such as adding VR/VM/Keyword from the Part 6 dictionary\n    to a Part 3 module, where nodes are matched by a unique attribute like elem_tag.\n\n    - Matching is performed globally (not by path): any node in the current model is matched to any node\n      in the other model with the same key value, regardless of their position in the tree.\n    - It is expected that there is only one matching node per key in the other model.\n    - If multiple nodes in the other model have the same key, a warning is logged and only the last one\n      found in pre-order traversal is used for merging.\n\n    Example use cases:\n        - Enrich a PS3.3 module attribute specification with VR/VM from the PS3.6 data elements dictionary.\n        - Merge any two models where a unique key (name or attribute) can be used for node correspondence.\n\n    Args:\n        other (SpecModel): The other model to merge with the current model.\n        match_by (str): \"name\" to match by node.name (stripped of leading '&gt;' and whitespace),\n            or \"attribute\" to match by a specific attribute value.\n        attribute_name (str, optional): The attribute name to use for matching if match_by=\"attribute\".\n        merge_attrs (list[str], optional): List of attribute names to merge from the other model's node.\n\n    Returns:\n        SpecModel: A new merged SpecModel with attributes from the other model merged in.\n\n    Raises:\n        ValueError: If match_by is invalid or attribute_name is missing when required.\n\n    \"\"\"        \n    return self._merge_nodes(\n        other,\n        match_by=match_by,\n        attribute_name=attribute_name,\n        merge_attrs=merge_attrs,\n        is_path_based=False\n    )\n</code></pre>"},{"location":"api/spec_model/#dcmspec.spec_model.SpecModel.merge_matching_path","title":"<code>merge_matching_path(other, match_by='name', attribute_name=None, merge_attrs=None, ignore_module_level=False)</code>","text":"<p>Merge with another SpecModel, producing a new model with attributes merged for nodes with matching paths.</p> <p>The path for matching is constructed at each level using either the node's <code>name</code> (if match_by=\"name\") or a specified attribute (if match_by=\"attribute\" and attribute_name is given). Only nodes whose full path matches (by the chosen key) will be merged.</p> <p>This method is useful for combining DICOM specification models from different parts of the standard. For example, it can be used to merge a PS3.3 model of a normalized IOD specification with a PS3.4 model of a SOP class specification.</p> PARAMETER DESCRIPTION <code>other</code> <p>The other model to merge with the current model.</p> <p> TYPE: <code>SpecModel</code> </p> <code>match_by</code> <p>\"name\" to match by node.name path, \"attribute\" to match by a specific attribute path.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'name'</code> </p> <code>attribute_name</code> <p>The attribute name to use for matching if match_by=\"attribute\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>merge_attrs</code> <p>List of attribute names to merge from the other model's node.</p> <p> TYPE: <code>list[str]</code> DEFAULT: <code>None</code> </p> <code>ignore_module_level</code> <p>If True, skip the module level in the path for matching.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>SpecModel</code> <p>A new merged SpecModel.</p> <p> TYPE: <code>SpecModel</code> </p> Source code in <code>src/dcmspec/spec_model.py</code> <pre><code>def merge_matching_path(\n    self,\n    other: \"SpecModel\",\n    match_by: str = \"name\",\n    attribute_name: Optional[str] = None,\n    merge_attrs: Optional[list[str]] = None,\n    ignore_module_level: bool = False,\n) -&gt; \"SpecModel\":\n    \"\"\"Merge with another SpecModel, producing a new model with attributes merged for nodes with matching paths.\n\n    The path for matching is constructed at each level using either the node's `name`\n    (if match_by=\"name\") or a specified attribute (if match_by=\"attribute\" and attribute_name is given).\n    Only nodes whose full path matches (by the chosen key) will be merged.\n\n    This method is useful for combining DICOM specification models from different parts of the standard.\n    For example, it can be used to merge a PS3.3 model of a normalized IOD specification with a PS3.4 model of a\n    SOP class specification.\n\n    Args:\n        other (SpecModel): The other model to merge with the current model.\n        match_by (str): \"name\" to match by node.name path, \"attribute\" to match by a specific attribute path.\n        attribute_name (str, optional): The attribute name to use for matching if match_by=\"attribute\".\n        merge_attrs (list[str], optional): List of attribute names to merge from the other model's node.\n        ignore_module_level (bool, optional): If True, skip the module level in the path for matching.\n\n    Returns:\n        SpecModel: A new merged SpecModel.\n\n    \"\"\"        \n    return self._merge_nodes(\n        other,\n        match_by=match_by,\n        attribute_name=attribute_name,\n        merge_attrs=merge_attrs,\n        is_path_based=True,\n        ignore_module_level=ignore_module_level\n    )\n</code></pre>"},{"location":"api/spec_parser/","title":"SpecParser","text":""},{"location":"api/spec_parser/#dcmspec.spec_parser.SpecParser","title":"<code>dcmspec.spec_parser.SpecParser</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for DICOM specification parsers.</p> <p>Handles DICOM specifications in various in-memory formats (e.g., DOM for XHTML/XML, CSV). Subclasses must implement the <code>parse</code> method to parse the specification content and build a structured model.</p> Source code in <code>src/dcmspec/spec_parser.py</code> <pre><code>class SpecParser(ABC):\n    \"\"\"Abstract base class for DICOM specification parsers.\n\n    Handles DICOM specifications in various in-memory formats (e.g., DOM for XHTML/XML, CSV).\n    Subclasses must implement the `parse` method to parse the specification content and build a structured model.\n    \"\"\"\n\n    def __init__(self, logger: Optional[logging.Logger] = None):\n        \"\"\"Initialize the DICOM Specification parser with an optional logger.\n\n        Args:\n            logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n        \"\"\"\n        if logger is not None and not isinstance(logger, logging.Logger):\n            raise TypeError(\"logger must be an instance of logging.Logger or None\")\n        self.logger = logger or logging.getLogger(self.__class__.__name__)\n\n    @abstractmethod\n    def parse(self, *args, **kwargs) -&gt; Tuple[Node, Node]:\n        \"\"\"Parse the DICOM specification and return metadata and attribute tree nodes.\n\n        Returns:\n            Tuple[Node, Node]: The metadata node and the content node.\n\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/spec_parser/#dcmspec.spec_parser.SpecParser.__init__","title":"<code>__init__(logger=None)</code>","text":"<p>Initialize the DICOM Specification parser with an optional logger.</p> PARAMETER DESCRIPTION <code>logger</code> <p>Logger instance to use. If None, a default logger is created.</p> <p> TYPE: <code>Optional[Logger]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/dcmspec/spec_parser.py</code> <pre><code>def __init__(self, logger: Optional[logging.Logger] = None):\n    \"\"\"Initialize the DICOM Specification parser with an optional logger.\n\n    Args:\n        logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n    \"\"\"\n    if logger is not None and not isinstance(logger, logging.Logger):\n        raise TypeError(\"logger must be an instance of logging.Logger or None\")\n    self.logger = logger or logging.getLogger(self.__class__.__name__)\n</code></pre>"},{"location":"api/spec_parser/#dcmspec.spec_parser.SpecParser.parse","title":"<code>parse(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Parse the DICOM specification and return metadata and attribute tree nodes.</p> RETURNS DESCRIPTION <code>Tuple[Node, Node]</code> <p>Tuple[Node, Node]: The metadata node and the content node.</p> Source code in <code>src/dcmspec/spec_parser.py</code> <pre><code>@abstractmethod\ndef parse(self, *args, **kwargs) -&gt; Tuple[Node, Node]:\n    \"\"\"Parse the DICOM specification and return metadata and attribute tree nodes.\n\n    Returns:\n        Tuple[Node, Node]: The metadata node and the content node.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/spec_printer/","title":"SpecPrinter","text":""},{"location":"api/spec_printer/#dcmspec.spec_printer.SpecPrinter","title":"<code>dcmspec.spec_printer.SpecPrinter</code>","text":"<p>Printer for DICOM specification models.</p> <p>Provides methods to print a SpecModel as a hierarchical tree or as a flat table, using rich formatting for console output. Supports colorized output and customizable logging.</p> Source code in <code>src/dcmspec/spec_printer.py</code> <pre><code>class SpecPrinter:\n    \"\"\"Printer for DICOM specification models.\n\n    Provides methods to print a SpecModel as a hierarchical tree or as a flat table,\n    using rich formatting for console output. Supports colorized output and customizable logging.\n    \"\"\"\n\n    def __init__(self, model: object, logger: Optional[logging.Logger] = None) -&gt; None:\n        \"\"\"Initialize the input handler with an optional logger.\n\n        Args:\n            model (object): An instance of SpecModel.\n            logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n        \"\"\"\n        if logger is not None and not isinstance(logger, logging.Logger):\n            raise TypeError(\"logger must be an instance of logging.Logger or None\")\n        self.logger = logger or logging.getLogger(self.__class__.__name__)\n\n        self.model = model\n        self.console = Console(highlight=False)\n\n    def print_tree(\n        self,\n        attr_names: Optional[Union[str, List[str]]] = None,\n        attr_widths: Optional[List[int]] = None,\n        colorize: bool = False,\n    ) -&gt; None:\n        \"\"\"Print the specification model as a hierarchical tree to the console.\n\n        Args:\n            attr_names (Optional[Union[str, list[str]]]): Attribute name(s) to display for each node.\n                If None, only the node's name is displayed.\n                If a string, displays that single attribute.\n                If a list of strings, displays all specified attributes.\n            attr_widths (Optional[list[int]]): List of widths for each attribute in attr_names.\n                If provided, each attribute will be padded/truncated to the specified width.\n            colorize (bool): Whether to colorize the output by node depth.\n\n        Example:\n            # This will nicely align the tag, type, and name values in the tree output:\n            printer.print_tree(attr_names=[\"elem_tag\", \"elem_type\", \"elem_name\"], attr_widths=[11, 2, 64])\n\n        Returns:\n            None\n\n        \"\"\"\n        for pre, fill, node in RenderTree(self.model.content):\n            style = LEVEL_COLORS[node.depth % len(LEVEL_COLORS)] if colorize else \"default\"\n            pre_text = Text(pre)\n            if attr_names is None:\n                node_text = Text(str(node.name), style=style)\n            else:\n                if isinstance(attr_names, str):\n                    attr_names = [attr_names]\n                values = [str(getattr(node, attr, \"\")) for attr in attr_names]\n                if attr_widths:\n                    # Pad/truncate each value to the specified width\n                    values = [\n                        v.ljust(w)[:w] if w is not None else v\n                        for v, w in zip(values, attr_widths)\n                    ]\n                attr_text = \" \".join(values)\n                node_text = Text(attr_text, style=style)\n            self.console.print(pre_text + node_text)\n\n    def print_table(self, colorize: bool = False) -&gt; None:\n        \"\"\"Print the specification model as a flat table to the console.\n\n        Traverses the content tree and prints each node's attributes in a flat table,\n        using column headers from the metadata node. Optionally colorizes rows.\n\n        Args:\n            colorize (bool): Whether to colorize the output by node depth.\n\n        Returns:\n            None\n\n        \"\"\"\n        table = Table(show_header=True, header_style=\"bold magenta\", show_lines=True, box=box.ASCII_DOUBLE_HEAD)\n\n        # Define the columns using the extracted headers\n        for header in self.model.metadata.header:\n            table.add_column(header, width=20)\n\n        # Traverse the tree and add rows to the table\n        for node in PreOrderIter(self.model.content):\n            # skip the root node\n            if node.name == \"content\":\n                continue\n\n            row = [getattr(node, attr, \"\") for attr in self.model.metadata.column_to_attr.values()]\n            # Skip row if all values are empty or whitespace\n            if all(not str(cell).strip() for cell in row):\n                continue\n            row_style = None\n            if colorize:\n                row_style = (\n                    \"yellow\"\n                    if self.model._is_include(node)\n                    else \"magenta\"\n                    if self.model._is_title(node)\n                    else LEVEL_COLORS[(node.depth - 1) % len(LEVEL_COLORS)]\n                )\n            table.add_row(*row, style=row_style)\n\n        self.console.print(table)\n</code></pre>"},{"location":"api/spec_printer/#dcmspec.spec_printer.SpecPrinter.__init__","title":"<code>__init__(model, logger=None)</code>","text":"<p>Initialize the input handler with an optional logger.</p> PARAMETER DESCRIPTION <code>model</code> <p>An instance of SpecModel.</p> <p> TYPE: <code>object</code> </p> <code>logger</code> <p>Logger instance to use. If None, a default logger is created.</p> <p> TYPE: <code>Optional[Logger]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/dcmspec/spec_printer.py</code> <pre><code>def __init__(self, model: object, logger: Optional[logging.Logger] = None) -&gt; None:\n    \"\"\"Initialize the input handler with an optional logger.\n\n    Args:\n        model (object): An instance of SpecModel.\n        logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n    \"\"\"\n    if logger is not None and not isinstance(logger, logging.Logger):\n        raise TypeError(\"logger must be an instance of logging.Logger or None\")\n    self.logger = logger or logging.getLogger(self.__class__.__name__)\n\n    self.model = model\n    self.console = Console(highlight=False)\n</code></pre>"},{"location":"api/spec_printer/#dcmspec.spec_printer.SpecPrinter.print_table","title":"<code>print_table(colorize=False)</code>","text":"<p>Print the specification model as a flat table to the console.</p> <p>Traverses the content tree and prints each node's attributes in a flat table, using column headers from the metadata node. Optionally colorizes rows.</p> PARAMETER DESCRIPTION <code>colorize</code> <p>Whether to colorize the output by node depth.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>None</code> <p>None</p> Source code in <code>src/dcmspec/spec_printer.py</code> <pre><code>def print_table(self, colorize: bool = False) -&gt; None:\n    \"\"\"Print the specification model as a flat table to the console.\n\n    Traverses the content tree and prints each node's attributes in a flat table,\n    using column headers from the metadata node. Optionally colorizes rows.\n\n    Args:\n        colorize (bool): Whether to colorize the output by node depth.\n\n    Returns:\n        None\n\n    \"\"\"\n    table = Table(show_header=True, header_style=\"bold magenta\", show_lines=True, box=box.ASCII_DOUBLE_HEAD)\n\n    # Define the columns using the extracted headers\n    for header in self.model.metadata.header:\n        table.add_column(header, width=20)\n\n    # Traverse the tree and add rows to the table\n    for node in PreOrderIter(self.model.content):\n        # skip the root node\n        if node.name == \"content\":\n            continue\n\n        row = [getattr(node, attr, \"\") for attr in self.model.metadata.column_to_attr.values()]\n        # Skip row if all values are empty or whitespace\n        if all(not str(cell).strip() for cell in row):\n            continue\n        row_style = None\n        if colorize:\n            row_style = (\n                \"yellow\"\n                if self.model._is_include(node)\n                else \"magenta\"\n                if self.model._is_title(node)\n                else LEVEL_COLORS[(node.depth - 1) % len(LEVEL_COLORS)]\n            )\n        table.add_row(*row, style=row_style)\n\n    self.console.print(table)\n</code></pre>"},{"location":"api/spec_printer/#dcmspec.spec_printer.SpecPrinter.print_tree","title":"<code>print_tree(attr_names=None, attr_widths=None, colorize=False)</code>","text":"<p>Print the specification model as a hierarchical tree to the console.</p> PARAMETER DESCRIPTION <code>attr_names</code> <p>Attribute name(s) to display for each node. If None, only the node's name is displayed. If a string, displays that single attribute. If a list of strings, displays all specified attributes.</p> <p> TYPE: <code>Optional[Union[str, list[str]]]</code> DEFAULT: <code>None</code> </p> <code>attr_widths</code> <p>List of widths for each attribute in attr_names. If provided, each attribute will be padded/truncated to the specified width.</p> <p> TYPE: <code>Optional[list[int]]</code> DEFAULT: <code>None</code> </p> <code>colorize</code> <p>Whether to colorize the output by node depth.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Example RETURNS DESCRIPTION <code>None</code> <p>None</p> Source code in <code>src/dcmspec/spec_printer.py</code> <pre><code>def print_tree(\n    self,\n    attr_names: Optional[Union[str, List[str]]] = None,\n    attr_widths: Optional[List[int]] = None,\n    colorize: bool = False,\n) -&gt; None:\n    \"\"\"Print the specification model as a hierarchical tree to the console.\n\n    Args:\n        attr_names (Optional[Union[str, list[str]]]): Attribute name(s) to display for each node.\n            If None, only the node's name is displayed.\n            If a string, displays that single attribute.\n            If a list of strings, displays all specified attributes.\n        attr_widths (Optional[list[int]]): List of widths for each attribute in attr_names.\n            If provided, each attribute will be padded/truncated to the specified width.\n        colorize (bool): Whether to colorize the output by node depth.\n\n    Example:\n        # This will nicely align the tag, type, and name values in the tree output:\n        printer.print_tree(attr_names=[\"elem_tag\", \"elem_type\", \"elem_name\"], attr_widths=[11, 2, 64])\n\n    Returns:\n        None\n\n    \"\"\"\n    for pre, fill, node in RenderTree(self.model.content):\n        style = LEVEL_COLORS[node.depth % len(LEVEL_COLORS)] if colorize else \"default\"\n        pre_text = Text(pre)\n        if attr_names is None:\n            node_text = Text(str(node.name), style=style)\n        else:\n            if isinstance(attr_names, str):\n                attr_names = [attr_names]\n            values = [str(getattr(node, attr, \"\")) for attr in attr_names]\n            if attr_widths:\n                # Pad/truncate each value to the specified width\n                values = [\n                    v.ljust(w)[:w] if w is not None else v\n                    for v, w in zip(values, attr_widths)\n                ]\n            attr_text = \" \".join(values)\n            node_text = Text(attr_text, style=style)\n        self.console.print(pre_text + node_text)\n</code></pre>"},{"location":"api/spec_printer/#dcmspec.spec_printer.SpecPrinter.print_tree--this-will-nicely-align-the-tag-type-and-name-values-in-the-tree-output","title":"This will nicely align the tag, type, and name values in the tree output:","text":"<p>printer.print_tree(attr_names=[\"elem_tag\", \"elem_type\", \"elem_name\"], attr_widths=[11, 2, 64])</p>"},{"location":"api/spec_store/","title":"SpecStore","text":""},{"location":"api/spec_store/#dcmspec.spec_store.SpecStore","title":"<code>dcmspec.spec_store.SpecStore</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for DICOM specification model storage backends.</p> <p>Subclasses should implement methods for loading and saving models.</p> Source code in <code>src/dcmspec/spec_store.py</code> <pre><code>class SpecStore(ABC):\n    \"\"\"Abstract base class for DICOM specification model storage backends.\n\n    Subclasses should implement methods for loading and saving models.\n    \"\"\"\n\n    def __init__(self, logger: Optional[logging.Logger] = None):\n        \"\"\"Initialize the model store with an optional logger.\n\n        Args:\n            logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n        \"\"\"\n        if logger is not None and not isinstance(logger, logging.Logger):\n            raise TypeError(\"logger must be an instance of logging.Logger or None\")\n        self.logger = logger or logging.getLogger(self.__class__.__name__)\n\n    @abstractmethod\n    def load(self, path: str) -&gt; SpecModel:\n        \"\"\"Load a model from the specified path.\n\n        Args:\n            path (str): The path to the file or resource to load from.\n\n        Returns:\n            SpecModel: The loaded model.\n\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def save(self, model: SpecModel, path: str) -&gt; None:\n        \"\"\"Save a model to the specified path.\n\n        Args:\n            model (SpecModel): The model to save.\n            path (str): The path to the file or resource to save to.\n\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/spec_store/#dcmspec.spec_store.SpecStore.__init__","title":"<code>__init__(logger=None)</code>","text":"<p>Initialize the model store with an optional logger.</p> PARAMETER DESCRIPTION <code>logger</code> <p>Logger instance to use. If None, a default logger is created.</p> <p> TYPE: <code>Optional[Logger]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/dcmspec/spec_store.py</code> <pre><code>def __init__(self, logger: Optional[logging.Logger] = None):\n    \"\"\"Initialize the model store with an optional logger.\n\n    Args:\n        logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n    \"\"\"\n    if logger is not None and not isinstance(logger, logging.Logger):\n        raise TypeError(\"logger must be an instance of logging.Logger or None\")\n    self.logger = logger or logging.getLogger(self.__class__.__name__)\n</code></pre>"},{"location":"api/spec_store/#dcmspec.spec_store.SpecStore.load","title":"<code>load(path)</code>  <code>abstractmethod</code>","text":"<p>Load a model from the specified path.</p> PARAMETER DESCRIPTION <code>path</code> <p>The path to the file or resource to load from.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>SpecModel</code> <p>The loaded model.</p> <p> TYPE: <code>SpecModel</code> </p> Source code in <code>src/dcmspec/spec_store.py</code> <pre><code>@abstractmethod\ndef load(self, path: str) -&gt; SpecModel:\n    \"\"\"Load a model from the specified path.\n\n    Args:\n        path (str): The path to the file or resource to load from.\n\n    Returns:\n        SpecModel: The loaded model.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/spec_store/#dcmspec.spec_store.SpecStore.save","title":"<code>save(model, path)</code>  <code>abstractmethod</code>","text":"<p>Save a model to the specified path.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to save.</p> <p> TYPE: <code>SpecModel</code> </p> <code>path</code> <p>The path to the file or resource to save to.</p> <p> TYPE: <code>str</code> </p> Source code in <code>src/dcmspec/spec_store.py</code> <pre><code>@abstractmethod\ndef save(self, model: SpecModel, path: str) -&gt; None:\n    \"\"\"Save a model to the specified path.\n\n    Args:\n        model (SpecModel): The model to save.\n        path (str): The path to the file or resource to save to.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ups_xhtml_doc_handler/","title":"UPSXHTMLDocHandler","text":""},{"location":"api/ups_xhtml_doc_handler/#dcmspec.ups_xhtml_doc_handler.UPSXHTMLDocHandler","title":"<code>dcmspec.ups_xhtml_doc_handler.UPSXHTMLDocHandler</code>","text":"<p>               Bases: <code>XHTMLDocHandler</code></p> <p>Subclass of XHTMLDocHandler that applies UPS-specific table patching.</p> <p>This handler applies UPS-specific patching to DICOM XHTML tables after parsing. It corrects known issues in Table CC.2.5-3 of DICOM PS3.4, where 'Include' rows under certain sequence attribute rows are missing a '&gt;' nesting symbol. The affected sequences are:     - Output Information Sequence     - Gender Identity Code Sequence     - Sex Parameters for Clinical Use Category Code Sequence     - Pronoun Code Sequence</p> Source code in <code>src/dcmspec/ups_xhtml_doc_handler.py</code> <pre><code>class UPSXHTMLDocHandler(XHTMLDocHandler):\n    \"\"\"Subclass of XHTMLDocHandler that applies UPS-specific table patching.\n\n    This handler applies UPS-specific patching to DICOM XHTML tables after parsing.\n    It corrects known issues in Table CC.2.5-3 of DICOM PS3.4, where 'Include' rows under certain\n    sequence attribute rows are missing a '&gt;' nesting symbol. The affected sequences are:\n        - Output Information Sequence\n        - Gender Identity Code Sequence\n        - Sex Parameters for Clinical Use Category Code Sequence\n        - Pronoun Code Sequence\n    \"\"\"\n\n    def __init__(self, config=None, logger=None):\n        \"\"\"Initialize the UPSXHTMLDocHandler.\n\n        Sets up the handler with the given configuration and logger, and creates a DOMUtils\n        instance for DOM navigation.\n\n        Args:\n            config (optional): Configuration object for the handler.\n            logger (optional): Logger instance to use. If None, a default logger is created.\n\n        \"\"\"\n        super().__init__(config=config, logger=logger)\n        self.dom_utils = DOMUtils(logger=self.logger)\n\n    def parse_dom(self, file_path: str) -&gt; BeautifulSoup:\n        \"\"\"Parse a cached XHTML file and apply UPS-specific table patching.\n\n        Calls the base class's parse_dom, then patches the Output Information Sequence Include nesting level.\n\n        Args:\n            file_path (str): Path to the cached XHTML file to parse.\n\n        Returns:\n            BeautifulSoup: The patched DOM object.\n\n        \"\"\"\n        dom = super().parse_dom(file_path)\n        # Patch the table after parsing\n        self._patch_table(dom, \"table_CC.2.5-3\")  # or pass table_id dynamically if needed\n        return dom\n\n    def _patch_table(self, dom, table_id):\n        \"\"\"Patch the specified XHTML table to fix Include nesting level errors.\n\n        In the UPS, the 'Include' row under some sequence attribute rows are missing one '&gt;' nesting symbol.\n\n        Args:\n            dom: The BeautifulSoup DOM object representing the XHTML document.\n            table_id: The ID of the table to patch.\n\n        \"\"\"\n        patch_labels = [\n            \"&gt;Output Information Sequence\",\n            \"&gt;Gender Identity Code Sequence\",\n            \"&gt;Sex Parameters for Clinical Use Category Code Sequence\",\n            \"&gt;Pronoun Code Sequence\",\n        ]\n        for label in patch_labels:\n            target_element_id = self._search_element_id(dom, table_id, label)\n            if not target_element_id:\n                self.logger.warning(f\"{label} Include Row element ID not found\")\n                continue\n            element = dom.find(id=target_element_id).find_parent()\n            span_element = element.find(\"span\", class_=\"italic\")\n            if span_element:\n                children_to_modify = [\n                    child for child in span_element.children\n                    if isinstance(child, str) and \"&gt;Include\" in child\n                ]\n                for child in children_to_modify:\n                    new_text = child.replace(\"&gt;Include\", \"&gt;&gt;Include\")\n                    child.replace_with(new_text)\n\n    def _search_element_id(self, dom, table_id, sequence_label):\n        table = self.dom_utils.get_table(dom, table_id)\n        if not table:\n            return None\n\n        self.logger.debug(f\"Table with id {table_id} found\")\n        tr_elements = table.find_all(\"tr\")\n        include_id = self._search_sequence_include_id(tr_elements, sequence_label)\n\n        if include_id is None:\n            self.logger.debug(\"No &lt;tr&gt; matching criteria found\")\n\n        return include_id\n\n    def _search_sequence_include_id(self, tr_elements, sequence_label):\n        target_found = False\n        for tr in tr_elements:\n            first_td = tr.find(\"td\")\n            if first_td and first_td.get_text(strip=True) == sequence_label:\n                self.logger.debug(f\"{sequence_label} row found\")\n                target_found = True\n                break\n\n        if target_found:\n            tr = tr.find_next(\"tr\")\n            if tr is not None:\n                first_td = tr.find(\"td\")\n                if first_td and first_td.get_text(strip=True).startswith(\"&gt;Include\"):\n                    self.logger.debug(\"Include &lt;tr&gt; found\")\n                    return first_td.find(\"a\")[\"id\"]\n\n        return None\n</code></pre>"},{"location":"api/ups_xhtml_doc_handler/#dcmspec.ups_xhtml_doc_handler.UPSXHTMLDocHandler.__init__","title":"<code>__init__(config=None, logger=None)</code>","text":"<p>Initialize the UPSXHTMLDocHandler.</p> <p>Sets up the handler with the given configuration and logger, and creates a DOMUtils instance for DOM navigation.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object for the handler.</p> <p> TYPE: <code>optional</code> DEFAULT: <code>None</code> </p> <code>logger</code> <p>Logger instance to use. If None, a default logger is created.</p> <p> TYPE: <code>optional</code> DEFAULT: <code>None</code> </p> Source code in <code>src/dcmspec/ups_xhtml_doc_handler.py</code> <pre><code>def __init__(self, config=None, logger=None):\n    \"\"\"Initialize the UPSXHTMLDocHandler.\n\n    Sets up the handler with the given configuration and logger, and creates a DOMUtils\n    instance for DOM navigation.\n\n    Args:\n        config (optional): Configuration object for the handler.\n        logger (optional): Logger instance to use. If None, a default logger is created.\n\n    \"\"\"\n    super().__init__(config=config, logger=logger)\n    self.dom_utils = DOMUtils(logger=self.logger)\n</code></pre>"},{"location":"api/ups_xhtml_doc_handler/#dcmspec.ups_xhtml_doc_handler.UPSXHTMLDocHandler.parse_dom","title":"<code>parse_dom(file_path)</code>","text":"<p>Parse a cached XHTML file and apply UPS-specific table patching.</p> <p>Calls the base class's parse_dom, then patches the Output Information Sequence Include nesting level.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to the cached XHTML file to parse.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>BeautifulSoup</code> <p>The patched DOM object.</p> <p> TYPE: <code>BeautifulSoup</code> </p> Source code in <code>src/dcmspec/ups_xhtml_doc_handler.py</code> <pre><code>def parse_dom(self, file_path: str) -&gt; BeautifulSoup:\n    \"\"\"Parse a cached XHTML file and apply UPS-specific table patching.\n\n    Calls the base class's parse_dom, then patches the Output Information Sequence Include nesting level.\n\n    Args:\n        file_path (str): Path to the cached XHTML file to parse.\n\n    Returns:\n        BeautifulSoup: The patched DOM object.\n\n    \"\"\"\n    dom = super().parse_dom(file_path)\n    # Patch the table after parsing\n    self._patch_table(dom, \"table_CC.2.5-3\")  # or pass table_id dynamically if needed\n    return dom\n</code></pre>"},{"location":"api/xhtml_doc_handler/","title":"XHTMLDocHandler","text":""},{"location":"api/xhtml_doc_handler/#dcmspec.xhtml_doc_handler.XHTMLDocHandler","title":"<code>dcmspec.xhtml_doc_handler.XHTMLDocHandler</code>","text":"<p>               Bases: <code>DocHandler</code></p> <p>Handler class for DICOM specifications documents in XHTML format.</p> <p>Provides methods to download, cache, and parse XHTML documents, returning a BeautifulSoup DOM object. Inherits configuration and logging from DocHandler.</p> <p>Note: Progress reporting via progress_observer covers both downloading and caching (writing to disk). Parsing and cache loading are typically fast and do not emit progress updates.</p> Source code in <code>src/dcmspec/xhtml_doc_handler.py</code> <pre><code>class XHTMLDocHandler(DocHandler):\n    \"\"\"Handler class for DICOM specifications documents in XHTML format.\n\n    Provides methods to download, cache, and parse XHTML documents, returning a BeautifulSoup DOM object.\n    Inherits configuration and logging from DocHandler.\n\n    Note:\n    Progress reporting via progress_observer covers both downloading and caching (writing to disk).\n    Parsing and cache loading are typically fast and do not emit progress updates.\n\n    \"\"\"\n\n    def __init__(self, config: Optional[Config] = None, logger: Optional[logging.Logger] = None):\n        \"\"\"Initialize the XHTML document handler and set cache_file_name to None.\"\"\"\n        super().__init__(config=config, logger=logger)\n        self.cache_file_name = None\n\n    def load_document(\n            self, cache_file_name: str,\n            url: Optional[str] = None,\n            force_download: bool = False,\n            progress_observer: 'Optional[ProgressObserver]' = None,\n            # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n            progress_callback: 'Optional[Callable[[int], None]]' = None,\n            # END LEGACY SUPPORT\n    ) -&gt; BeautifulSoup:\n        # sourcery skip: merge-else-if-into-elif, reintroduce-else, swap-if-else-branches\n        \"\"\"Open and parse an XHTML file, downloading it if needed.\n\n        Args:\n            cache_file_name (str): Path to the local cached XHTML file.\n            url (str, optional): URL to download the file from if not cached or if force_download is True.\n            force_download (bool): If True, do not use cache and download the file from the URL.\n            progress_observer (Optional[ProgressObserver]): Optional observer to report download progress.\n            progress_callback (Optional[Callable[[int], None]]): [LEGACY, Deprecated] Optional callback to\n                report progress as an integer percent (0-100, or -1 if indeterminate). Use progress_observer\n                instead. Will be removed in a future release.\n\n        Returns:\n            BeautifulSoup: Parsed DOM.\n\n        \"\"\"\n        # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n        progress_observer = handle_legacy_callback(progress_observer, progress_callback)\n        # END LEGACY SUPPORT\n\n        # Set cache_file_name as an attribute for downstream use (e.g., in SpecFactory)\n        self.cache_file_name = cache_file_name\n\n        cache_file_path = os.path.join(self.config.get_param(\"cache_dir\"), \"standard\", cache_file_name)\n        need_download = force_download or (not os.path.exists(cache_file_path))\n        if need_download:\n            if not url:\n                raise ValueError(\"URL must be provided to download the file.\")\n            cache_file_path = self.download(url, cache_file_name, progress_observer=progress_observer)\n        else:\n            # Also report progress when XHTML file was loaded from cache (keeping DOWNLOADING status for consistency)\n            if progress_observer:\n                progress_observer(Progress(100, status=ProgressStatus.DOWNLOADING))     \n\n        # No need to report progress for parsing as, even for the largest DICOM standard XHTML file of 35 MB,\n        # the parsing is fast and not a bottleneck. If future files or operations make parsing slow,\n        # consider extending progress reporting here.\n        return self.parse_dom(cache_file_path)\n\n    def download(\n        self,\n        url: str,\n        cache_file_name: str,\n        progress_observer: 'Optional[ProgressObserver]' = None,\n        # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n        progress_callback: 'Optional[Callable[[int], None]]' = None\n        # END LEGACY SUPPORT\n    ) -&gt; str:\n        \"\"\"Download and cache an XHTML file from a URL.\n\n        Uses the base class download method, saving as UTF-8 text and cleaning ZWSP/NBSP.\n\n        Args:\n            url: The URL of the XHTML document to download.\n            cache_file_name: The filename of the cached document.\n            progress_observer: Optional observer to report download progress.\n            progress_callback (Optional[Callable[[int], None]]): [LEGACY, Deprecated] Optional callback to\n                report progress as an integer percent (0-100, or -1 if indeterminate). Use progress_observer\n                instead. Will be removed in a future release.\n\n        Returns:\n            The file path where the document was saved.\n\n        Raises:\n            RuntimeError: If the download or save fails.\n\n        \"\"\"\n        # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n        progress_observer = handle_legacy_callback(progress_observer, progress_callback)\n        # END LEGACY SUPPORT\n        file_path = os.path.join(self.config.get_param(\"cache_dir\"), \"standard\", cache_file_name)\n        return super().download(url, file_path, binary=False, progress_observer=progress_observer)\n\n    def clean_text(self, text: str) -&gt; str:\n        \"\"\"Clean text content before saving.\n\n        Removes zero-width space (ZWSP) and non-breaking space (NBSP) characters.\n\n        Args:\n            text (str): The text content to clean.\n\n        Returns:\n            str: The cleaned text.\n\n        \"\"\"\n        cleaned_content = re.sub(r\"\\u200b\", \"\", text)\n        cleaned_content = re.sub(r\"\\u00a0\", \" \", cleaned_content)\n        return cleaned_content\n\n    def parse_dom(self, file_path: str) -&gt; BeautifulSoup:\n        \"\"\"Parse a cached XHTML file into a BeautifulSoup DOM object.\n\n        Args:\n            file_path (str): Path to the cached XHTML file to parse.\n\n        Returns:\n            BeautifulSoup: The parsed DOM object.\n\n        Raises:\n            RuntimeError: If the file cannot be read or parsed.\n\n        \"\"\"\n        self.logger.info(f\"Reading XHTML DOM from {file_path}\")\n        try:\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                content = f.read()\n            # Use the built-in 'xml' parser since DICOM files and cell values are well-formed XHTML.\n            # \"html.parser\" is fine for XHTML but unreliable for strict XML.\n            # \"lxml\" defaults to HTML mode and generates a warning for XML.\n            # \"lxml-xml\" forces XML parsing but adds the lxml dependency.\n            dom = BeautifulSoup(content, features=\"xml\") \n            self.logger.info(\"XHTML DOM read successfully\")\n\n            return dom\n        except OSError as e:\n            self.logger.error(f\"Failed to read file {file_path}: {e}\")\n            raise RuntimeError(f\"Failed to read file {file_path}: {e}\") from e\n        except Exception as e:\n            self.logger.error(f\"Failed to parse XHTML file {file_path}: {e}\")\n            raise RuntimeError(f\"Failed to parse XHTML file {file_path}: {e}\") from e\n\n    def _patch_table(self, dom: BeautifulSoup, table_id: str) -&gt; None:\n        \"\"\"Patch an XHTML table to fix potential errors.\n\n        This method does nothing and may be overridden in derived classes if patching is needed.\n\n        Args:\n            dom (BeautifulSoup): The parsed XHTML DOM object.\n            table_id (str): The ID of the table to patch.\n\n        Returns:\n            None\n\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/xhtml_doc_handler/#dcmspec.xhtml_doc_handler.XHTMLDocHandler.__init__","title":"<code>__init__(config=None, logger=None)</code>","text":"<p>Initialize the XHTML document handler and set cache_file_name to None.</p> Source code in <code>src/dcmspec/xhtml_doc_handler.py</code> <pre><code>def __init__(self, config: Optional[Config] = None, logger: Optional[logging.Logger] = None):\n    \"\"\"Initialize the XHTML document handler and set cache_file_name to None.\"\"\"\n    super().__init__(config=config, logger=logger)\n    self.cache_file_name = None\n</code></pre>"},{"location":"api/xhtml_doc_handler/#dcmspec.xhtml_doc_handler.XHTMLDocHandler.clean_text","title":"<code>clean_text(text)</code>","text":"<p>Clean text content before saving.</p> <p>Removes zero-width space (ZWSP) and non-breaking space (NBSP) characters.</p> PARAMETER DESCRIPTION <code>text</code> <p>The text content to clean.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The cleaned text.</p> <p> TYPE: <code>str</code> </p> Source code in <code>src/dcmspec/xhtml_doc_handler.py</code> <pre><code>def clean_text(self, text: str) -&gt; str:\n    \"\"\"Clean text content before saving.\n\n    Removes zero-width space (ZWSP) and non-breaking space (NBSP) characters.\n\n    Args:\n        text (str): The text content to clean.\n\n    Returns:\n        str: The cleaned text.\n\n    \"\"\"\n    cleaned_content = re.sub(r\"\\u200b\", \"\", text)\n    cleaned_content = re.sub(r\"\\u00a0\", \" \", cleaned_content)\n    return cleaned_content\n</code></pre>"},{"location":"api/xhtml_doc_handler/#dcmspec.xhtml_doc_handler.XHTMLDocHandler.download","title":"<code>download(url, cache_file_name, progress_observer=None, progress_callback=None)</code>","text":"<p>Download and cache an XHTML file from a URL.</p> <p>Uses the base class download method, saving as UTF-8 text and cleaning ZWSP/NBSP.</p> PARAMETER DESCRIPTION <code>url</code> <p>The URL of the XHTML document to download.</p> <p> TYPE: <code>str</code> </p> <code>cache_file_name</code> <p>The filename of the cached document.</p> <p> TYPE: <code>str</code> </p> <code>progress_observer</code> <p>Optional observer to report download progress.</p> <p> TYPE: <code>Optional[ProgressObserver]</code> DEFAULT: <code>None</code> </p> <code>progress_callback</code> <p>[LEGACY, Deprecated] Optional callback to report progress as an integer percent (0-100, or -1 if indeterminate). Use progress_observer instead. Will be removed in a future release.</p> <p> TYPE: <code>Optional[Callable[[int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The file path where the document was saved.</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If the download or save fails.</p> Source code in <code>src/dcmspec/xhtml_doc_handler.py</code> <pre><code>def download(\n    self,\n    url: str,\n    cache_file_name: str,\n    progress_observer: 'Optional[ProgressObserver]' = None,\n    # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n    progress_callback: 'Optional[Callable[[int], None]]' = None\n    # END LEGACY SUPPORT\n) -&gt; str:\n    \"\"\"Download and cache an XHTML file from a URL.\n\n    Uses the base class download method, saving as UTF-8 text and cleaning ZWSP/NBSP.\n\n    Args:\n        url: The URL of the XHTML document to download.\n        cache_file_name: The filename of the cached document.\n        progress_observer: Optional observer to report download progress.\n        progress_callback (Optional[Callable[[int], None]]): [LEGACY, Deprecated] Optional callback to\n            report progress as an integer percent (0-100, or -1 if indeterminate). Use progress_observer\n            instead. Will be removed in a future release.\n\n    Returns:\n        The file path where the document was saved.\n\n    Raises:\n        RuntimeError: If the download or save fails.\n\n    \"\"\"\n    # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n    progress_observer = handle_legacy_callback(progress_observer, progress_callback)\n    # END LEGACY SUPPORT\n    file_path = os.path.join(self.config.get_param(\"cache_dir\"), \"standard\", cache_file_name)\n    return super().download(url, file_path, binary=False, progress_observer=progress_observer)\n</code></pre>"},{"location":"api/xhtml_doc_handler/#dcmspec.xhtml_doc_handler.XHTMLDocHandler.load_document","title":"<code>load_document(cache_file_name, url=None, force_download=False, progress_observer=None, progress_callback=None)</code>","text":"<p>Open and parse an XHTML file, downloading it if needed.</p> PARAMETER DESCRIPTION <code>cache_file_name</code> <p>Path to the local cached XHTML file.</p> <p> TYPE: <code>str</code> </p> <code>url</code> <p>URL to download the file from if not cached or if force_download is True.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>force_download</code> <p>If True, do not use cache and download the file from the URL.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>progress_observer</code> <p>Optional observer to report download progress.</p> <p> TYPE: <code>Optional[ProgressObserver]</code> DEFAULT: <code>None</code> </p> <code>progress_callback</code> <p>[LEGACY, Deprecated] Optional callback to report progress as an integer percent (0-100, or -1 if indeterminate). Use progress_observer instead. Will be removed in a future release.</p> <p> TYPE: <code>Optional[Callable[[int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>BeautifulSoup</code> <p>Parsed DOM.</p> <p> TYPE: <code>BeautifulSoup</code> </p> Source code in <code>src/dcmspec/xhtml_doc_handler.py</code> <pre><code>def load_document(\n        self, cache_file_name: str,\n        url: Optional[str] = None,\n        force_download: bool = False,\n        progress_observer: 'Optional[ProgressObserver]' = None,\n        # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n        progress_callback: 'Optional[Callable[[int], None]]' = None,\n        # END LEGACY SUPPORT\n) -&gt; BeautifulSoup:\n    # sourcery skip: merge-else-if-into-elif, reintroduce-else, swap-if-else-branches\n    \"\"\"Open and parse an XHTML file, downloading it if needed.\n\n    Args:\n        cache_file_name (str): Path to the local cached XHTML file.\n        url (str, optional): URL to download the file from if not cached or if force_download is True.\n        force_download (bool): If True, do not use cache and download the file from the URL.\n        progress_observer (Optional[ProgressObserver]): Optional observer to report download progress.\n        progress_callback (Optional[Callable[[int], None]]): [LEGACY, Deprecated] Optional callback to\n            report progress as an integer percent (0-100, or -1 if indeterminate). Use progress_observer\n            instead. Will be removed in a future release.\n\n    Returns:\n        BeautifulSoup: Parsed DOM.\n\n    \"\"\"\n    # BEGIN LEGACY SUPPORT: Remove for int progress callback deprecation\n    progress_observer = handle_legacy_callback(progress_observer, progress_callback)\n    # END LEGACY SUPPORT\n\n    # Set cache_file_name as an attribute for downstream use (e.g., in SpecFactory)\n    self.cache_file_name = cache_file_name\n\n    cache_file_path = os.path.join(self.config.get_param(\"cache_dir\"), \"standard\", cache_file_name)\n    need_download = force_download or (not os.path.exists(cache_file_path))\n    if need_download:\n        if not url:\n            raise ValueError(\"URL must be provided to download the file.\")\n        cache_file_path = self.download(url, cache_file_name, progress_observer=progress_observer)\n    else:\n        # Also report progress when XHTML file was loaded from cache (keeping DOWNLOADING status for consistency)\n        if progress_observer:\n            progress_observer(Progress(100, status=ProgressStatus.DOWNLOADING))     \n\n    # No need to report progress for parsing as, even for the largest DICOM standard XHTML file of 35 MB,\n    # the parsing is fast and not a bottleneck. If future files or operations make parsing slow,\n    # consider extending progress reporting here.\n    return self.parse_dom(cache_file_path)\n</code></pre>"},{"location":"api/xhtml_doc_handler/#dcmspec.xhtml_doc_handler.XHTMLDocHandler.parse_dom","title":"<code>parse_dom(file_path)</code>","text":"<p>Parse a cached XHTML file into a BeautifulSoup DOM object.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to the cached XHTML file to parse.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>BeautifulSoup</code> <p>The parsed DOM object.</p> <p> TYPE: <code>BeautifulSoup</code> </p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If the file cannot be read or parsed.</p> Source code in <code>src/dcmspec/xhtml_doc_handler.py</code> <pre><code>def parse_dom(self, file_path: str) -&gt; BeautifulSoup:\n    \"\"\"Parse a cached XHTML file into a BeautifulSoup DOM object.\n\n    Args:\n        file_path (str): Path to the cached XHTML file to parse.\n\n    Returns:\n        BeautifulSoup: The parsed DOM object.\n\n    Raises:\n        RuntimeError: If the file cannot be read or parsed.\n\n    \"\"\"\n    self.logger.info(f\"Reading XHTML DOM from {file_path}\")\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            content = f.read()\n        # Use the built-in 'xml' parser since DICOM files and cell values are well-formed XHTML.\n        # \"html.parser\" is fine for XHTML but unreliable for strict XML.\n        # \"lxml\" defaults to HTML mode and generates a warning for XML.\n        # \"lxml-xml\" forces XML parsing but adds the lxml dependency.\n        dom = BeautifulSoup(content, features=\"xml\") \n        self.logger.info(\"XHTML DOM read successfully\")\n\n        return dom\n    except OSError as e:\n        self.logger.error(f\"Failed to read file {file_path}: {e}\")\n        raise RuntimeError(f\"Failed to read file {file_path}: {e}\") from e\n    except Exception as e:\n        self.logger.error(f\"Failed to parse XHTML file {file_path}: {e}\")\n        raise RuntimeError(f\"Failed to parse XHTML file {file_path}: {e}\") from e\n</code></pre>"},{"location":"apps/","title":"Applications","text":"<p>This section documents the sample interactive applications provided by dcmspec.</p>"},{"location":"apps/#available-applications","title":"Available Applications","text":"<ul> <li>IOD Explorer - GUI application using Tkinter for browsing DICOM IODs and modules</li> </ul>"},{"location":"apps/#getting-started","title":"Getting Started","text":"<p>All applications can be launched using Poetry. Make sure you have completed the installation steps first.</p> <p>For configuration and caching information, see the Configuration &amp; Caching page.</p>"},{"location":"apps/iod-explorer/","title":"IOD Explorer","text":"<p>A GUI application for exploring DICOM IOD specifications interactively.</p>"},{"location":"apps/iod-explorer/#dependencies","title":"Dependencies","text":"<ul> <li>tkhtmlview (required for the GUI, but now installed only if you request the GUI extra)</li> <li>tkinter (required for the GUI, but not installed via pip or Poetry)</li> </ul> <p>Note: <code>tkinter</code> is part of the Python standard library, but on some Linux distributions and on macOS with Homebrew Python, it must be installed separately.</p> <ul> <li>On Ubuntu/Debian: <code>sudo apt install python3-tk</code></li> <li>On Fedora: <code>sudo dnf install python3-tkinter</code></li> <li>On macOS (Homebrew Python): <code>brew install tcl-tk</code></li> <li>You may also need to set environment variables so Python can find the Tk libraries. See Homebrew Python and Tkinter for details.</li> <li>On Windows/macOS (python.org installer): Usually included with the official Python installer.</li> </ul> <p>If you get an error about <code>tkinter</code> not being found, please install it as shown above.</p>"},{"location":"apps/iod-explorer/#installation","title":"Installation","text":"<p>Clone the repository and install dependencies with Poetry:</p> <pre><code>git clone https://github.com/dwikler/dcmspec.git\ncd dcmspec\npoetry install --with gui\n</code></pre> <p>Or, with pip (from the repo root):</p> <pre><code>pip install .[gui]\n</code></pre> <p>This installs the package and its dependencies from your local source directory, including the GUI dependencies. If you want to make changes to the code and have them reflected immediately, use:</p> <pre><code>pip install -e .[gui]\n</code></pre> <p>Tip: It is recommended to use a virtual environment (venv) before running <code>pip install .[gui]</code> to avoid installing packages globally. Poetry manages a venv automatically, but if you use pip directly, create one with:</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate  # On Unix/macOS\n.\\.venv\\Scripts\\Activate.ps1  # On Windows PowerShell\n</code></pre>"},{"location":"apps/iod-explorer/#running-the-application","title":"Running the Application","text":""},{"location":"apps/iod-explorer/#option-1-using-poetry-run-recommended","title":"Option 1: Using poetry run (recommended)","text":"<pre><code>poetry run iod-explorer\n</code></pre>"},{"location":"apps/iod-explorer/#option-2-activate-environment-then-run-directly","title":"Option 2: Activate environment then run directly","text":"<pre><code># On Windows PowerShell\n.\\.venv\\Scripts\\Activate.ps1\niod-explorer\n\n# On Unix/macOS\nsource .venv/bin/activate\niod-explorer\n</code></pre>"},{"location":"apps/iod-explorer/#option-3-using-the-module-path","title":"Option 3: Using the module path","text":"<pre><code>poetry run python -m src.dcmspec.apps.iod_explorer.iod_explorer\n</code></pre>"},{"location":"apps/iod-explorer/#note-on-poetry-20-environment-activation","title":"Note on Poetry 2.0+ Environment Activation","text":"<p>If you're using Poetry 2.0+, the <code>poetry env activate</code> command only prints the activation command but doesn't actually activate the environment in your current shell. For direct command execution, use one of these approaches:</p> <ul> <li>Manual activation (Windows): <code>.\\.venv\\Scripts\\Activate.ps1</code></li> <li>Install shell plugin: <code>poetry self add poetry-plugin-shell</code> then use <code>poetry shell</code></li> <li>Always use <code>poetry run</code>: No activation needed</li> </ul>"},{"location":"apps/iod-explorer/#configuration","title":"Configuration","text":"<p>The application supports customizable configuration through JSON files. Configuration files are searched in the following order:</p> <ol> <li>Current directory: <code>iod_explorer_config.json</code></li> <li>User config: <code>~/.config/dcmspec/iod_explorer_config.json</code></li> <li>App config directory: <code>src/dcmspec/apps/iod_explorer/config/iod_explorer_config.json</code></li> <li>Legacy location: Same directory as script</li> </ol> <p>For detailed configuration options and examples, see the config directory in the application source.</p>"},{"location":"apps/iod-explorer/#features","title":"Features","text":"<ul> <li>Browse DICOM IODs (Information Object Definitions)</li> <li>Explore IOD modules and attributes hierarchically</li> <li>Persistent caching of downloaded specifications</li> </ul>"},{"location":"apps/iod-explorer/#configuration-examples","title":"Configuration Examples","text":"<p>The application includes several example configurations:</p> <ul> <li>Default: Basic configuration with INFO logging</li> <li>Debug: Verbose logging for troubleshooting</li> <li>Minimal: Minimal logging for production use</li> </ul> <p>Copy any example to one of the search locations and rename to <code>iod_explorer_config.json</code> to use it.</p>"},{"location":"cli/","title":"CLI Applications Overview","text":"<p>This section documents the sample command-line interface (CLI) applications provided by dcmspec.</p> <p>These sample CLI applications enable extraction, parsing, and processing of specification tables and related data from the DICOM standard and IHE documents. Each CLI script is located in the <code>src/dcmspec/cli/</code> folder and can be run as a standalone application.</p> <ul> <li>modattributes (Module Attributes tables from Part 3)</li> <li>iodattributes (Complete set of attributes for a given IOD from Part 3)</li> <li>iodmodules (IOD Module tables from Part 3)</li> <li>dataelements (Data Elements from Part 6)</li> <li>uidvalues (Unique Identifiers (UIDs) from Part 6)</li> <li>upsdimseattributes (UPS DIMSE Service Attribute tables from Part 4)</li> <li>upsioddimseattributes (UPS IOD attributes from Part 3 and Part 4)</li> <li>tdwiicontent (TDW-II Content Specification from the IHE-RO Supplement)</li> </ul>"},{"location":"cli/#how-to-run","title":"How to Run","text":"<p>CLI scripts can be executed using one of the following methods:</p> <ul> <li>With Poetry:</li> </ul> <pre><code>poetry run python -m src.dcmspec.cli.&lt;script_name&gt; --help\n</code></pre> <ul> <li>Directly (if installed as a script):</li> </ul> <pre><code>poetry run &lt;script_name&gt; --help\n</code></pre> <ul> <li>With Python (after setting PYTHONPATH): <pre><code>export PYTHONPATH=$(pwd)/src\npython -m dcmspec.cli.&lt;script_name&gt; --help\n</code></pre></li> </ul>"},{"location":"cli/#example","title":"Example","text":"<p>To parse the Patient Module Attributes Table:</p> <pre><code>poetry run python -m src.dcmspec.cli.modattributes table_C.7-1\n</code></pre>"},{"location":"cli/dataelements/","title":"dataelements","text":""},{"location":"cli/dataelements/#description","title":"Description","text":"<p>CLI for extracting, caching, and printing DICOM Data Elements from Part 6 of the DICOM standard.</p> <p>This CLI downloads, caches, and prints the list of DICOM Data Elements from Part 6 of the DICOM standard. The tool parses the Data Elements table to extract tags, names, keywords, VR (Value Representation), VM (Value Multiplicity), and status for all DICOM data elements. The output can be printed as a table or tree.</p> <p>The resulting model is cached as a JSON file. The primary purpose of this cache file is to provide a structured, machine-readable representation of the DICOM Data Elements, which can be used for further processing or integration in other tools. As a secondary benefit, the cache file is also used to speed up subsequent runs of the CLI scripts.</p> <p>For more information on configuration and caching location see the Configuration and Caching page.</p>"},{"location":"cli/dataelements/#usage","title":"Usage","text":"<pre><code>poetry run python -m src.dcmspec.cli.dataelements [options]\n</code></pre> <code>[options]</code> Additional command-line options (see below)."},{"location":"cli/dataelements/#options","title":"Options","text":"<code>--config &lt;file&gt;</code> Path to the configuration file. <code>--print-mode &lt;mode&gt;</code> Print as <code>'table'</code> (default), <code>'tree'</code>, or <code>'none'</code> to skip printing. <code>-h</code>, <code>--help</code> Show this help message and exit."},{"location":"cli/dataelements/#examples","title":"Examples","text":"<p>To print the DICOM Data Elements as a table:</p> <pre><code>poetry run python -m src.dcmspec.cli.dataelements\n</code></pre> <p>To print the DICOM Data Elements as a tree:</p> <pre><code>poetry run python -m src.dcmspec.cli.dataelements --print-mode tree\n</code></pre>"},{"location":"cli/iodattributes/","title":"iodattributes","text":""},{"location":"cli/iodattributes/#description","title":"Description","text":"<p>CLI for extracting, caching, and printing the complete set of DICOM attributes for a given IOD (Information Object Definition) from Part 3 of the DICOM standard.</p> <p>This CLI downloads, caches, and prints all attributes for a specified DICOM IOD, supporting both Composite and Normalized IODs. When an IOD table is specified, the tool parses the IOD table to determine which modules are referenced, then automatically parses each referenced Module Attributes table. The resulting model contains both the list of modules and, for each module, all its attributes. The print output (table or tree) shows only the attributes, not the IOD table or module structure itself.</p> <p>The resulting model is cached as a JSON file. The primary purpose of this cache file is to provide a structured, machine-readable representation of the IOD and its modules' attributes, which can be used for further processing or integration in other tools. As a secondary benefit, the cache file is also used to speed up subsequent runs of the CLI scripts.</p> <p>For more information on configuration and caching location see the Configuration and Caching page.</p>"},{"location":"cli/iodattributes/#usage","title":"Usage","text":"<pre><code>poetry run python -m src.dcmspec.cli.iodattributes &lt;table_id&gt; [options]\n</code></pre> <code>&lt;table_id&gt;</code> The DICOM IOD table ID to extract (e.g., <code>table_A.1-1</code> for Composite IODs or <code>table_B.1-1</code> for Normalized IODs). <code>[options]</code> Additional command-line options (see below)."},{"location":"cli/iodattributes/#options","title":"Options","text":"<code>--config &lt;file&gt;</code> Path to the configuration file. <code>--print-mode &lt;mode&gt;</code> Print as <code>'table'</code> (default), <code>'tree'</code>, or <code>'none'</code> to skip printing. <code>-h</code>, <code>--help</code> Show this help message and exit."},{"location":"cli/iodattributes/#examples","title":"Examples","text":"<p>To parse a Composite IOD table and print it as a table:</p> <pre><code>poetry run python -m src.dcmspec.cli.iodattributes table_A.1-1\n</code></pre> <p>To parse a Normalized IOD table and print it as a tree:</p> <pre><code>poetry run python -m src.dcmspec.cli.iodattributes table_B.1-1 --print-mode tree\n</code></pre>"},{"location":"cli/iodmodules/","title":"iodmodules","text":""},{"location":"cli/iodmodules/#description","title":"Description","text":"<p>CLI for extracting, caching, and printing DICOM IOD Module tables from Part 3 of the DICOM standard.</p> <p>This CLI downloads, caches, and prints the list of modules of a given DICOM IOD (Information Object Definition) from Part 3 of the DICOM standard.</p> <p>The tool parses only the specified IOD table to extract the list of referenced modules, including their Information Entity (IE), reference, and usage. It does not parse or include the attributes of the referenced module tables. The output is a table listing all modules for the specified IOD.</p> <p>The resulting model is cached as a JSON file. The primary purpose of this cache file is to provide a structured, machine-readable representation of the IOD's module composition, which can be used for further processing or integration in other tools. As a secondary benefit, the cache file is also used to speed up subsequent runs of the CLI scripts.</p> <p>For more information on configuration and caching location see the Configuration and Caching page.</p>"},{"location":"cli/iodmodules/#usage","title":"Usage","text":"<pre><code>poetry run python -m src.dcmspec.cli.iodmodules &lt;table_id&gt; [options]\n</code></pre> <code>&lt;table_id&gt;</code> The DICOM IOD table ID to extract (e.g., <code>table_A.1-1</code> for Composite IODs or <code>table_B.1-1</code> for Normalized IODs). <code>[options]</code> Additional command-line options (see below)."},{"location":"cli/iodmodules/#options","title":"Options","text":"<code>--config &lt;file&gt;</code> Path to the configuration file. <code>-h</code>, <code>--help</code> Show this help message and exit."},{"location":"cli/iodmodules/#examples","title":"Examples","text":"<p>To parse a Composite IOD table and print its module list as a table:</p> <pre><code>poetry run python -m src.dcmspec.cli.iodmodules table_A.1-1\n</code></pre> <p>To parse a Normalized IOD table and print its module list as a table:</p> <pre><code>poetry run python -m src.dcmspec.cli.iodmodules table_B.1-1\n</code></pre>"},{"location":"cli/modattributes/","title":"modattributes","text":""},{"location":"cli/modattributes/#description","title":"Description","text":"<p>CLI for extracting, caching, and printing DICOM Module Attributes tables from Part 3 of the DICOM standard.</p> <p>This CLI downloads, caches, and prints the attributes of a given DICOM Module from Part 3 of the DICOM standard. Optionally, it can enrich the module with VR, VM, Keyword, or Status information from Part 6 (Data Elements dictionary).</p> <p>The tool parses the specified Module Attributes table to extract all attributes, tags, types, and descriptions for the module. Optionally, it can merge in VR, VM, Keyword, or Status information from Part 6. The output can be printed as a table or tree.</p> <p>The resulting model is cached as a JSON file. The primary purpose of this cache file is to provide a structured, machine-readable representation of the module's attributes, which can be used for further processing or integration in other tools. As a secondary benefit, the cache file is also used to speed up subsequent runs of the CLI scripts.</p> <p>For more information on configuration and caching location see the Configuration and Caching page.</p>"},{"location":"cli/modattributes/#usage","title":"Usage","text":"<pre><code>poetry run python -m src.dcmspec.cli.modattributes &lt;table_id&gt; [options]\n</code></pre> <code>&lt;table_id&gt;</code> The DICOM table ID to extract (e.g., <code>table_C.7-1</code>). <code>[options]</code> Additional command-line options (see below)."},{"location":"cli/modattributes/#options","title":"Options","text":"<code>--config &lt;file&gt;</code> Path to the configuration file. <code>--add-part6 [VR VM Keyword Status]</code> Specification(s) to merge from Part 6 (e.g., <code>--add-part6 VR VM</code>). <code>--force-update</code> Force update of the specifications merged from part 6, even if cached. Only applies when <code>--add-part6</code> is used. <code>--print-mode &lt;mode&gt;</code> Print as <code>'table'</code> (default), <code>'tree'</code>, or <code>'none'</code> to skip printing. <code>--include-depth &lt;int&gt;</code> Depth to which included tables should be parsed (default: unlimited). <code>--force-parse</code> Force reparsing of the DOM and regeneration of the JSON model, even if the JSON cache exists. <code>--force-download</code> Force download of the input file and regeneration of the model, even if cached. Implies <code>--force-parse</code>. <code>-v</code>, <code>--verbose</code> Enable verbose (info-level) logging to the console. <code>-d</code>, <code>--debug</code> Enable debug logging to the console (overrides <code>--verbose</code>). <code>-h</code>, <code>--help</code> Show this help message and exit."},{"location":"cli/modattributes/#examples","title":"Examples","text":"<p>To parse the Patient Module Attributes Table and print it as a table:</p> <pre><code>poetry run python -m src.dcmspec.cli.modattributes table_C.7-1\n</code></pre> <p>To enrich the table with VR and VM information from Part 6:</p> <pre><code>poetry run python -m src.dcmspec.cli.modattributes table_C.7-1 --add-part6 VR VM\n</code></pre> <p>To print the result as a tree:</p> <pre><code>poetry run python -m src.dcmspec.cli.modattributes table_C.7-1 --print-mode tree\n</code></pre>"},{"location":"cli/tdwiicontent/","title":"tdwiicontent","text":""},{"location":"cli/tdwiicontent/#description","title":"Description","text":"<p>CLI for extracting and printing the TDW-II profile content definitions from the IHE-RO TDW-II Supplement.</p> <p>This CLI downloads, parses, and prints the TDW-II content definition tables from the TDW-II IHE-RO Supplement in PDF format. The tool extracts the relevant table(s) for the selected content definition, parses the module definition, and outputs the result as a table and a tree.</p> <p>The resulting model is cached as a JSON file. The primary purpose of this cache file is to provide a structured, machine-readable representation of the content definition, which can be used for further processing or integration in other tools. As a secondary benefit, the cache file is also used to speed up subsequent runs of the CLI scripts.</p> <p>For more information on configuration and caching location see the Configuration and Caching page.</p>"},{"location":"cli/tdwiicontent/#usage","title":"Usage","text":"<pre><code>poetry run python -m src.dcmspec.cli.tdwiicontent &lt;content_definition&gt; [options]\n</code></pre> <code>&lt;content_definition&gt;</code> Required positional argument. One of: <ul> <li><code>ups_create</code>: content of scheduled UPS creation (combined UPS Scheduled and Relationship definitions)</li> <li><code>ups_query</code>: content of C-FIND identifier for UPS Query transaction</li> <li><code>ups_progress</code>: content of N-SET dataset for UPS Progress Update transaction</li> <li><code>ups_performed</code>: content of N-SET dataset for UPS Final Status Update transaction</li> <li><code>rt_bdi</code>: content of RT Beam Delivery Instruction Module</li> </ul> <code>[options]</code> Additional command-line options (see below)."},{"location":"cli/tdwiicontent/#options","title":"Options","text":"<code>-d</code>, <code>--debug</code> Enable debug logging. <code>-v</code>, <code>--verbose</code> Enable verbose output. <code>-h</code>, <code>--help</code> Show this help message and exit."},{"location":"cli/tdwiicontent/#examples","title":"Examples","text":"<p>To extract and print the TDW-II Progress Update transaction UPS content specification:</p> <pre><code>poetry run python -m src.dcmspec.cli.tdwiicontent ups_progress --debug\n</code></pre> <p>To extract and print the RT Beam Delivery Instruction Module definition:</p> <pre><code>poetry run python -m src.dcmspec.cli.tdwiicontent rt_bdi\n</code></pre>"},{"location":"cli/uidvalues/","title":"uidvalues","text":""},{"location":"cli/uidvalues/#description","title":"Description","text":"<p>CLI for extracting, caching, and printing DICOM Unique Identifiers (UIDs) from Part 6 of the DICOM standard.</p> <p>This CLI downloads, caches, and prints the list of DICOM UIDs from Part 6 of the DICOM standard. The tool parses the UID Values table to extract UID values, names, types, and additional information. The output can be printed as a table.</p> <p>The resulting model is cached as a JSON file. The primary purpose of this cache file is to provide a structured, machine-readable representation of the DICOM UIDs, which can be used for further processing or integration in other tools. As a secondary benefit, the cache file is also used to speed up subsequent runs of the CLI scripts.</p> <p>For more information on configuration and caching location see the Configuration and Caching page.</p>"},{"location":"cli/uidvalues/#usage","title":"Usage","text":"<pre><code>poetry run python -m src.dcmspec.cli.uidvalues [options]\n</code></pre> <code>[options]</code> Additional command-line options (see below)."},{"location":"cli/uidvalues/#options","title":"Options","text":"<code>--config &lt;file&gt;</code> Path to the configuration file. <code>--print-mode &lt;mode&gt;</code> Print as <code>'table'</code> (default), <code>'tree'</code>, or <code>'none'</code> to skip printing. <code>-h</code>, <code>--help</code> Show this help message and exit."},{"location":"cli/uidvalues/#examples","title":"Examples","text":"<p>To print the DICOM UID Values as a table:</p> <pre><code>poetry run python -m src.dcmspec.cli.uidvalues\n</code></pre> <p>To print the DICOM UID Values as a tree:</p> <pre><code>poetry run python -m src.dcmspec.cli.uidvalues --print-mode tree\n</code></pre>"},{"location":"cli/upsdimseattributes/","title":"upsdimseattributes","text":""},{"location":"cli/upsdimseattributes/#description","title":"Description","text":"<p>CLI for extracting, caching, and printing DICOM UPS (Unified Procedure Step) DIMSE Service Attribute tables from Part 4 of the DICOM standard.</p> <p>This CLI downloads, caches, and prints the attributes for the UPS DIMSE services from Part 4 of the DICOM standard. The tool parses the UPS Service Attribute table and allows selection of a specific DIMSE service (e.g., N-CREATE, N-SET, N-GET, C-FIND, FINAL) and role (SCU or SCP). The output can be printed as a table.</p> <p>The resulting model is cached as a JSON file. The primary purpose of this cache file is to provide a structured, machine-readable representation of the UPS DIMSE service attributes, which can be used for further processing or integration in other tools. As a secondary benefit, the cache file is also used to speed up subsequent runs of the CLI scripts.</p> <p>For more information on configuration and caching location see the Configuration and Caching page.</p>"},{"location":"cli/upsdimseattributes/#usage","title":"Usage","text":"<pre><code>poetry run python -m src.dcmspec.cli.upsdimseattributes [options]\n</code></pre> <code>[options]</code> Additional command-line options (see below)."},{"location":"cli/upsdimseattributes/#options","title":"Options","text":"<code>--config &lt;file&gt;</code> Path to the configuration file. <code>--dimse &lt;service&gt;</code> DIMSE service to select (<code>ALL_DIMSE</code>, <code>N-CREATE</code>, <code>N-SET</code>, <code>N-GET</code>, <code>C-FIND</code>, <code>FINAL</code>). Default: <code>ALL_DIMSE</code>. <code>--role &lt;role&gt;</code> Role to select (<code>SCU</code> or <code>SCP</code>). Only valid if <code>--dimse</code> is not <code>ALL_DIMSE</code>. <code>-h</code>, <code>--help</code> Show this help message and exit."},{"location":"cli/upsdimseattributes/#examples","title":"Examples","text":"<p>To print all UPS DIMSE service attributes as a table:</p> <pre><code>poetry run python -m src.dcmspec.cli.upsdimseattributes\n</code></pre> <p>To print only the N-CREATE service attributes for the SCU role:</p> <pre><code>poetry run python -m src.dcmspec.cli.upsdimseattributes --dimse N-CREATE --role SCU\n</code></pre>"},{"location":"cli/upsioddimseattributes/","title":"upsioddimseattributes","text":""},{"location":"cli/upsioddimseattributes/#description","title":"Description","text":"<p>CLI for extracting, merging, caching, and printing DICOM UPS IOD attributes aligned with DIMSE service requirements from Part 3 and Part 4 of the DICOM standard.</p> <p>This CLI downloads, merges, caches, and prints the attributes for a DICOM UPS IOD (Unified Procedure Step Information Object Definition) from Part 3, aligned with the requirements of a selected DIMSE service and role from Part 4. The tool parses the IOD table and all referenced module attribute tables, then merges in the UPS DIMSE service requirements (e.g., N-CREATE, N-SET, N-GET, C-FIND, FINAL) and role (SCU or SCP) from Part 4. The output can be printed as a table or tree.</p> <p>The resulting model is cached as a JSON file. The primary purpose of this cache file is to provide a structured, machine-readable representation of the merged IOD and DIMSE service attributes, which can be used for further processing or integration in other tools. As a secondary benefit, the cache file is also used to speed up subsequent runs of the CLI scripts.</p> <p>For more information on configuration and caching location see the Configuration and Caching page.</p>"},{"location":"cli/upsioddimseattributes/#usage","title":"Usage","text":"<pre><code>poetry run python -m src.dcmspec.cli.upsioddimseattributes [options]\n</code></pre> <code>[options]</code> Additional command-line options (see below)."},{"location":"cli/upsioddimseattributes/#options","title":"Options","text":"<code>--config &lt;file&gt;</code> Path to the configuration file. <code>--dimse &lt;service&gt;</code> DIMSE service to use (e.g., <code>ALL_DIMSE</code>, <code>N-CREATE</code>, <code>N-SET</code>, <code>N-GET</code>, <code>C-FIND</code>, <code>FINAL</code>). Default: <code>ALL_DIMSE</code>. <code>--role &lt;role&gt;</code> DIMSE role to use (<code>SCU</code> or <code>SCP</code>). <code>--print-mode &lt;mode&gt;</code> Print as <code>'table'</code> (default), <code>'tree'</code>, or <code>'none'</code> to skip printing. <code>-v</code>, <code>--verbose</code> Enable verbose (info-level) logging to the console. <code>-d</code>, <code>--debug</code> Enable debug logging to the console (overrides <code>--verbose</code>). <code>-h</code>, <code>--help</code> Show this help message and exit."},{"location":"cli/upsioddimseattributes/#examples","title":"Examples","text":"<p>To print the merged UPS IOD attributes for all DIMSE services as a table:</p> <pre><code>poetry run python -m src.dcmspec.cli.upsioddimseattributes\n</code></pre> <p>To print only the N-CREATE service attributes for the SCU role as a tree:</p> <pre><code>poetry run python -m src.dcmspec.cli.upsioddimseattributes --dimse N-CREATE --role SCU --print-mode tree\n</code></pre>"}]}