{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DCMspec Documentation","text":"<p>Welcome to the documentation for dcmspec!</p> <p>dcmspec is a Python library for parsing, modeling, and working with DICOM standard specifications.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>An API to parse DICOM standard and IHE documents specifications into structured models.</li> <li>CLI tools for extracting and printing DICOM specifications.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>See Installation for setup instructions, or browse the API Reference for details.</p> <p>For more information, visit the GitHub repository.</p>"},{"location":"installation/","title":"Installation Using Poetry","text":"<p>To install the package using Poetry, follow these steps:</p>"},{"location":"installation/#add-the-following-to-your-pyprojecttoml","title":"Add the following to your <code>pyproject.toml</code>","text":"<pre><code>[tool.poetry.dependencies]\ndcmspec = { git = \"https://github.com/dwikler/dcmspec.git\", tag = \"v0.1.0\" }\n</code></pre>"},{"location":"installation/#install-the-dependencies","title":"Install the Dependencies","text":"<pre><code>poetry install\n</code></pre>"},{"location":"api/config/","title":"Config","text":""},{"location":"api/config/#dcmspec.config.Config","title":"<code>dcmspec.config.Config</code>","text":"<p>Manages application configuration.</p> <p>Reserved configuration keys: - cache_dir: Cache Directory path used by the library. If not set by the user, OS-specific default is used.</p> <p>Users may add their own keys, but should not overwrite reserved keys unless they intend to change library behavior.</p> Source code in <code>src/dcmspec/config.py</code> <pre><code>class Config:\n    \"\"\"Manages application configuration.\n\n    Reserved configuration keys:\n    - cache_dir: Cache Directory path used by the library. If not set by the user, OS-specific default is used.\n\n    Users may add their own keys, but should not overwrite reserved keys unless they intend to change library behavior.\n    \"\"\"\n\n    def __init__(self, app_name: str = \"dcmspec\", config_file: Optional[str] = None):\n        \"\"\"Initialize the Config object.\n\n        Args:\n            app_name: The application name used for determining default config/cache directories.\n            config_file: Optional path to a specific config file. If not provided, a default location is used.\n\n        \"\"\"\n        self.app_name: str = app_name\n        self.config_file: str = config_file or os.path.join(user_config_dir(app_name), \"config.json\")\n\n        # Check if config_file is a directory; if so, warn and fall back to default config\n        if os.path.isdir(self.config_file):\n            print(f\"Warning: The config_file path '{self.config_file}' is a directory, not a file. Using default.\")\n            self._data: Dict[str, Any] = {\"cache_dir\": user_cache_dir(app_name)}\n            return\n\n        # Initialize config with OS-specific default value for cache directory\n        self._data: Dict[str, Any] = {\"cache_dir\": user_cache_dir(app_name)}\n\n        self.load_config()\n\n    def load_config(self) -&gt; None:\n        \"\"\"Load configuration from the config file if it exists.\n\n        Creates the cache directory if it does not exist.\n        \"\"\"\n        try:\n            if os.path.exists(self.config_file):\n                with open(self.config_file, \"r\", encoding=\"utf-8\") as f:\n                    config: Dict[str, Any] = json.load(f)\n                    self._data.update(config)\n        except (OSError, json.JSONDecodeError) as e:\n            print(f\"Failed to load configuration file {self.config_file}: {e}\")\n\n        cache_dir = self.get_param(\"cache_dir\")\n        try:\n            os.makedirs(cache_dir, exist_ok=True)\n        except FileExistsError:\n            print(f\"Error: The cache_dir path '{cache_dir}' exists and is not a directory.\")\n            return\n\n        # Handle rare case where the path may not be a directory and, for any reason, os.makedirs did not fail.\n        if not os.path.isdir(cache_dir):\n            print(f\"Error: The cache_dir path '{cache_dir}' is not a directory.\")\n            return\n\n    def save_config(self) -&gt; None:\n        \"\"\"Save the current configuration to the config file.\"\"\"\n        try:\n            os.makedirs(os.path.dirname(self.config_file), exist_ok=True)\n            with open(self.config_file, \"w\", encoding=\"utf-8\") as f:\n                json.dump(self._data, f, indent=4)\n        except OSError as e:\n            print(f\"Failed to save configuration file {self.config_file}: {e}\")\n\n    def set_param(self, key: str, value: Any) -&gt; None:\n        \"\"\"Set a configuration parameter.\"\"\"\n        self._data[key] = value\n\n    def get_param(self, key: str) -&gt; Optional[Any]:\n        \"\"\"Get a configuration parameter by key.\"\"\"\n        return self._data.get(key)\n\n    @property\n    def cache_dir(self) -&gt; str:\n        \"\"\"Access the cache directory used by the library.\"\"\"\n        return self.get_param(\"cache_dir\")\n</code></pre>"},{"location":"api/config/#dcmspec.config.Config.cache_dir","title":"<code>cache_dir</code>  <code>property</code>","text":"<p>Access the cache directory used by the library.</p>"},{"location":"api/config/#dcmspec.config.Config.__init__","title":"<code>__init__(app_name='dcmspec', config_file=None)</code>","text":"<p>Initialize the Config object.</p> PARAMETER DESCRIPTION <code>app_name</code> <p>The application name used for determining default config/cache directories.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'dcmspec'</code> </p> <code>config_file</code> <p>Optional path to a specific config file. If not provided, a default location is used.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/dcmspec/config.py</code> <pre><code>def __init__(self, app_name: str = \"dcmspec\", config_file: Optional[str] = None):\n    \"\"\"Initialize the Config object.\n\n    Args:\n        app_name: The application name used for determining default config/cache directories.\n        config_file: Optional path to a specific config file. If not provided, a default location is used.\n\n    \"\"\"\n    self.app_name: str = app_name\n    self.config_file: str = config_file or os.path.join(user_config_dir(app_name), \"config.json\")\n\n    # Check if config_file is a directory; if so, warn and fall back to default config\n    if os.path.isdir(self.config_file):\n        print(f\"Warning: The config_file path '{self.config_file}' is a directory, not a file. Using default.\")\n        self._data: Dict[str, Any] = {\"cache_dir\": user_cache_dir(app_name)}\n        return\n\n    # Initialize config with OS-specific default value for cache directory\n    self._data: Dict[str, Any] = {\"cache_dir\": user_cache_dir(app_name)}\n\n    self.load_config()\n</code></pre>"},{"location":"api/config/#dcmspec.config.Config.get_param","title":"<code>get_param(key)</code>","text":"<p>Get a configuration parameter by key.</p> Source code in <code>src/dcmspec/config.py</code> <pre><code>def get_param(self, key: str) -&gt; Optional[Any]:\n    \"\"\"Get a configuration parameter by key.\"\"\"\n    return self._data.get(key)\n</code></pre>"},{"location":"api/config/#dcmspec.config.Config.load_config","title":"<code>load_config()</code>","text":"<p>Load configuration from the config file if it exists.</p> <p>Creates the cache directory if it does not exist.</p> Source code in <code>src/dcmspec/config.py</code> <pre><code>def load_config(self) -&gt; None:\n    \"\"\"Load configuration from the config file if it exists.\n\n    Creates the cache directory if it does not exist.\n    \"\"\"\n    try:\n        if os.path.exists(self.config_file):\n            with open(self.config_file, \"r\", encoding=\"utf-8\") as f:\n                config: Dict[str, Any] = json.load(f)\n                self._data.update(config)\n    except (OSError, json.JSONDecodeError) as e:\n        print(f\"Failed to load configuration file {self.config_file}: {e}\")\n\n    cache_dir = self.get_param(\"cache_dir\")\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n    except FileExistsError:\n        print(f\"Error: The cache_dir path '{cache_dir}' exists and is not a directory.\")\n        return\n\n    # Handle rare case where the path may not be a directory and, for any reason, os.makedirs did not fail.\n    if not os.path.isdir(cache_dir):\n        print(f\"Error: The cache_dir path '{cache_dir}' is not a directory.\")\n        return\n</code></pre>"},{"location":"api/config/#dcmspec.config.Config.save_config","title":"<code>save_config()</code>","text":"<p>Save the current configuration to the config file.</p> Source code in <code>src/dcmspec/config.py</code> <pre><code>def save_config(self) -&gt; None:\n    \"\"\"Save the current configuration to the config file.\"\"\"\n    try:\n        os.makedirs(os.path.dirname(self.config_file), exist_ok=True)\n        with open(self.config_file, \"w\", encoding=\"utf-8\") as f:\n            json.dump(self._data, f, indent=4)\n    except OSError as e:\n        print(f\"Failed to save configuration file {self.config_file}: {e}\")\n</code></pre>"},{"location":"api/config/#dcmspec.config.Config.set_param","title":"<code>set_param(key, value)</code>","text":"<p>Set a configuration parameter.</p> Source code in <code>src/dcmspec/config.py</code> <pre><code>def set_param(self, key: str, value: Any) -&gt; None:\n    \"\"\"Set a configuration parameter.\"\"\"\n    self._data[key] = value\n</code></pre>"},{"location":"api/doc_handler/","title":"DocHandler","text":""},{"location":"api/doc_handler/#dcmspec.doc_handler.DocHandler","title":"<code>dcmspec.doc_handler.DocHandler</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for DICOM document handlers.</p> <p>Handles DICOM documents in various formats (e.g., XHTML, XML). Subclasses must implement the <code>get_dom</code> and <code>download</code> methods to handle reading/parsing input files and downloading files from URLs.</p> Source code in <code>src/dcmspec/doc_handler.py</code> <pre><code>class DocHandler(ABC):\n    \"\"\"Abstract base class for DICOM document handlers.\n\n    Handles DICOM documents in various formats (e.g., XHTML, XML).\n    Subclasses must implement the `get_dom` and `download` methods to handle\n    reading/parsing input files and downloading files from URLs.\n    \"\"\"\n\n    def __init__(self, config: Optional[Config] = None, logger: Optional[logging.Logger] = None):\n        \"\"\"Initialize the document handler with an optional logger.\n\n        Args:\n            config (Optional[Config]): Config instance to use. If None, a default Config is created.\n            logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n        \"\"\"\n        if logger is not None and not isinstance(logger, logging.Logger):\n            raise TypeError(\"logger must be an instance of logging.Logger or None\")\n        self.logger = logger or logging.getLogger(self.__class__.__name__)\n\n        # Add a StreamHandler and set level if there are no handlers\n        if not self.logger.handlers:\n            self.logger.setLevel(logging.INFO)\n            console_handler = logging.StreamHandler()\n            console_handler.setLevel(logging.INFO)\n            self.logger.addHandler(console_handler)\n\n        if config is not None and not isinstance(config, Config):\n            raise TypeError(\"config must be an instance of Config or None\")\n        self.config = config or Config()\n\n    @abstractmethod\n    def get_dom(self, file_path: str) -&gt; Any:\n        \"\"\"Read and parses the document file, returning a DOM object.\n\n        Args:\n            file_path (str): Path to the document file.\n\n        Returns:\n            Any: The parsed DOM object.\n\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def download(self, url: str, file_path: str) -&gt; None:\n        \"\"\"Download the file from a URL and saves it to the specified path.\n\n        Args:\n            url (str): The URL to download the file from.\n            file_path (str): The path to save the downloaded file.\n\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/doc_handler/#dcmspec.doc_handler.DocHandler.__init__","title":"<code>__init__(config=None, logger=None)</code>","text":"<p>Initialize the document handler with an optional logger.</p> PARAMETER DESCRIPTION <code>config</code> <p>Config instance to use. If None, a default Config is created.</p> <p> TYPE: <code>Optional[Config]</code> DEFAULT: <code>None</code> </p> <code>logger</code> <p>Logger instance to use. If None, a default logger is created.</p> <p> TYPE: <code>Optional[Logger]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/dcmspec/doc_handler.py</code> <pre><code>def __init__(self, config: Optional[Config] = None, logger: Optional[logging.Logger] = None):\n    \"\"\"Initialize the document handler with an optional logger.\n\n    Args:\n        config (Optional[Config]): Config instance to use. If None, a default Config is created.\n        logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n    \"\"\"\n    if logger is not None and not isinstance(logger, logging.Logger):\n        raise TypeError(\"logger must be an instance of logging.Logger or None\")\n    self.logger = logger or logging.getLogger(self.__class__.__name__)\n\n    # Add a StreamHandler and set level if there are no handlers\n    if not self.logger.handlers:\n        self.logger.setLevel(logging.INFO)\n        console_handler = logging.StreamHandler()\n        console_handler.setLevel(logging.INFO)\n        self.logger.addHandler(console_handler)\n\n    if config is not None and not isinstance(config, Config):\n        raise TypeError(\"config must be an instance of Config or None\")\n    self.config = config or Config()\n</code></pre>"},{"location":"api/doc_handler/#dcmspec.doc_handler.DocHandler.download","title":"<code>download(url, file_path)</code>  <code>abstractmethod</code>","text":"<p>Download the file from a URL and saves it to the specified path.</p> PARAMETER DESCRIPTION <code>url</code> <p>The URL to download the file from.</p> <p> TYPE: <code>str</code> </p> <code>file_path</code> <p>The path to save the downloaded file.</p> <p> TYPE: <code>str</code> </p> Source code in <code>src/dcmspec/doc_handler.py</code> <pre><code>@abstractmethod\ndef download(self, url: str, file_path: str) -&gt; None:\n    \"\"\"Download the file from a URL and saves it to the specified path.\n\n    Args:\n        url (str): The URL to download the file from.\n        file_path (str): The path to save the downloaded file.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/doc_handler/#dcmspec.doc_handler.DocHandler.get_dom","title":"<code>get_dom(file_path)</code>  <code>abstractmethod</code>","text":"<p>Read and parses the document file, returning a DOM object.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to the document file.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The parsed DOM object.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>src/dcmspec/doc_handler.py</code> <pre><code>@abstractmethod\ndef get_dom(self, file_path: str) -&gt; Any:\n    \"\"\"Read and parses the document file, returning a DOM object.\n\n    Args:\n        file_path (str): Path to the document file.\n\n    Returns:\n        Any: The parsed DOM object.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/dom_table_spec_parser/","title":"DOMTableSpecParser","text":""},{"location":"api/dom_table_spec_parser/#dcmspec.dom_table_spec_parser.DOMTableSpecParser","title":"<code>dcmspec.dom_table_spec_parser.DOMTableSpecParser</code>","text":"<p>               Bases: <code>SpecParser</code></p> <p>Parser for DICOM specification tables in XHTML DOM format.</p> <p>Provides methods to extract, parse, and structure DICOM specification tables from XHTML documents, returning anytree Node objects as structured in-memory representations. Inherits logging from SpecParser.</p> Source code in <code>src/dcmspec/dom_table_spec_parser.py</code> <pre><code>class DOMTableSpecParser(SpecParser):\n    \"\"\"Parser for DICOM specification tables in XHTML DOM format.\n\n    Provides methods to extract, parse, and structure DICOM specification tables from XHTML documents,\n    returning anytree Node objects as structured in-memory representations.\n    Inherits logging from SpecParser.\n    \"\"\"\n\n    def __init__(self, logger=None):\n        \"\"\"Initialize the DOMTableSpecParser.\n\n        Sets up the parser with an optional logger and a DOMUtils instance for DOM navigation.\n\n        Args:\n            logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n        \"\"\"\n        super().__init__(logger=logger)\n        self.dom_utils = DOMUtils(logger=self.logger)\n\n    def parse(\n        self,\n        dom: BeautifulSoup,\n        table_id: str,\n        column_to_attr: Dict[int, str],\n        name_attr: str,\n        include_depth: Optional[int] = None,  # None means unlimited\n    ) -&gt; tuple[Node, Node]:\n        \"\"\"Parse specification metadata and content from tables in the DOM.\n\n        Parses tables within the DOM of a DICOM document and returns a tuple containing\n        the metadata node and the table content node as structured in-memory representations.\n\n        Args:\n            dom (BeautifulSoup): The parsed XHTML DOM object.\n            table_id (str): The ID of the table to parse.\n            column_to_attr (Dict[int, str]): Mapping from column indices to attribute names for tree nodes.\n            name_attr (str): The attribute name to use for building node names.\n            include_depth (Optional[int], optional): The depth to which included tables should be parsed. \n                None means unlimited.\n\n        Returns:\n            Tuple[Node, Node]: The metadata node and the table content node.\n\n        \"\"\"\n        metadata = self.parse_metadata(dom, table_id, column_to_attr)\n        content = self.parse_table(dom, table_id, column_to_attr, name_attr, include_depth=include_depth)\n        return metadata, content\n\n    def parse_table(\n        self,\n        dom: BeautifulSoup,\n        table_id: str,\n        column_to_attr: Dict[int, str],\n        name_attr: str,\n        table_nesting_level: int = 0,\n        include_depth: Optional[int] = None,  # None means unlimited\n    ) -&gt; Node:\n        \"\"\"Parse specification content from tables within the DOM of a DICOM document.\n\n        This method extracts data from each row of the table, handles nested\n        tables indicated by \"Include\" links, and builds a tree-like structure\n        of the DICOM attributes which root node is assigned to the attribute\n        model.\n\n        Args:\n            dom: The BeautifulSoup DOM object.\n            table_id: The ID of the table to parse.\n            column_to_attr: Mapping between index of columns to parse and tree nodes attributes names\n            name_attr: tree node attribute name to use to build node name\n            table_nesting_level: The nesting level of the table (used for recursion call only).\n            include_depth: The depth to which included tables should be parsed.\n\n        Returns:\n            root: The root node of the tree representation of the specification table.\n\n        \"\"\"\n        self.logger.info(f\"Nesting Level: {table_nesting_level}, Parsing table with id {table_id}\")\n        # Maps column indices in the DICOM standard table to corresponding node attribute names\n        # for constructing a tree-like representation of the table's data.\n        # self.column_to_attr = {**{0: \"elem_name\", 1: \"elem_tag\"}, **(column_to_attr or {})}\n\n        table = self.dom_utils.get_table(dom, table_id)\n        if not table:\n            raise ValueError(f\"Table with id '{table_id}' not found.\")\n\n        if not column_to_attr:\n            raise ValueError(\"Columns to node attributes missing.\")\n        else:\n            self.column_to_attr = column_to_attr\n\n        root = Node(\"content\")\n        level_nodes: Dict[int, Node] = {0: root}\n\n        for row in table.find_all(\"tr\")[1:]:\n            row_data = self._extract_row_data(row, table_nesting_level)\n\n            row_nesting_level = table_nesting_level + row_data[name_attr].count(\"&gt;\")\n\n            # Add nesting level symbols to included table element names except if row is a title\n            if table_nesting_level &gt; 0 and not row_data[name_attr].isupper():\n                row_data[name_attr] = \"&gt;\" * table_nesting_level + row_data[name_attr]\n\n            # Process Include statement unless include_depth is defined and not reached\n            if \"Include\" in row_data[name_attr] and (include_depth is None or include_depth &gt; 0):\n                next_depth = None if include_depth is None else include_depth - 1\n                self._parse_included_table(\n                    dom, row, column_to_attr, name_attr, row_nesting_level, next_depth, level_nodes, root\n                )\n            else:\n                node_name = self._sanitize_string(row_data[name_attr])\n                self._create_node(node_name, row_data, row_nesting_level, level_nodes, root)\n\n        self.logger.info(f\"Nesting Level: {table_nesting_level}, Table parsed successfully\")\n        return root\n\n    def parse_metadata(\n        self,\n        dom: BeautifulSoup,\n        table_id: str,\n        column_to_attr: Dict[int, str],\n    ) -&gt; Node:\n        \"\"\"Parse specification metadata from the document and the table within the DOM of a DICOM document.\n\n        This method extracts the version of the DICOM standard and the headers of the tables.\n\n        Args:\n            dom: The BeautifulSoup DOM object.\n            table_id: The ID of the table to parse.\n            column_to_attr: Mapping between index of columns to parse and attributes name.\n\n        Returns:\n            metadata_node: The root node of the tree representation of the specification metadata.\n\n        \"\"\"\n        table = self.dom_utils.get_table(dom, table_id)\n        if not table:\n            raise ValueError(f\"Table with id '{table_id}' not found.\")\n\n        metadata = Node(\"metadata\")\n        # Parse the DICOM Standard document information\n        version = self.get_version(dom, table_id)\n        metadata.version = version\n        # Parse the Attribute table header\n        header = self._extract_header(table, column_to_attr=column_to_attr)\n        metadata.header = header\n\n        return metadata\n\n    def get_version(self, dom: BeautifulSoup, table_id: str) -&gt; str:\n        \"\"\"Retrieve the DICOM Standard version from the DOM.\n\n        Args:\n            dom: The BeautifulSoup DOM object.\n            table_id: The ID of the table to retrieve.\n\n        Returns:\n            info_node: The info tree node.\n\n        \"\"\"\n        version = self._version_from_book(dom) or self._version_from_section(dom)\n        if not version:\n            version = \"\"\n            self.logger.warning(\"DICOM Standard version not found\")\n        return version\n\n    def _version_from_book(self, dom):\n        \"\"\"Extract version of DICOM books in HTML format.\"\"\"\n        titlepage = dom.find(\"div\", class_=\"titlepage\")\n        if titlepage:\n            subtitle = titlepage.find(\"h2\", class_=\"subtitle\")\n        return subtitle.text.split()[2] if subtitle else None\n\n    def _version_from_section(self, dom):\n        \"\"\"Extract version of DICOM sections in the CHTML format.\"\"\"\n        document_release = dom.find(\"span\", class_=\"documentreleaseinformation\")\n        return document_release.text.split()[2] if document_release else None\n\n    def _extract_row_data(self, row: Tag, table_nesting_level: int) -&gt; Dict[str, Any]:\n        \"\"\"Extract data from a table row.\n\n        Processes each cell in the row, handling colspans and extracting text\n        content from paragraphs within the cells. Constructs a dictionary\n        containing the extracted data.\n\n        Args:\n            row: The table row element (BeautifulSoup Tag).\n            table_nesting_level: The nesting level of the table.\n\n        Returns:\n            A dictionary containing the extracted data from the row.\n\n        \"\"\"\n        # Initialize rowspan trackers if not present\n        if not hasattr(self, \"_rowspan_trackers\") or self._rowspan_trackers is None:\n            self._rowspan_trackers = []\n\n        # Add cells from pending rowspans\n        cells, colspans, rowspans, col_idx = self._handle_pending_rowspans()\n\n        # Process the actual cells in this row\n        col_idx = self._process_actual_cells(row, cells, colspans, rowspans, col_idx)\n\n        # Clean up rowspan trackers for cells that are no longer needed\n        if len(self._rowspan_trackers) &gt; col_idx:\n            self._rowspan_trackers = self._rowspan_trackers[:col_idx]\n\n        # Build row_data dictionary\n        row_data: Dict[str, Any] = {}\n        attr_index = 0\n        for cell, colspan in zip(cells, colspans):\n            if attr_index in self.column_to_attr:\n                row_data[self.column_to_attr[attr_index]] = cell\n            attr_index += colspan\n\n        return row_data\n\n    def _handle_pending_rowspans(self):\n        cells = []\n        colspans = []\n        rowspans = []\n        col_idx = 0\n        for tracker in self._rowspan_trackers:\n            if tracker and tracker[\"rows_left\"] &gt; 0:\n                cells.append(tracker[\"value\"])\n                colspans.append(tracker[\"colspan\"])\n                rowspans.append(tracker[\"rows_left\"])\n                tracker[\"rows_left\"] -= 1\n                col_idx += tracker[\"colspan\"]\n        return cells, colspans, rowspans, col_idx\n\n    def _process_actual_cells(self, row, cells, colspans, rowspans, col_idx):\n        cell_iter = iter(row.find_all(\"td\"))\n        while True:\n            if col_idx &gt;= len(self._rowspan_trackers):\n                self._rowspan_trackers.append(None)\n            if self._rowspan_trackers[col_idx] and self._rowspan_trackers[col_idx][\"rows_left\"] &gt; 0:\n                # Already filled by rowspan above\n                col_idx += self._rowspan_trackers[col_idx][\"colspan\"]\n                continue\n            try:\n                cell = next(cell_iter)\n            except StopIteration:\n                break\n            paragraphs = cell.find_all(\"p\")\n            if paragraphs:\n                cell_text = \"\\n\".join(p.text.strip() for p in paragraphs)\n            else:\n                # Handle cases where no &lt;p&gt; tags present\n                cell_text = cell.get_text(strip=True)\n            colspan = int(cell.get(\"colspan\", 1))\n            rowspan = int(cell.get(\"rowspan\", 1))\n            cells.append(cell_text)\n            colspans.append(colspan)\n            rowspans.append(rowspan)\n\n            for i in range(colspan):\n                while len(self._rowspan_trackers) &lt;= col_idx + i:\n                    self._rowspan_trackers.append(None)\n                # If rowspan &gt; 1, track for future rows\n                if rowspan &gt; 1:\n                    self._rowspan_trackers[col_idx + i] = {\n                        \"value\": cell_text,\n                        \"rows_left\": rowspan - 1,\n                        \"colspan\": 1,\n                    }\n                else:\n                    self._rowspan_trackers[col_idx + i] = None\n            col_idx += colspan\n        return col_idx\n\n    def _parse_included_table(\n        self,\n        dom: BeautifulSoup,\n        row: Tag,\n        column_to_attr: Dict[int, str],\n        name_attr: str,\n        table_nesting_level: int,\n        include_depth: int,\n        level_nodes: Dict[int, Node],\n        root: Node,\n    ) -&gt; None:\n        \"\"\"Recursively parse Included Table.\"\"\"\n        include_anchor = row.find(\"a\", {\"class\": \"xref\"})\n        if not include_anchor:\n            self.logger.warning(f\"Nesting Level: {table_nesting_level}, Include Table Id not found\")\n            return\n\n        include_table_id = include_anchor[\"href\"].split(\"#\", 1)[-1]\n        self.logger.debug(f\"Nesting Level: {table_nesting_level}, Include Table Id: {include_table_id}\")\n\n        included_table_tree = self.parse_table(\n            dom,\n            include_table_id,\n            column_to_attr=column_to_attr,\n            name_attr=name_attr,\n            table_nesting_level=table_nesting_level,\n            include_depth=include_depth,\n        )\n        if not included_table_tree:\n            return\n\n        self._nest_included_table(included_table_tree, level_nodes, table_nesting_level, root)\n\n    def _nest_included_table(\n        self, included_table_tree: Node, level_nodes: Dict[int, Node], row_nesting_level: int, root: Node\n    ) -&gt; None:\n        parent_node = level_nodes.get(row_nesting_level - 1, root)\n        for child in included_table_tree.children:\n            child.parent = parent_node\n\n    def _create_node(\n        self, node_name: str, row_data: Dict[str, Any], row_nesting_level: int, level_nodes: Dict[int, Node], root: Node\n    ) -&gt; None:\n        parent_node = level_nodes.get(row_nesting_level - 1, root)\n        self.logger.debug(\n            f\"Nesting Level: {row_nesting_level}, Name: {node_name}, \"\n            f\"Parent: {parent_node.name if parent_node else 'None'}\"\n        )\n        node = Node(node_name, parent=parent_node, **row_data)\n        level_nodes[row_nesting_level] = node\n\n    def _extract_header(self, table: Tag, column_to_attr: Dict[int, str]) -&gt; list:\n        \"\"\"Extract headers from the table and saves them in the headers attribute.\n\n        Only extracts headers of the columns corresponding to the keys in column_to_attr.\n\n        Args:\n            table: The table element from which to extract headers.\n            column_to_attr: Mapping between index of columns to parse and attributes name. \n\n        \"\"\"\n        cells = table.find_all(\"th\")\n        header = [header.get_text(strip=True) for i, header in enumerate(cells) if i in column_to_attr]\n        self.logger.info(f\"Extracted Header: {header}\")\n        return header\n\n    def _sanitize_string(self, input_string: str) -&gt; str:\n        \"\"\"Sanitize string to use it as a node attribute name.\n\n        - Convert non-ASCII characters to closest ASCII equivalents\n        - Replace space characters with underscores\n        - Replace parentheses characters with dashes\n\n        Args:\n            input_string (str): The string to be sanitized.\n\n        Returns:\n            str: The sanitized string.\n\n        \"\"\"\n        # Normalize the string to NFC form and transliterate to ASCII\n        normalized_str = unidecode(input_string.lower())\n        return re.sub(\n            r\"[ \\-()']\",\n            lambda match: \"-\" if match.group(0) in \"()\" else \"_\",\n            normalized_str,\n        )\n</code></pre>"},{"location":"api/dom_table_spec_parser/#dcmspec.dom_table_spec_parser.DOMTableSpecParser.__init__","title":"<code>__init__(logger=None)</code>","text":"<p>Initialize the DOMTableSpecParser.</p> <p>Sets up the parser with an optional logger and a DOMUtils instance for DOM navigation.</p> PARAMETER DESCRIPTION <code>logger</code> <p>Logger instance to use. If None, a default logger is created.</p> <p> TYPE: <code>Optional[Logger]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/dcmspec/dom_table_spec_parser.py</code> <pre><code>def __init__(self, logger=None):\n    \"\"\"Initialize the DOMTableSpecParser.\n\n    Sets up the parser with an optional logger and a DOMUtils instance for DOM navigation.\n\n    Args:\n        logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n    \"\"\"\n    super().__init__(logger=logger)\n    self.dom_utils = DOMUtils(logger=self.logger)\n</code></pre>"},{"location":"api/dom_table_spec_parser/#dcmspec.dom_table_spec_parser.DOMTableSpecParser.get_version","title":"<code>get_version(dom, table_id)</code>","text":"<p>Retrieve the DICOM Standard version from the DOM.</p> PARAMETER DESCRIPTION <code>dom</code> <p>The BeautifulSoup DOM object.</p> <p> TYPE: <code>BeautifulSoup</code> </p> <code>table_id</code> <p>The ID of the table to retrieve.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>info_node</code> <p>The info tree node.</p> <p> TYPE: <code>str</code> </p> Source code in <code>src/dcmspec/dom_table_spec_parser.py</code> <pre><code>def get_version(self, dom: BeautifulSoup, table_id: str) -&gt; str:\n    \"\"\"Retrieve the DICOM Standard version from the DOM.\n\n    Args:\n        dom: The BeautifulSoup DOM object.\n        table_id: The ID of the table to retrieve.\n\n    Returns:\n        info_node: The info tree node.\n\n    \"\"\"\n    version = self._version_from_book(dom) or self._version_from_section(dom)\n    if not version:\n        version = \"\"\n        self.logger.warning(\"DICOM Standard version not found\")\n    return version\n</code></pre>"},{"location":"api/dom_table_spec_parser/#dcmspec.dom_table_spec_parser.DOMTableSpecParser.parse","title":"<code>parse(dom, table_id, column_to_attr, name_attr, include_depth=None)</code>","text":"<p>Parse specification metadata and content from tables in the DOM.</p> <p>Parses tables within the DOM of a DICOM document and returns a tuple containing the metadata node and the table content node as structured in-memory representations.</p> PARAMETER DESCRIPTION <code>dom</code> <p>The parsed XHTML DOM object.</p> <p> TYPE: <code>BeautifulSoup</code> </p> <code>table_id</code> <p>The ID of the table to parse.</p> <p> TYPE: <code>str</code> </p> <code>column_to_attr</code> <p>Mapping from column indices to attribute names for tree nodes.</p> <p> TYPE: <code>Dict[int, str]</code> </p> <code>name_attr</code> <p>The attribute name to use for building node names.</p> <p> TYPE: <code>str</code> </p> <code>include_depth</code> <p>The depth to which included tables should be parsed.  None means unlimited.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[Node, Node]</code> <p>Tuple[Node, Node]: The metadata node and the table content node.</p> Source code in <code>src/dcmspec/dom_table_spec_parser.py</code> <pre><code>def parse(\n    self,\n    dom: BeautifulSoup,\n    table_id: str,\n    column_to_attr: Dict[int, str],\n    name_attr: str,\n    include_depth: Optional[int] = None,  # None means unlimited\n) -&gt; tuple[Node, Node]:\n    \"\"\"Parse specification metadata and content from tables in the DOM.\n\n    Parses tables within the DOM of a DICOM document and returns a tuple containing\n    the metadata node and the table content node as structured in-memory representations.\n\n    Args:\n        dom (BeautifulSoup): The parsed XHTML DOM object.\n        table_id (str): The ID of the table to parse.\n        column_to_attr (Dict[int, str]): Mapping from column indices to attribute names for tree nodes.\n        name_attr (str): The attribute name to use for building node names.\n        include_depth (Optional[int], optional): The depth to which included tables should be parsed. \n            None means unlimited.\n\n    Returns:\n        Tuple[Node, Node]: The metadata node and the table content node.\n\n    \"\"\"\n    metadata = self.parse_metadata(dom, table_id, column_to_attr)\n    content = self.parse_table(dom, table_id, column_to_attr, name_attr, include_depth=include_depth)\n    return metadata, content\n</code></pre>"},{"location":"api/dom_table_spec_parser/#dcmspec.dom_table_spec_parser.DOMTableSpecParser.parse_metadata","title":"<code>parse_metadata(dom, table_id, column_to_attr)</code>","text":"<p>Parse specification metadata from the document and the table within the DOM of a DICOM document.</p> <p>This method extracts the version of the DICOM standard and the headers of the tables.</p> PARAMETER DESCRIPTION <code>dom</code> <p>The BeautifulSoup DOM object.</p> <p> TYPE: <code>BeautifulSoup</code> </p> <code>table_id</code> <p>The ID of the table to parse.</p> <p> TYPE: <code>str</code> </p> <code>column_to_attr</code> <p>Mapping between index of columns to parse and attributes name.</p> <p> TYPE: <code>Dict[int, str]</code> </p> RETURNS DESCRIPTION <code>metadata_node</code> <p>The root node of the tree representation of the specification metadata.</p> <p> TYPE: <code>Node</code> </p> Source code in <code>src/dcmspec/dom_table_spec_parser.py</code> <pre><code>def parse_metadata(\n    self,\n    dom: BeautifulSoup,\n    table_id: str,\n    column_to_attr: Dict[int, str],\n) -&gt; Node:\n    \"\"\"Parse specification metadata from the document and the table within the DOM of a DICOM document.\n\n    This method extracts the version of the DICOM standard and the headers of the tables.\n\n    Args:\n        dom: The BeautifulSoup DOM object.\n        table_id: The ID of the table to parse.\n        column_to_attr: Mapping between index of columns to parse and attributes name.\n\n    Returns:\n        metadata_node: The root node of the tree representation of the specification metadata.\n\n    \"\"\"\n    table = self.dom_utils.get_table(dom, table_id)\n    if not table:\n        raise ValueError(f\"Table with id '{table_id}' not found.\")\n\n    metadata = Node(\"metadata\")\n    # Parse the DICOM Standard document information\n    version = self.get_version(dom, table_id)\n    metadata.version = version\n    # Parse the Attribute table header\n    header = self._extract_header(table, column_to_attr=column_to_attr)\n    metadata.header = header\n\n    return metadata\n</code></pre>"},{"location":"api/dom_table_spec_parser/#dcmspec.dom_table_spec_parser.DOMTableSpecParser.parse_table","title":"<code>parse_table(dom, table_id, column_to_attr, name_attr, table_nesting_level=0, include_depth=None)</code>","text":"<p>Parse specification content from tables within the DOM of a DICOM document.</p> <p>This method extracts data from each row of the table, handles nested tables indicated by \"Include\" links, and builds a tree-like structure of the DICOM attributes which root node is assigned to the attribute model.</p> PARAMETER DESCRIPTION <code>dom</code> <p>The BeautifulSoup DOM object.</p> <p> TYPE: <code>BeautifulSoup</code> </p> <code>table_id</code> <p>The ID of the table to parse.</p> <p> TYPE: <code>str</code> </p> <code>column_to_attr</code> <p>Mapping between index of columns to parse and tree nodes attributes names</p> <p> TYPE: <code>Dict[int, str]</code> </p> <code>name_attr</code> <p>tree node attribute name to use to build node name</p> <p> TYPE: <code>str</code> </p> <code>table_nesting_level</code> <p>The nesting level of the table (used for recursion call only).</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>include_depth</code> <p>The depth to which included tables should be parsed.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>root</code> <p>The root node of the tree representation of the specification table.</p> <p> TYPE: <code>Node</code> </p> Source code in <code>src/dcmspec/dom_table_spec_parser.py</code> <pre><code>def parse_table(\n    self,\n    dom: BeautifulSoup,\n    table_id: str,\n    column_to_attr: Dict[int, str],\n    name_attr: str,\n    table_nesting_level: int = 0,\n    include_depth: Optional[int] = None,  # None means unlimited\n) -&gt; Node:\n    \"\"\"Parse specification content from tables within the DOM of a DICOM document.\n\n    This method extracts data from each row of the table, handles nested\n    tables indicated by \"Include\" links, and builds a tree-like structure\n    of the DICOM attributes which root node is assigned to the attribute\n    model.\n\n    Args:\n        dom: The BeautifulSoup DOM object.\n        table_id: The ID of the table to parse.\n        column_to_attr: Mapping between index of columns to parse and tree nodes attributes names\n        name_attr: tree node attribute name to use to build node name\n        table_nesting_level: The nesting level of the table (used for recursion call only).\n        include_depth: The depth to which included tables should be parsed.\n\n    Returns:\n        root: The root node of the tree representation of the specification table.\n\n    \"\"\"\n    self.logger.info(f\"Nesting Level: {table_nesting_level}, Parsing table with id {table_id}\")\n    # Maps column indices in the DICOM standard table to corresponding node attribute names\n    # for constructing a tree-like representation of the table's data.\n    # self.column_to_attr = {**{0: \"elem_name\", 1: \"elem_tag\"}, **(column_to_attr or {})}\n\n    table = self.dom_utils.get_table(dom, table_id)\n    if not table:\n        raise ValueError(f\"Table with id '{table_id}' not found.\")\n\n    if not column_to_attr:\n        raise ValueError(\"Columns to node attributes missing.\")\n    else:\n        self.column_to_attr = column_to_attr\n\n    root = Node(\"content\")\n    level_nodes: Dict[int, Node] = {0: root}\n\n    for row in table.find_all(\"tr\")[1:]:\n        row_data = self._extract_row_data(row, table_nesting_level)\n\n        row_nesting_level = table_nesting_level + row_data[name_attr].count(\"&gt;\")\n\n        # Add nesting level symbols to included table element names except if row is a title\n        if table_nesting_level &gt; 0 and not row_data[name_attr].isupper():\n            row_data[name_attr] = \"&gt;\" * table_nesting_level + row_data[name_attr]\n\n        # Process Include statement unless include_depth is defined and not reached\n        if \"Include\" in row_data[name_attr] and (include_depth is None or include_depth &gt; 0):\n            next_depth = None if include_depth is None else include_depth - 1\n            self._parse_included_table(\n                dom, row, column_to_attr, name_attr, row_nesting_level, next_depth, level_nodes, root\n            )\n        else:\n            node_name = self._sanitize_string(row_data[name_attr])\n            self._create_node(node_name, row_data, row_nesting_level, level_nodes, root)\n\n    self.logger.info(f\"Nesting Level: {table_nesting_level}, Table parsed successfully\")\n    return root\n</code></pre>"},{"location":"api/dom_utils/","title":"DOMUtils","text":""},{"location":"api/dom_utils/#dcmspec.dom_utils.DOMUtils","title":"<code>dcmspec.dom_utils.DOMUtils</code>","text":"<p>Utility class for DOM navigation and extraction in DICOM XHTML documents.</p> <p>Provides methods for locating tables and table IDs within a parsed BeautifulSoup DOM, with optional logging for warnings and debug information.</p> Typical usage <p>dom_utils = DOMUtils(logger=logger) table = dom_utils.get_table(dom, table_id) table_id = dom_utils.get_table_id_from_section(dom, section_id)</p> Source code in <code>src/dcmspec/dom_utils.py</code> <pre><code>class DOMUtils:\n    \"\"\"Utility class for DOM navigation and extraction in DICOM XHTML documents.\n\n    Provides methods for locating tables and table IDs within a parsed BeautifulSoup DOM,\n    with optional logging for warnings and debug information.\n\n    Typical usage:\n        dom_utils = DOMUtils(logger=logger)\n        table = dom_utils.get_table(dom, table_id)\n        table_id = dom_utils.get_table_id_from_section(dom, section_id)\n    \"\"\"\n\n    def __init__(self, logger: Optional[logging.Logger] = None):\n        \"\"\"Initialize DOMUtils with an optional logger.\n\n        Args:\n            logger (Optional[logging.Logger]): Logger instance to use for warnings and debug messages.\n                If None, a default logger is created.\n\n        \"\"\"\n        if logger is not None and not isinstance(logger, logging.Logger):\n            raise TypeError(\"logger must be an instance of logging.Logger or None\")\n        self.logger = logger or logging.getLogger(self.__class__.__name__)\n\n    def get_table(self, dom: BeautifulSoup, table_id: str) -&gt; Optional[Tag]:\n        \"\"\"Retrieve the table element with the specified ID from the DOM.\n\n        DocBook XML to XHTML conversion stylesheets enclose tables in a\n        &lt;div class=\"table\"&gt; with the table identifier in &lt;a id=\"table_ID\"&gt;&lt;/a&gt;\n\n        Searches for an anchor tag with the given ID and then finds the next\n        table element.\n\n        Args:\n            dom: The BeautifulSoup DOM object.\n            table_id: The ID of the table to retrieve.\n\n        Returns:\n            The table element if found, otherwise None.\n\n        \"\"\"\n        anchor = dom.find(\"a\", {\"id\": table_id})\n        if anchor is None:\n            self.logger.warning(f\"Table Id {table_id} not found.\")\n            return None\n        table_div = anchor.find_parent(\"div\", class_=\"table\")\n        if not table_div:\n            self.logger.warning(f\"Parent &lt;div class='table'&gt; for Table Id {table_id} not found.\")\n            return None\n        table = table_div.find(\"table\")\n        if not table:\n            self.logger.warning(f\"Table for Table Id {table_id} not found inside its &lt;div class='table'&gt;.\")\n            return None\n        return table\n\n    def get_table_id_from_section(self, dom: BeautifulSoup, section_id: str) -&gt; Optional[str]:\n        \"\"\"Get the id of the first table in a section.\n\n        Retrieve the first table_id (anchor id) of a &lt;div class=\"table\"&gt; inside a &lt;div class=\"section\"&gt;\n        that contains an &lt;a&gt; anchor with the given section id.\n\n        Args:\n            dom (BeautifulSoup): The parsed XHTML DOM object.\n            section_id (str): The id of the section to search for the table_id.\n\n        Returns:\n            Optional[str]: The id of the first table anchor found, or None if not found.\n\n        \"\"\"\n        # Find the anchor with the given id\n        anchor = dom.find(\"a\", {\"id\": section_id})\n        if not anchor:\n            self.logger.warning(f\"Section with id '{section_id}' not found.\")\n            return None\n\n        # Find the parent section div\n        section_div = anchor.find_parent(\"div\", class_=\"section\")\n        if not section_div:\n            self.logger.warning(f\"No parent &lt;div class='section'&gt; found for section id '{section_id}'.\")\n            return None\n\n        # Find the first &lt;div class=\"table\"&gt; inside the section\n        table_div = section_div.find(\"div\", class_=\"table\")\n        if not table_div:\n            self.logger.warning(f\"No &lt;div class='table'&gt; found in section for section id '{section_id}'.\")\n            return None\n\n        # Find the first anchor with an id inside the table div (the table id)\n        table_anchor = table_div.find(\"a\", id=True)\n        if table_anchor and table_anchor.get(\"id\"):\n            return table_anchor[\"id\"]\n\n        self.logger.warning(f\"No table id found in &lt;div class='table'&gt; for section id '{section_id}'.\")\n        return None\n</code></pre>"},{"location":"api/dom_utils/#dcmspec.dom_utils.DOMUtils.__init__","title":"<code>__init__(logger=None)</code>","text":"<p>Initialize DOMUtils with an optional logger.</p> PARAMETER DESCRIPTION <code>logger</code> <p>Logger instance to use for warnings and debug messages. If None, a default logger is created.</p> <p> TYPE: <code>Optional[Logger]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/dcmspec/dom_utils.py</code> <pre><code>def __init__(self, logger: Optional[logging.Logger] = None):\n    \"\"\"Initialize DOMUtils with an optional logger.\n\n    Args:\n        logger (Optional[logging.Logger]): Logger instance to use for warnings and debug messages.\n            If None, a default logger is created.\n\n    \"\"\"\n    if logger is not None and not isinstance(logger, logging.Logger):\n        raise TypeError(\"logger must be an instance of logging.Logger or None\")\n    self.logger = logger or logging.getLogger(self.__class__.__name__)\n</code></pre>"},{"location":"api/dom_utils/#dcmspec.dom_utils.DOMUtils.get_table","title":"<code>get_table(dom, table_id)</code>","text":"<p>Retrieve the table element with the specified ID from the DOM.</p> <p>DocBook XML to XHTML conversion stylesheets enclose tables in a</p>  with the table identifier in   Searches for an anchor tag with the given ID and then finds the next table element.    PARAMETER DESCRIPTION <code>dom</code> <p>The BeautifulSoup DOM object.</p> <p> TYPE: <code>BeautifulSoup</code> </p> <code>table_id</code> <p>The ID of the table to retrieve.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[Tag]</code> <p>The table element if found, otherwise None.</p> Source code in <code>src/dcmspec/dom_utils.py</code> <pre><code>def get_table(self, dom: BeautifulSoup, table_id: str) -&gt; Optional[Tag]:\n    \"\"\"Retrieve the table element with the specified ID from the DOM.\n\n    DocBook XML to XHTML conversion stylesheets enclose tables in a\n    &lt;div class=\"table\"&gt; with the table identifier in &lt;a id=\"table_ID\"&gt;&lt;/a&gt;\n\n    Searches for an anchor tag with the given ID and then finds the next\n    table element.\n\n    Args:\n        dom: The BeautifulSoup DOM object.\n        table_id: The ID of the table to retrieve.\n\n    Returns:\n        The table element if found, otherwise None.\n\n    \"\"\"\n    anchor = dom.find(\"a\", {\"id\": table_id})\n    if anchor is None:\n        self.logger.warning(f\"Table Id {table_id} not found.\")\n        return None\n    table_div = anchor.find_parent(\"div\", class_=\"table\")\n    if not table_div:\n        self.logger.warning(f\"Parent &lt;div class='table'&gt; for Table Id {table_id} not found.\")\n        return None\n    table = table_div.find(\"table\")\n    if not table:\n        self.logger.warning(f\"Table for Table Id {table_id} not found inside its &lt;div class='table'&gt;.\")\n        return None\n    return table\n</code></pre>"},{"location":"api/dom_utils/#dcmspec.dom_utils.DOMUtils.get_table_id_from_section","title":"<code>get_table_id_from_section(dom, section_id)</code>","text":"<p>Get the id of the first table in a section.</p> <p>Retrieve the first table_id (anchor id) of a  inside a  that contains an  anchor with the given section id. PARAMETER DESCRIPTION <code>dom</code> <p>The parsed XHTML DOM object.</p> <p> TYPE: <code>BeautifulSoup</code> </p> <code>section_id</code> <p>The id of the section to search for the table_id.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[str]</code> <p>Optional[str]: The id of the first table anchor found, or None if not found.</p> Source code in <code>src/dcmspec/dom_utils.py</code> <pre><code>def get_table_id_from_section(self, dom: BeautifulSoup, section_id: str) -&gt; Optional[str]:\n    \"\"\"Get the id of the first table in a section.\n\n    Retrieve the first table_id (anchor id) of a &lt;div class=\"table\"&gt; inside a &lt;div class=\"section\"&gt;\n    that contains an &lt;a&gt; anchor with the given section id.\n\n    Args:\n        dom (BeautifulSoup): The parsed XHTML DOM object.\n        section_id (str): The id of the section to search for the table_id.\n\n    Returns:\n        Optional[str]: The id of the first table anchor found, or None if not found.\n\n    \"\"\"\n    # Find the anchor with the given id\n    anchor = dom.find(\"a\", {\"id\": section_id})\n    if not anchor:\n        self.logger.warning(f\"Section with id '{section_id}' not found.\")\n        return None\n\n    # Find the parent section div\n    section_div = anchor.find_parent(\"div\", class_=\"section\")\n    if not section_div:\n        self.logger.warning(f\"No parent &lt;div class='section'&gt; found for section id '{section_id}'.\")\n        return None\n\n    # Find the first &lt;div class=\"table\"&gt; inside the section\n    table_div = section_div.find(\"div\", class_=\"table\")\n    if not table_div:\n        self.logger.warning(f\"No &lt;div class='table'&gt; found in section for section id '{section_id}'.\")\n        return None\n\n    # Find the first anchor with an id inside the table div (the table id)\n    table_anchor = table_div.find(\"a\", id=True)\n    if table_anchor and table_anchor.get(\"id\"):\n        return table_anchor[\"id\"]\n\n    self.logger.warning(f\"No table id found in &lt;div class='table'&gt; for section id '{section_id}'.\")\n    return None\n</code></pre>"},{"location":"api/iod_spec_builder/","title":"IODSpecBuilder","text":""},{"location":"api/iod_spec_builder/#dcmspec.iod_spec_builder.IODSpecBuilder","title":"<code>dcmspec.iod_spec_builder.IODSpecBuilder</code>","text":"<p>Orchestrates the construction of a expanded DICOM IOD specification model.</p> <p>The IODSpecBuilder uses a factory to build the IOD Modules model and, for each referenced module, uses a (possibly different) factory to build and cache the Module models. It then assembles a new model with the IOD nodes and their referenced module nodes as children, and caches the expanded model.</p> Source code in <code>src/dcmspec/iod_spec_builder.py</code> <pre><code>class IODSpecBuilder:\n    \"\"\"Orchestrates the construction of a expanded DICOM IOD specification model.\n\n    The IODSpecBuilder uses a factory to build the IOD Modules model and, for each referenced module,\n    uses a (possibly different) factory to build and cache the Module models. It then assembles a new\n    model with the IOD nodes and their referenced module nodes as children, and caches the expanded model.\n    \"\"\"\n\n    def __init__(\n        self,\n        iod_factory: SpecFactory = None,\n        module_factory: SpecFactory = None,\n        logger: logging.Logger = None,\n    ):\n        \"\"\"Initialize the IODSpecBuilder.\n\n        If no factory is provided, a default SpecFactory is used for both IOD and module models.\n\n        Args:\n            iod_factory (Optional[SpecFactory]): Factory for building the IOD model. If None, uses SpecFactory().\n            module_factory (Optional[SpecFactory]): Factory for building module models. If None, uses iod_factory.\n            logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n        The builder is initialized with factories for the IOD and module models. By default, the same\n        factory is used for both, but a different factory can be provided for modules if needed.\n\n        \"\"\"\n        self.logger = logger or logging.getLogger(self.__class__.__name__)\n        self.iod_factory = iod_factory or SpecFactory(logger=self.logger)\n        self.module_factory = module_factory or self.iod_factory\n        self.dom_utils = DOMUtils(logger=self.logger)\n\n    def build_from_url(\n        self,\n        url: str,\n        cache_file_name: str,\n        table_id: str,\n        force_download: bool = False,\n        json_file_name: str = None,\n        **kwargs: object,\n    ) -&gt; SpecModel:\n        \"\"\"Build and cache a DICOM IOD specification model from a URL.\n\n        This method orchestrates the full workflow:\n        - Loads or downloads the IOD table and builds/caches the IOD model using the iod_factory.\n        - Finds all nodes in the IOD model with a \"ref\" attribute, indicating a referenced module.\n        - For each referenced module, loads or parses and caches the module model using the module_factory.\n        - Assembles a new expanded model, where each IOD node has its referenced module's content node as a child.\n        - Uses the first module's metadata header and version for the expanded model's metadata.\n        - Caches the expanded model if a json_file_name is provided.\n\n        Args:\n            url (str): The URL to download the input file from.\n            cache_file_name (str): Filename of the cached input file.\n            table_id (str): The ID of the IOD table to parse.\n            force_download (bool): If True, always download the input file and generate the model even if cached.\n            json_file_name (str, optional): Filename to save the cached expanded JSON model.\n            **kwargs: Additional arguments for model construction.\n\n        Returns:\n            SpecModel: The expanded model with IOD and module content.\n\n        \"\"\"\n        # Load from cache if the expanded IOD model is already present\n        cached_model = self._load_expanded_model_from_cache(json_file_name, force_download)\n        if cached_model is not None:\n            return cached_model\n\n        # Load the DOM from cache file or download and cache DOM in memory.\n        dom = self.iod_factory.load_dom(\n            url=url,\n            cache_file_name=cache_file_name,\n            force_download=force_download,\n        )\n\n        # Build the IOD Modules model from the DOM\n        iodmodules_model = self.iod_factory.build_model(\n            dom=dom,\n            table_id=table_id,\n            url=url,\n            json_file_name=json_file_name,\n            **kwargs,\n        )\n\n        # Find all nodes with a \"ref\" attribute in the IOD Modules model\n        nodes_with_ref = [node for node in iodmodules_model.content.children if hasattr(node, \"ref\")]\n\n        # Build or load module models for each referenced section\n        module_models = self._build_module_models(nodes_with_ref, dom, url)\n        # Fail if no module models were found.\n        if not module_models:\n            raise RuntimeError(\"No module models were found for the referenced modules in the IOD table.\")\n\n        # Create the expanded model from the IOD modules and module models\n        iod_model = self._create_expanded_model(iodmodules_model, module_models)\n\n        # Cache the expanded model if a json_file_name was provided\n        if json_file_name:\n            iod_json_file_path = os.path.join(\n                self.iod_factory.config.get_param(\"cache_dir\"), \"model\", json_file_name\n            )\n            try:\n                self.iod_factory.model_store.save(iod_model, iod_json_file_path)\n            except Exception as e:\n                self.logger.warning(f\"Failed to cache expanded model to {iod_json_file_path}: {e}\")\n        else:\n            self.logger.info(\"No json_file_name specified; IOD model not cached.\")\n\n        return iod_model\n\n    def _load_expanded_model_from_cache(self, json_file_name: str, force_download: bool) -&gt; SpecModel | None:\n        \"\"\"Return the cached expanded IOD model if available and not force_download, else None.\"\"\"\n        iod_json_file_path = None\n        if json_file_name:\n            iod_json_file_path = os.path.join(\n                self.iod_factory.config.get_param(\"cache_dir\"), \"model\", json_file_name\n            )\n        if iod_json_file_path and os.path.exists(iod_json_file_path) and not force_download:\n            try:\n                return self.iod_factory.model_store.load(iod_json_file_path)\n            except Exception as e:\n                self.logger.warning(\n                    f\"Failed to load expanded IOD model from cache {iod_json_file_path}: {e}\"\n                )\n        return None\n\n    def _build_module_models(self, nodes_with_ref, dom, url) -&gt; dict:\n        \"\"\"Build or load module models for each referenced section.\"\"\"\n        module_models = {}\n        for node in nodes_with_ref:\n            ref_value = getattr(node, \"ref\", None)\n            if not ref_value:\n                continue\n            section_id = f\"sect_{ref_value}\"\n            module_table_id = self.dom_utils.get_table_id_from_section(dom, section_id)\n            if not module_table_id:\n                self.logger.warning(f\"No table found for section id {section_id}\")\n                continue\n\n            module_json_file_name = f\"{module_table_id}.json\"\n            module_json_file_path = os.path.join(\n                self.module_factory.config.get_param(\"cache_dir\"), \"model\", module_json_file_name\n            )\n            if os.path.exists(module_json_file_path):\n                try:\n                    module_model = self.module_factory.model_store.load(module_json_file_path)\n                except Exception as e:\n                    self.logger.warning(f\"Failed to load module model from cache {module_json_file_path}: {e}\")\n                    module_model = self.module_factory.build_model(\n                        dom=dom,\n                        table_id=module_table_id,\n                        url=url,\n                        json_file_name=module_json_file_name,\n                    )\n            else:\n                module_model = self.module_factory.build_model(\n                    dom=dom,\n                    table_id=module_table_id,\n                    url=url,\n                    json_file_name=module_json_file_name,\n                )\n            module_models[ref_value] = module_model\n        return module_models\n\n    def _create_expanded_model(self, iodmodules_model: SpecModel, module_models: dict) -&gt; SpecModel:\n        \"\"\"Create the expanded model by attaching Module nodes content to IOD nodes.\"\"\"\n        # Use the first module's metadata node for the expanded model\n        first_module = next(iter(module_models.values()))\n        iod_metadata = first_module.metadata\n\n        # The content node will have as children the IOD model's nodes,\n        # and for each referenced module, its content's children will be attached directly under the iod node\n        iod_content = Node(\"content\")\n        for iod_node in iodmodules_model.content.children:\n            ref_value = getattr(iod_node, \"ref\", None)\n            if ref_value and ref_value in module_models:\n                module_content = module_models[ref_value].content\n                for child in list(module_content.children):\n                    child.parent = iod_node\n            iod_node.parent = iod_content\n\n        # Create and return the expanded model\n        return SpecModel(metadata=iod_metadata, content=iod_content)\n</code></pre>"},{"location":"api/iod_spec_builder/#dcmspec.iod_spec_builder.IODSpecBuilder.__init__","title":"<code>__init__(iod_factory=None, module_factory=None, logger=None)</code>","text":"<p>Initialize the IODSpecBuilder.</p> <p>If no factory is provided, a default SpecFactory is used for both IOD and module models.</p> PARAMETER DESCRIPTION <code>iod_factory</code> <p>Factory for building the IOD model. If None, uses SpecFactory().</p> <p> TYPE: <code>Optional[SpecFactory]</code> DEFAULT: <code>None</code> </p> <code>module_factory</code> <p>Factory for building module models. If None, uses iod_factory.</p> <p> TYPE: <code>Optional[SpecFactory]</code> DEFAULT: <code>None</code> </p> <code>logger</code> <p>Logger instance to use. If None, a default logger is created.</p> <p> TYPE: <code>Optional[Logger]</code> DEFAULT: <code>None</code> </p> <p>The builder is initialized with factories for the IOD and module models. By default, the same factory is used for both, but a different factory can be provided for modules if needed.</p> Source code in <code>src/dcmspec/iod_spec_builder.py</code> <pre><code>def __init__(\n    self,\n    iod_factory: SpecFactory = None,\n    module_factory: SpecFactory = None,\n    logger: logging.Logger = None,\n):\n    \"\"\"Initialize the IODSpecBuilder.\n\n    If no factory is provided, a default SpecFactory is used for both IOD and module models.\n\n    Args:\n        iod_factory (Optional[SpecFactory]): Factory for building the IOD model. If None, uses SpecFactory().\n        module_factory (Optional[SpecFactory]): Factory for building module models. If None, uses iod_factory.\n        logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n    The builder is initialized with factories for the IOD and module models. By default, the same\n    factory is used for both, but a different factory can be provided for modules if needed.\n\n    \"\"\"\n    self.logger = logger or logging.getLogger(self.__class__.__name__)\n    self.iod_factory = iod_factory or SpecFactory(logger=self.logger)\n    self.module_factory = module_factory or self.iod_factory\n    self.dom_utils = DOMUtils(logger=self.logger)\n</code></pre>"},{"location":"api/iod_spec_builder/#dcmspec.iod_spec_builder.IODSpecBuilder.build_from_url","title":"<code>build_from_url(url, cache_file_name, table_id, force_download=False, json_file_name=None, **kwargs)</code>","text":"<p>Build and cache a DICOM IOD specification model from a URL.</p> <p>This method orchestrates the full workflow: - Loads or downloads the IOD table and builds/caches the IOD model using the iod_factory. - Finds all nodes in the IOD model with a \"ref\" attribute, indicating a referenced module. - For each referenced module, loads or parses and caches the module model using the module_factory. - Assembles a new expanded model, where each IOD node has its referenced module's content node as a child. - Uses the first module's metadata header and version for the expanded model's metadata. - Caches the expanded model if a json_file_name is provided.</p> PARAMETER DESCRIPTION <code>url</code> <p>The URL to download the input file from.</p> <p> TYPE: <code>str</code> </p> <code>cache_file_name</code> <p>Filename of the cached input file.</p> <p> TYPE: <code>str</code> </p> <code>table_id</code> <p>The ID of the IOD table to parse.</p> <p> TYPE: <code>str</code> </p> <code>force_download</code> <p>If True, always download the input file and generate the model even if cached.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>json_file_name</code> <p>Filename to save the cached expanded JSON model.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>Additional arguments for model construction.</p> <p> TYPE: <code>object</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>SpecModel</code> <p>The expanded model with IOD and module content.</p> <p> TYPE: <code>SpecModel</code> </p> Source code in <code>src/dcmspec/iod_spec_builder.py</code> <pre><code>def build_from_url(\n    self,\n    url: str,\n    cache_file_name: str,\n    table_id: str,\n    force_download: bool = False,\n    json_file_name: str = None,\n    **kwargs: object,\n) -&gt; SpecModel:\n    \"\"\"Build and cache a DICOM IOD specification model from a URL.\n\n    This method orchestrates the full workflow:\n    - Loads or downloads the IOD table and builds/caches the IOD model using the iod_factory.\n    - Finds all nodes in the IOD model with a \"ref\" attribute, indicating a referenced module.\n    - For each referenced module, loads or parses and caches the module model using the module_factory.\n    - Assembles a new expanded model, where each IOD node has its referenced module's content node as a child.\n    - Uses the first module's metadata header and version for the expanded model's metadata.\n    - Caches the expanded model if a json_file_name is provided.\n\n    Args:\n        url (str): The URL to download the input file from.\n        cache_file_name (str): Filename of the cached input file.\n        table_id (str): The ID of the IOD table to parse.\n        force_download (bool): If True, always download the input file and generate the model even if cached.\n        json_file_name (str, optional): Filename to save the cached expanded JSON model.\n        **kwargs: Additional arguments for model construction.\n\n    Returns:\n        SpecModel: The expanded model with IOD and module content.\n\n    \"\"\"\n    # Load from cache if the expanded IOD model is already present\n    cached_model = self._load_expanded_model_from_cache(json_file_name, force_download)\n    if cached_model is not None:\n        return cached_model\n\n    # Load the DOM from cache file or download and cache DOM in memory.\n    dom = self.iod_factory.load_dom(\n        url=url,\n        cache_file_name=cache_file_name,\n        force_download=force_download,\n    )\n\n    # Build the IOD Modules model from the DOM\n    iodmodules_model = self.iod_factory.build_model(\n        dom=dom,\n        table_id=table_id,\n        url=url,\n        json_file_name=json_file_name,\n        **kwargs,\n    )\n\n    # Find all nodes with a \"ref\" attribute in the IOD Modules model\n    nodes_with_ref = [node for node in iodmodules_model.content.children if hasattr(node, \"ref\")]\n\n    # Build or load module models for each referenced section\n    module_models = self._build_module_models(nodes_with_ref, dom, url)\n    # Fail if no module models were found.\n    if not module_models:\n        raise RuntimeError(\"No module models were found for the referenced modules in the IOD table.\")\n\n    # Create the expanded model from the IOD modules and module models\n    iod_model = self._create_expanded_model(iodmodules_model, module_models)\n\n    # Cache the expanded model if a json_file_name was provided\n    if json_file_name:\n        iod_json_file_path = os.path.join(\n            self.iod_factory.config.get_param(\"cache_dir\"), \"model\", json_file_name\n        )\n        try:\n            self.iod_factory.model_store.save(iod_model, iod_json_file_path)\n        except Exception as e:\n            self.logger.warning(f\"Failed to cache expanded model to {iod_json_file_path}: {e}\")\n    else:\n        self.logger.info(\"No json_file_name specified; IOD model not cached.\")\n\n    return iod_model\n</code></pre>"},{"location":"api/iod_spec_printer/","title":"IODSpecPrinter","text":""},{"location":"api/iod_spec_printer/#dcmspec.iod_spec_printer.IODSpecPrinter","title":"<code>dcmspec.iod_spec_printer.IODSpecPrinter</code>","text":"<p>               Bases: <code>SpecPrinter</code></p> <p>Printer for DICOM IOD specification models with mixed node types.</p> <p>Overrides print_table to display IOD Modules nodes as a one-cell title row (spanning all columns) The table columns are those of the Module Attributes nodes.</p> Source code in <code>src/dcmspec/iod_spec_printer.py</code> <pre><code>class IODSpecPrinter(SpecPrinter):\n    \"\"\"Printer for DICOM IOD specification models with mixed node types.\n\n    Overrides print_table to display IOD Modules nodes as a one-cell title row (spanning all columns)\n    The table columns are those of the Module Attributes nodes.\n    \"\"\"\n\n    def print_table(self, colorize: bool = False):\n        \"\"\"Print the specification model as a flat table with module title rows.\n\n        Args:\n            colorize (bool): Whether to colorize the output by node depth.\n\n        \"\"\"\n        table = Table(show_header=True, header_style=\"bold magenta\", show_lines=True, box=box.ASCII_DOUBLE_HEAD)\n\n        attr_headers = list(self.model.metadata.header)\n        for header in attr_headers:\n            table.add_column(header, width=20)\n\n        # Traverse the tree in PreOrder (as in the base class)\n        for node in PreOrderIter(self.model.content):\n            # skip the root node\n            if node.name == \"content\":\n                continue            # Print IOD module nodes as a title row (one cell, spanning all columns)\n            if hasattr(node, \"module\"):\n                iod_title = getattr(node, \"module\", getattr(node, \"name\", \"\"))\n                iod_usage = getattr(node, \"usage\", \"\")\n                iod_title_text = f\"{iod_title} Module ({iod_usage})\" if iod_usage else iod_title\n                # Set title style\n                row_style = (\n                    \"magenta\" if colorize else None\n                )\n                table.add_row(iod_title_text, *[\"\"] * (len(attr_headers) - 1), style=row_style)\n            # Print module attribute nodes as regular rows\n            else:\n                row = [getattr(node, attr, \"\") for attr in self.model.metadata.column_to_attr.values()]\n                row_style = None\n                if colorize:\n                    row_style = (\n                        \"yellow\"\n                        if self.model._is_include(node)\n                        else \"magenta\"\n                        if self.model._is_title(node)\n                        else LEVEL_COLORS[(node.depth - 1) % len(LEVEL_COLORS)]\n                    )\n                table.add_row(*row, style=row_style)\n\n        self.console.print(table)\n</code></pre>"},{"location":"api/iod_spec_printer/#dcmspec.iod_spec_printer.IODSpecPrinter.print_table","title":"<code>print_table(colorize=False)</code>","text":"<p>Print the specification model as a flat table with module title rows.</p> PARAMETER DESCRIPTION <code>colorize</code> <p>Whether to colorize the output by node depth.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/dcmspec/iod_spec_printer.py</code> <pre><code>def print_table(self, colorize: bool = False):\n    \"\"\"Print the specification model as a flat table with module title rows.\n\n    Args:\n        colorize (bool): Whether to colorize the output by node depth.\n\n    \"\"\"\n    table = Table(show_header=True, header_style=\"bold magenta\", show_lines=True, box=box.ASCII_DOUBLE_HEAD)\n\n    attr_headers = list(self.model.metadata.header)\n    for header in attr_headers:\n        table.add_column(header, width=20)\n\n    # Traverse the tree in PreOrder (as in the base class)\n    for node in PreOrderIter(self.model.content):\n        # skip the root node\n        if node.name == \"content\":\n            continue            # Print IOD module nodes as a title row (one cell, spanning all columns)\n        if hasattr(node, \"module\"):\n            iod_title = getattr(node, \"module\", getattr(node, \"name\", \"\"))\n            iod_usage = getattr(node, \"usage\", \"\")\n            iod_title_text = f\"{iod_title} Module ({iod_usage})\" if iod_usage else iod_title\n            # Set title style\n            row_style = (\n                \"magenta\" if colorize else None\n            )\n            table.add_row(iod_title_text, *[\"\"] * (len(attr_headers) - 1), style=row_style)\n        # Print module attribute nodes as regular rows\n        else:\n            row = [getattr(node, attr, \"\") for attr in self.model.metadata.column_to_attr.values()]\n            row_style = None\n            if colorize:\n                row_style = (\n                    \"yellow\"\n                    if self.model._is_include(node)\n                    else \"magenta\"\n                    if self.model._is_title(node)\n                    else LEVEL_COLORS[(node.depth - 1) % len(LEVEL_COLORS)]\n                )\n            table.add_row(*row, style=row_style)\n\n    self.console.print(table)\n</code></pre>"},{"location":"api/json_spec_store/","title":"JSONSpecStore","text":""},{"location":"api/json_spec_store/#dcmspec.json_spec_store.JSONSpecStore","title":"<code>dcmspec.json_spec_store.JSONSpecStore</code>","text":"<p>               Bases: <code>SpecStore</code></p> <p>Model store class for DICOM specification models storage in JSON format.</p> <p>Provides methods to load and save DICOM specification models to and from JSON files. Inherits logging from SpecStore.</p> Source code in <code>src/dcmspec/json_spec_store.py</code> <pre><code>class JSONSpecStore(SpecStore):\n    \"\"\"Model store class for DICOM specification models storage in JSON format.\n\n    Provides methods to load and save DICOM specification models to and from JSON files.\n    Inherits logging from SpecStore.\n    \"\"\"\n\n    def load(self, path: str) -&gt; SpecModel:\n        \"\"\"Load a specification model from a JSON file.\n\n        Args:\n            path (str): The path to the JSON file to load.\n\n        Returns:\n            SpecModel: The specification model containing both metadata and content nodes.\n\n        Raises:\n            RuntimeError: If the file cannot be read, parsed, or has an invalid structure.\n\n        \"\"\"\n        try:\n            importer = JsonImporter()\n            with open(path, \"r\", encoding=\"utf-8\") as json_file:\n                root = importer.read(json_file)\n\n            # Check that the root node is named \"dcmspec\"\n            if root.name != \"dcmspec\":\n                raise RuntimeError(f\"Invalid model structure in JSON file {path}: root node must be 'dcmspec'.\")\n\n            # Search for metadata and content nodes directly under the root\n            metadata = next((node for node in root.children if node.name == \"metadata\"), None)\n            content = next((node for node in root.children if node.name == \"content\"), None)\n\n            if metadata is None or content is None:\n                raise RuntimeError(\n                    f\"Invalid model structure in JSON file {path}: \"\n                    f\"Both 'metadata' and 'content' nodes must be present as children of 'dcmspec'.\"\n                )\n\n            # Detach the model nodes from the file root node\n            metadata.parent = None\n            content.parent = None\n\n            # Convert keys of column_to_attr back to integers if present in metadata\n            if \"column_to_attr\" in metadata.__dict__:\n                metadata.column_to_attr = {int(k): v for k, v in metadata.column_to_attr.items()}\n\n            return SpecModel(metadata=metadata, content=content)\n        except OSError as e:\n            raise RuntimeError(f\"Failed to read model data from JSON file {path}: {e}\") from e\n        except json.JSONDecodeError as e:\n            raise RuntimeError(f\"Failed to parse JSON file {path}: {e}\") from e\n\n    def save(self, model: SpecModel, path: str) -&gt; None:\n        \"\"\"Save a specification model to a JSON file.\n\n        Args:\n            model (SpecModel): The model object (an instance of SpecModel or a derived class)\n                containing metadata and content nodes to save.\n            path (str): The path to the JSON file to write.\n\n        Returns:\n            None\n\n        Raises:\n            RuntimeError: If the file cannot be written.\n\n        \"\"\"\n        # Create the destination folder if it does not exist\n        dir_name = os.path.dirname(path)\n        if dir_name:\n            os.makedirs(dir_name, exist_ok=True)\n\n        # Create a new root node \"dcmspec\"\n        root_node = Node(\"dcmspec\")\n\n        # Temporarily add the model's metadata and content as children of the new root node\n        model.metadata.parent = root_node\n        model.content.parent = root_node\n\n        try:\n            exporter = JsonExporter(indent=4, sort_keys=False)\n            with open(path, \"w\", encoding=\"utf-8\") as json_file:\n                exporter.write(root_node, json_file)\n            self.logger.info(f\"Attribute model saved as JSON to {path}\")\n\n        except OSError as e:\n            raise RuntimeError(f\"Failed to write JSON file {path}: {e}\") from e\n\n        # Detach the temporary children to leave the model unchanged\n        model.metadata.parent = None\n        model.content.parent = None\n</code></pre>"},{"location":"api/json_spec_store/#dcmspec.json_spec_store.JSONSpecStore.load","title":"<code>load(path)</code>","text":"<p>Load a specification model from a JSON file.</p> PARAMETER DESCRIPTION <code>path</code> <p>The path to the JSON file to load.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>SpecModel</code> <p>The specification model containing both metadata and content nodes.</p> <p> TYPE: <code>SpecModel</code> </p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If the file cannot be read, parsed, or has an invalid structure.</p> Source code in <code>src/dcmspec/json_spec_store.py</code> <pre><code>def load(self, path: str) -&gt; SpecModel:\n    \"\"\"Load a specification model from a JSON file.\n\n    Args:\n        path (str): The path to the JSON file to load.\n\n    Returns:\n        SpecModel: The specification model containing both metadata and content nodes.\n\n    Raises:\n        RuntimeError: If the file cannot be read, parsed, or has an invalid structure.\n\n    \"\"\"\n    try:\n        importer = JsonImporter()\n        with open(path, \"r\", encoding=\"utf-8\") as json_file:\n            root = importer.read(json_file)\n\n        # Check that the root node is named \"dcmspec\"\n        if root.name != \"dcmspec\":\n            raise RuntimeError(f\"Invalid model structure in JSON file {path}: root node must be 'dcmspec'.\")\n\n        # Search for metadata and content nodes directly under the root\n        metadata = next((node for node in root.children if node.name == \"metadata\"), None)\n        content = next((node for node in root.children if node.name == \"content\"), None)\n\n        if metadata is None or content is None:\n            raise RuntimeError(\n                f\"Invalid model structure in JSON file {path}: \"\n                f\"Both 'metadata' and 'content' nodes must be present as children of 'dcmspec'.\"\n            )\n\n        # Detach the model nodes from the file root node\n        metadata.parent = None\n        content.parent = None\n\n        # Convert keys of column_to_attr back to integers if present in metadata\n        if \"column_to_attr\" in metadata.__dict__:\n            metadata.column_to_attr = {int(k): v for k, v in metadata.column_to_attr.items()}\n\n        return SpecModel(metadata=metadata, content=content)\n    except OSError as e:\n        raise RuntimeError(f\"Failed to read model data from JSON file {path}: {e}\") from e\n    except json.JSONDecodeError as e:\n        raise RuntimeError(f\"Failed to parse JSON file {path}: {e}\") from e\n</code></pre>"},{"location":"api/json_spec_store/#dcmspec.json_spec_store.JSONSpecStore.save","title":"<code>save(model, path)</code>","text":"<p>Save a specification model to a JSON file.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model object (an instance of SpecModel or a derived class) containing metadata and content nodes to save.</p> <p> TYPE: <code>SpecModel</code> </p> <code>path</code> <p>The path to the JSON file to write.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>None</code> <p>None</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If the file cannot be written.</p> Source code in <code>src/dcmspec/json_spec_store.py</code> <pre><code>def save(self, model: SpecModel, path: str) -&gt; None:\n    \"\"\"Save a specification model to a JSON file.\n\n    Args:\n        model (SpecModel): The model object (an instance of SpecModel or a derived class)\n            containing metadata and content nodes to save.\n        path (str): The path to the JSON file to write.\n\n    Returns:\n        None\n\n    Raises:\n        RuntimeError: If the file cannot be written.\n\n    \"\"\"\n    # Create the destination folder if it does not exist\n    dir_name = os.path.dirname(path)\n    if dir_name:\n        os.makedirs(dir_name, exist_ok=True)\n\n    # Create a new root node \"dcmspec\"\n    root_node = Node(\"dcmspec\")\n\n    # Temporarily add the model's metadata and content as children of the new root node\n    model.metadata.parent = root_node\n    model.content.parent = root_node\n\n    try:\n        exporter = JsonExporter(indent=4, sort_keys=False)\n        with open(path, \"w\", encoding=\"utf-8\") as json_file:\n            exporter.write(root_node, json_file)\n        self.logger.info(f\"Attribute model saved as JSON to {path}\")\n\n    except OSError as e:\n        raise RuntimeError(f\"Failed to write JSON file {path}: {e}\") from e\n\n    # Detach the temporary children to leave the model unchanged\n    model.metadata.parent = None\n    model.content.parent = None\n</code></pre>"},{"location":"api/service_attribute_defaults/","title":"Service Attribute Defaults","text":""},{"location":"api/service_attribute_defaults/#dcmspec.service_attribute_defaults","title":"<code>dcmspec.service_attribute_defaults</code>","text":"<p>Default name_attr, column_to_attr and DIMSE mappings for DICOM Services attribute specification tables.</p> <ul> <li>UPS:  For PS3.4 CC.2.5.1 Unified Procedure Step.</li> <li>MPPS: For PS3.4 F.7.2.1 Modality Performed Procedure Step.</li> </ul> <p>Each mapping provides a default <code>column_to_attr</code> dictionary, a corresponding DIMSE mapping dictionary and <code>name_attr</code> string for use with ServiceAttributeModel and related classes.</p> Note <p>These mappings are designed to be used together. If you use custom attribute names, you must adapt both mappings accordingly.</p> Note <p>These constants are intended as read-only defaults. If you need to modify a mapping, make a copy first (e.g., <code>UPS_COLUMNS_MAPPING.copy()</code> or <code>copy.deepcopy(UPS_DIMSE_MAPPING)</code>). Modifying the shared constants directly can lead to unexpected behavior elsewhere in your code. Only the dictionary constants (e.g., <code>*_COLUMNS_MAPPING</code>, <code>*_DIMSE_MAPPING</code>) need to be copied. String constants (e.g., <code>*_NAME_ATTR</code>) are immutable and do not need to be copied.</p> <p>Example usage:</p> <pre><code>from dcmspec.service_attribute_defaults import UPS_DIMSE_MAPPING, UPS_COLUMNS_MAPPING, UPS_NAME_ATTR\nfactory = SpecFactory(\n    model_class=ServiceAttributeModel,\n    input_handler=UPSXHTMLDocHandler(),\n    column_to_attr=UPS_COLUMNS_MAPPING.copy(),\n    name_attr=UPS_NAME_ATTR,\n    config=config\n)\nmodel = factory.create_model(\n    url=url,\n    cache_file_name=cache_file_name,\n    table_id=table_id,\n    force_download=False,\n    model_kwargs={\"dimse_mapping\": copy.deepcopy(UPS_DIMSE_MAPPING)},\n    )\n</code></pre>"},{"location":"api/service_attribute_defaults/#dcmspec.service_attribute_defaults.MPPS_COLUMNS_MAPPING","title":"<code>MPPS_COLUMNS_MAPPING = {0: 'elem_name', 1: 'elem_tag', 2: 'dimse_ncreate', 3: 'dimse_nset', 4: 'dimse_final'}</code>  <code>module-attribute</code>","text":"<p>dict: Default column-to-attribute mapping for MPPS attribute tables.</p>"},{"location":"api/service_attribute_defaults/#dcmspec.service_attribute_defaults.MPPS_DIMSE_MAPPING","title":"<code>MPPS_DIMSE_MAPPING = {'ALL_DIMSE': {'attributes': ['dimse_ncreate', 'dimse_nset', 'dimse_final']}, 'N-CREATE': {'attributes': ['dimse_ncreate'], 'req_attributes': ['dimse_ncreate'], 'req_separator': '/'}, 'N-SET': {'attributes': ['dimse_nset'], 'req_attributes': ['dimse_nset'], 'req_separator': '/'}, 'FINAL': {'attributes': ['dimse_final'], 'req_attributes': ['dimse_final']}}</code>  <code>module-attribute</code>","text":"<p>dict: Default DIMSE mapping for MPPS attribute tables.</p>"},{"location":"api/service_attribute_defaults/#dcmspec.service_attribute_defaults.MPPS_NAME_ATTR","title":"<code>MPPS_NAME_ATTR = 'elem_name'</code>  <code>module-attribute</code>","text":"<p>str: Default name_attr for MPPS attribute tables.</p>"},{"location":"api/service_attribute_defaults/#dcmspec.service_attribute_defaults.UPS_COLUMNS_MAPPING","title":"<code>UPS_COLUMNS_MAPPING = {0: 'elem_name', 1: 'elem_tag', 2: 'dimse_ncreate', 3: 'dimse_nset', 4: 'dimse_final', 5: 'dimse_nget', 6: 'key_matching', 7: 'key_return', 8: 'type_remark'}</code>  <code>module-attribute</code>","text":"<p>dict: Default column-to-attribute mapping for UPS attribute tables.</p>"},{"location":"api/service_attribute_defaults/#dcmspec.service_attribute_defaults.UPS_DIMSE_MAPPING","title":"<code>UPS_DIMSE_MAPPING = {'ALL_DIMSE': {'attributes': ['dimse_ncreate', 'dimse_nset', 'dimse_final', 'dimse_nget', 'key_matching', 'key_return', 'type_remark']}, 'N-CREATE': {'attributes': ['dimse_ncreate', 'type_remark'], 'req_attributes': ['dimse_ncreate'], 'req_separator': '/'}, 'N-SET': {'attributes': ['dimse_nset', 'type_remark'], 'req_attributes': ['dimse_nset'], 'req_separator': '/'}, 'N-GET': {'attributes': ['dimse_nget', 'type_remark'], 'req_attributes': ['dimse_nget'], 'req_separator': '/'}, 'C-FIND': {'attributes': ['key_matching', 'key_return', 'type_remark'], 'req_attributes': ['key_matching', 'key_return']}, 'FINAL': {'attributes': ['dimse_final', 'type_remark'], 'req_attributes': ['dimse_final']}}</code>  <code>module-attribute</code>","text":"<p>dict: Default DIMSE mapping for UPS attribute tables.</p>"},{"location":"api/service_attribute_defaults/#dcmspec.service_attribute_defaults.UPS_NAME_ATTR","title":"<code>UPS_NAME_ATTR = 'elem_name'</code>  <code>module-attribute</code>","text":"<p>str: Default name_attr for UPS attribute tables.</p>"},{"location":"api/service_attribute_model/","title":"ServiceAttributeModel","text":""},{"location":"api/service_attribute_model/#dcmspec.service_attribute_model.ServiceAttributeModel","title":"<code>dcmspec.service_attribute_model.ServiceAttributeModel</code>","text":"<p>               Bases: <code>SpecModel</code></p> <p>A model for DICOM Services with mixed DIMSE and role requirements.</p> <p>ServiceAttributeModel extends SpecModel to support selection and filtering of attributes and columns based on DIMSE service and role, using a provided mapping. It enables extraction of service- and role-specific attribute sets from tables where multiple DIMSE Services and Roles are combined.</p> Source code in <code>src/dcmspec/service_attribute_model.py</code> <pre><code>class ServiceAttributeModel(SpecModel):\n    \"\"\"A model for DICOM Services with mixed DIMSE and role requirements.\n\n    ServiceAttributeModel extends SpecModel to support selection and filtering of attributes\n    and columns based on DIMSE service and role, using a provided mapping. It enables\n    extraction of service- and role-specific attribute sets from tables where multiple\n    DIMSE Services and Roles are combined.\n    \"\"\"\n\n    def __init__(\n        self,\n        metadata: Node,\n        content: Node,\n        dimse_mapping: dict,\n        logger: Optional[logging.Logger] = None\n    ) -&gt; None:\n        \"\"\"Initialize the ServiceAttributeModel.\n\n        Sets up the model with metadata, content, and a DIMSE mapping for filtering.\n        Initializes the DIMSE and role selection to None.\n\n        The `dimse_mapping` argument should be a dictionary with the following structure:\n\n        ```python\n        {\n            \"ALL_DIMSE\": {\n                \"attributes\": [&lt;attribute_name&gt;, ...]\n            },\n            \"&lt;DIMSE&gt;\": {\n                \"attributes\": [&lt;attribute_name&gt;, ...],\n                \"req_attributes\": [&lt;attribute_name&gt;, ...],  # optional, for role-based requirements\n                \"req_separator\": \"&lt;separator&gt;\",             # optional, e.g. \"/\"\n            },\n            ...\n        }\n        ```\n\n        Args:\n            metadata (Node): Node holding table and document metadata.\n            content (Node): Node holding the hierarchical content tree of the DICOM specification.\n            dimse_mapping (dict): Dictionary defining DIMSE and role-based attribute requirements.\n            logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n&gt;\n\n        Example:\n            ```python\n            UPS_DIMSE_MAPPING = {\n                \"ALL_DIMSE\": {\n                    \"attributes\": [\n                        \"dimse_ncreate\", \"dimse_nset\", \"dimse_final\", \"dimse_nget\",\n                        \"key_matching\", \"key_return\", \"type_remark\"\n                    ]\n                },\n                \"N-CREATE\": {\n                    \"attributes\": [\"dimse_ncreate\", \"type_remark\"],\n                    \"req_attributes\": [\"dimse_ncreate\"],\n                    \"req_separator\": \"/\"\n                },\n                \"N-SET\": {\n                    \"attributes\": [\"dimse_nset\", \"type_remark\"],\n                    \"req_attributes\": [\"dimse_nset\"],\n                    \"req_separator\": \"/\"\n                },\n                \"N-GET\": {\n                    \"attributes\": [\"dimse_nget\", \"type_remark\"],\n                    \"req_attributes\": [\"dimse_nget\"],\n                    \"req_separator\": \"/\"\n                },\n                \"C-FIND\": {\n                    \"attributes\": [\"key_matching\", \"key_return\", \"type_remark\"],\n                    \"req_attributes\": [\"key_matching\", \"key_return\"]\n                },\n                \"FINAL\": {\n                    \"attributes\": [\"dimse_final\", \"type_remark\"],\n                    \"req_attributes\": [\"dimse_final\"]\n                },\n            }\n            model = ServiceAttributeModel(metadata, content, UPS_DIMSE_MAPPING)\n            ```\n\n        \"\"\"\n        super().__init__(metadata, content, logger=logger)\n        self.DIMSE_MAPPING = dimse_mapping\n        self.dimse = None\n        self.role = None\n\n\n    def select_dimse(self, dimse: str) -&gt; None:\n        \"\"\"Filter the model to only retain attributes relevant to the specified DIMSE SOP Class.\n\n        This method updates the model so that only the attributes required for the selected\n        DIMSE are kept. All other DIMSE-specific attributes are removed from the model,\n        while other attributes not listed in ALL_DIMSE are retained. This enables extraction\n        of a DIMSE-specific attribute set from a combined table. The model's metadata is also\n        updated to reflect the retained attributes.\n\n        Args:\n            dimse (str): The key of DIMSE_MAPPING to select.\n\n        \"\"\"\n        if dimse not in self.DIMSE_MAPPING:\n            self.logger.warning(f\"DIMSE '{dimse}' not found in DIMSE_MAPPING\")\n            return\n        self.dimse = dimse\n\n        dimse_info = self.DIMSE_MAPPING[dimse]\n        all_dimse_info = self.DIMSE_MAPPING[\"ALL_DIMSE\"]\n\n        # Determine which columns/attributes to keep for this DIMSE\n        dimse_attributes = dimse_info.get(\"attributes\", [])\n        all_attributes = all_dimse_info.get(\"attributes\", [])\n\n        self._filter_node_attributes(dimse_attributes, all_attributes)\n        self._update_metadata_for_dimse(dimse_attributes, all_attributes)\n\n\n    def _filter_node_attributes(self, dimse_attributes: Sequence[str], all_attributes: Sequence[str]) -&gt; None:\n        \"\"\"Remove DIMSE attributes that are not belonging to the selected DIMSE.\"\"\"\n        for node in PreOrderIter(self.content):\n            for attr in list(node.__dict__.keys()):\n                # Retaining non-DIMSE attributes (not in ALL_DIMSE)\n                if attr in all_attributes and attr not in dimse_attributes:\n                    delattr(node, attr)\n\n    def _update_metadata_for_dimse(self, dimse_attributes: Sequence[str], all_attributes: Sequence[str]) -&gt; None:\n        if hasattr(self.metadata, \"header\") and hasattr(self.metadata, \"column_to_attr\"):\n            # Build new header and mapping, keeping original indices for column_to_attr\n            new_header = []\n            new_column_to_attr = {}\n            for i, cell in enumerate(self.metadata.header):\n                # Only keep columns that are in the selected DIMSE or not in ALL_DIMSE\n                if i in self.metadata.column_to_attr:\n                    attr = self.metadata.column_to_attr[i]\n                    if (attr in dimse_attributes) or (attr not in all_attributes):\n                        new_header.append(cell)\n                        new_column_to_attr[i] = attr\n            self.metadata.header = new_header\n            self.metadata.column_to_attr = new_column_to_attr\n        elif hasattr(self.metadata, \"column_to_attr\"):\n            # Only update column_to_attr if no header in metadata\n            self.metadata.column_to_attr = {\n                key: value\n                for key, value in self.metadata.column_to_attr.items()\n                if (value in dimse_attributes) or (value not in all_attributes)\n            }\n\n    def select_role(self, role: str) -&gt; None:\n        \"\"\"Filter the model to only retain requirements for a specific role (SCU or SCP) of the selected DIMSE.\n\n        This method updates the model so that, for attributes with role-specific requirements (e.g., \"SCU/SCP\"),\n        only the requirements relevant to the selected role are retained. For example, if a attribute contains\n        \"1/2\", selecting \"SCU\" will keep \"1\" and selecting \"SCP\" will keep \"2\". Any additional comments\n        after a newline are preserved in a separate \"comment\" attribute. The model's metadata is also\n        updated to reflect the changes in attributes.\n\n        Args:\n            role (str): The role to filter for (\"SCU\" or \"SCP\").\n\n        Note:\n            You must call select_dimse() before calling select_role(), or a RuntimeError will be raised.\n\n        Note:\n            For DIMSEs that do not have explicit SCU and SCP requirements (i.e., no \"req_separator\" specified\n            in the mapping), this function may have no effect and will not modify the model.\n\n        Raises:\n            RuntimeError: If select_dimse was not called before select_role.\n\n        \"\"\"\n        if role is None:\n            return\n        if self.dimse is None or self.dimse == \"ALL_DIMSE\":\n            raise RuntimeError(\"select_dimse must be called before select_role.\")\n        self.role = role\n\n        dimse_info = self.DIMSE_MAPPING[self.dimse]\n        req_attributes = dimse_info.get(\"req_attributes\", [])\n        req_separator = dimse_info.get(\"req_separator\", None)\n\n        comment_needed = self._filter_role_attributes(req_attributes, req_separator, role)\n        self._update_metadata_for_role(comment_needed, role)\n\n    def _filter_role_attributes(self, req_attributes: list, req_separator: str, role: str) -&gt; bool:\n        \"\"\"Filter node attributes for the selected role, handle comments, and return if comment column is needed.\"\"\"\n        comment_needed = False\n        for req_attr in req_attributes:\n            attribute_name = req_attr\n            for node in PreOrderIter(self.content):\n                if hasattr(node, attribute_name):\n                    value = getattr(node, attribute_name)\n                    if not isinstance(value, str):\n                        continue\n                    # Split SCU/SCP optionality requirements and any additional comment\n                    parts = value.split(\"\\n\", 1)\n                    optionality = parts[0]\n                    if len(parts) &gt; 1:\n                        setattr(node, attribute_name, optionality)\n                        setattr(node, \"comment\", parts[1])\n                        comment_needed = True\n                    # Split SCU/SCP optionality requirements\n                    if req_separator and req_separator in optionality:\n                        sub_parts = optionality.split(req_separator, 1)\n                        setattr(node, attribute_name, sub_parts[0] if role == \"SCU\" else sub_parts[1])\n        return comment_needed\n\n    def _update_metadata_for_role(self, comment_needed: bool, role: str) -&gt; None:\n        \"\"\"Update metadata (header and column_to_attr) for role-specific requirements and comments.\"\"\"\n        if comment_needed:\n            if hasattr(self.metadata, \"column_to_attr\") and \"comment\" not in self.metadata.column_to_attr.values():\n                next_key = max(self.metadata.column_to_attr.keys(), default=-1) + 1\n                self.metadata.column_to_attr[next_key] = \"comment\"\n            if hasattr(self.metadata, \"header\") and \"Comment\" not in self.metadata.header:\n                self.metadata.header.append(\"Comment\")\n\n        if hasattr(self.metadata, \"header\"):\n            for i, header in enumerate(self.metadata.header):\n                if \"SCU/SCP\" in header:\n                    self.metadata.header[i] = header.replace(\"SCU/SCP\", role)\n</code></pre>"},{"location":"api/service_attribute_model/#dcmspec.service_attribute_model.ServiceAttributeModel.__init__","title":"<code>__init__(metadata, content, dimse_mapping, logger=None)</code>","text":"<p>Initialize the ServiceAttributeModel.</p> <pre><code>    Sets up the model with metadata, content, and a DIMSE mapping for filtering.\n    Initializes the DIMSE and role selection to None.\n\n    The `dimse_mapping` argument should be a dictionary with the following structure:\n\n    ```python\n    {\n        \"ALL_DIMSE\": {\n            \"attributes\": [&lt;attribute_name&gt;, ...]\n        },\n        \"&lt;DIMSE&gt;\": {\n            \"attributes\": [&lt;attribute_name&gt;, ...],\n            \"req_attributes\": [&lt;attribute_name&gt;, ...],  # optional, for role-based requirements\n            \"req_separator\": \"&lt;separator&gt;\",             # optional, e.g. \"/\"\n        },\n        ...\n    }\n    ```\n\n    Args:\n        metadata (Node): Node holding table and document metadata.\n        content (Node): Node holding the hierarchical content tree of the DICOM specification.\n        dimse_mapping (dict): Dictionary defining DIMSE and role-based attribute requirements.\n        logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n</code></pre> <pre><code>    Example:\n        ```python\n        UPS_DIMSE_MAPPING = {\n            \"ALL_DIMSE\": {\n                \"attributes\": [\n                    \"dimse_ncreate\", \"dimse_nset\", \"dimse_final\", \"dimse_nget\",\n                    \"key_matching\", \"key_return\", \"type_remark\"\n                ]\n            },\n            \"N-CREATE\": {\n                \"attributes\": [\"dimse_ncreate\", \"type_remark\"],\n                \"req_attributes\": [\"dimse_ncreate\"],\n                \"req_separator\": \"/\"\n            },\n            \"N-SET\": {\n                \"attributes\": [\"dimse_nset\", \"type_remark\"],\n                \"req_attributes\": [\"dimse_nset\"],\n                \"req_separator\": \"/\"\n            },\n            \"N-GET\": {\n                \"attributes\": [\"dimse_nget\", \"type_remark\"],\n                \"req_attributes\": [\"dimse_nget\"],\n                \"req_separator\": \"/\"\n            },\n            \"C-FIND\": {\n                \"attributes\": [\"key_matching\", \"key_return\", \"type_remark\"],\n                \"req_attributes\": [\"key_matching\", \"key_return\"]\n            },\n            \"FINAL\": {\n                \"attributes\": [\"dimse_final\", \"type_remark\"],\n                \"req_attributes\": [\"dimse_final\"]\n            },\n        }\n        model = ServiceAttributeModel(metadata, content, UPS_DIMSE_MAPPING)\n        ```\n</code></pre> Source code in <code>src/dcmspec/service_attribute_model.py</code> <pre><code>    def __init__(\n        self,\n        metadata: Node,\n        content: Node,\n        dimse_mapping: dict,\n        logger: Optional[logging.Logger] = None\n    ) -&gt; None:\n        \"\"\"Initialize the ServiceAttributeModel.\n\n        Sets up the model with metadata, content, and a DIMSE mapping for filtering.\n        Initializes the DIMSE and role selection to None.\n\n        The `dimse_mapping` argument should be a dictionary with the following structure:\n\n        ```python\n        {\n            \"ALL_DIMSE\": {\n                \"attributes\": [&lt;attribute_name&gt;, ...]\n            },\n            \"&lt;DIMSE&gt;\": {\n                \"attributes\": [&lt;attribute_name&gt;, ...],\n                \"req_attributes\": [&lt;attribute_name&gt;, ...],  # optional, for role-based requirements\n                \"req_separator\": \"&lt;separator&gt;\",             # optional, e.g. \"/\"\n            },\n            ...\n        }\n        ```\n\n        Args:\n            metadata (Node): Node holding table and document metadata.\n            content (Node): Node holding the hierarchical content tree of the DICOM specification.\n            dimse_mapping (dict): Dictionary defining DIMSE and role-based attribute requirements.\n            logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n&gt;\n\n        Example:\n            ```python\n            UPS_DIMSE_MAPPING = {\n                \"ALL_DIMSE\": {\n                    \"attributes\": [\n                        \"dimse_ncreate\", \"dimse_nset\", \"dimse_final\", \"dimse_nget\",\n                        \"key_matching\", \"key_return\", \"type_remark\"\n                    ]\n                },\n                \"N-CREATE\": {\n                    \"attributes\": [\"dimse_ncreate\", \"type_remark\"],\n                    \"req_attributes\": [\"dimse_ncreate\"],\n                    \"req_separator\": \"/\"\n                },\n                \"N-SET\": {\n                    \"attributes\": [\"dimse_nset\", \"type_remark\"],\n                    \"req_attributes\": [\"dimse_nset\"],\n                    \"req_separator\": \"/\"\n                },\n                \"N-GET\": {\n                    \"attributes\": [\"dimse_nget\", \"type_remark\"],\n                    \"req_attributes\": [\"dimse_nget\"],\n                    \"req_separator\": \"/\"\n                },\n                \"C-FIND\": {\n                    \"attributes\": [\"key_matching\", \"key_return\", \"type_remark\"],\n                    \"req_attributes\": [\"key_matching\", \"key_return\"]\n                },\n                \"FINAL\": {\n                    \"attributes\": [\"dimse_final\", \"type_remark\"],\n                    \"req_attributes\": [\"dimse_final\"]\n                },\n            }\n            model = ServiceAttributeModel(metadata, content, UPS_DIMSE_MAPPING)\n            ```\n\n        \"\"\"\n        super().__init__(metadata, content, logger=logger)\n        self.DIMSE_MAPPING = dimse_mapping\n        self.dimse = None\n        self.role = None\n</code></pre>"},{"location":"api/service_attribute_model/#dcmspec.service_attribute_model.ServiceAttributeModel.select_dimse","title":"<code>select_dimse(dimse)</code>","text":"<p>Filter the model to only retain attributes relevant to the specified DIMSE SOP Class.</p> <p>This method updates the model so that only the attributes required for the selected DIMSE are kept. All other DIMSE-specific attributes are removed from the model, while other attributes not listed in ALL_DIMSE are retained. This enables extraction of a DIMSE-specific attribute set from a combined table. The model's metadata is also updated to reflect the retained attributes.</p> PARAMETER DESCRIPTION <code>dimse</code> <p>The key of DIMSE_MAPPING to select.</p> <p> TYPE: <code>str</code> </p> Source code in <code>src/dcmspec/service_attribute_model.py</code> <pre><code>def select_dimse(self, dimse: str) -&gt; None:\n    \"\"\"Filter the model to only retain attributes relevant to the specified DIMSE SOP Class.\n\n    This method updates the model so that only the attributes required for the selected\n    DIMSE are kept. All other DIMSE-specific attributes are removed from the model,\n    while other attributes not listed in ALL_DIMSE are retained. This enables extraction\n    of a DIMSE-specific attribute set from a combined table. The model's metadata is also\n    updated to reflect the retained attributes.\n\n    Args:\n        dimse (str): The key of DIMSE_MAPPING to select.\n\n    \"\"\"\n    if dimse not in self.DIMSE_MAPPING:\n        self.logger.warning(f\"DIMSE '{dimse}' not found in DIMSE_MAPPING\")\n        return\n    self.dimse = dimse\n\n    dimse_info = self.DIMSE_MAPPING[dimse]\n    all_dimse_info = self.DIMSE_MAPPING[\"ALL_DIMSE\"]\n\n    # Determine which columns/attributes to keep for this DIMSE\n    dimse_attributes = dimse_info.get(\"attributes\", [])\n    all_attributes = all_dimse_info.get(\"attributes\", [])\n\n    self._filter_node_attributes(dimse_attributes, all_attributes)\n    self._update_metadata_for_dimse(dimse_attributes, all_attributes)\n</code></pre>"},{"location":"api/service_attribute_model/#dcmspec.service_attribute_model.ServiceAttributeModel.select_role","title":"<code>select_role(role)</code>","text":"<p>Filter the model to only retain requirements for a specific role (SCU or SCP) of the selected DIMSE.</p> <p>This method updates the model so that, for attributes with role-specific requirements (e.g., \"SCU/SCP\"), only the requirements relevant to the selected role are retained. For example, if a attribute contains \"1/2\", selecting \"SCU\" will keep \"1\" and selecting \"SCP\" will keep \"2\". Any additional comments after a newline are preserved in a separate \"comment\" attribute. The model's metadata is also updated to reflect the changes in attributes.</p> PARAMETER DESCRIPTION <code>role</code> <p>The role to filter for (\"SCU\" or \"SCP\").</p> <p> TYPE: <code>str</code> </p> Note <p>You must call select_dimse() before calling select_role(), or a RuntimeError will be raised.</p> Note <p>For DIMSEs that do not have explicit SCU and SCP requirements (i.e., no \"req_separator\" specified in the mapping), this function may have no effect and will not modify the model.</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If select_dimse was not called before select_role.</p> Source code in <code>src/dcmspec/service_attribute_model.py</code> <pre><code>def select_role(self, role: str) -&gt; None:\n    \"\"\"Filter the model to only retain requirements for a specific role (SCU or SCP) of the selected DIMSE.\n\n    This method updates the model so that, for attributes with role-specific requirements (e.g., \"SCU/SCP\"),\n    only the requirements relevant to the selected role are retained. For example, if a attribute contains\n    \"1/2\", selecting \"SCU\" will keep \"1\" and selecting \"SCP\" will keep \"2\". Any additional comments\n    after a newline are preserved in a separate \"comment\" attribute. The model's metadata is also\n    updated to reflect the changes in attributes.\n\n    Args:\n        role (str): The role to filter for (\"SCU\" or \"SCP\").\n\n    Note:\n        You must call select_dimse() before calling select_role(), or a RuntimeError will be raised.\n\n    Note:\n        For DIMSEs that do not have explicit SCU and SCP requirements (i.e., no \"req_separator\" specified\n        in the mapping), this function may have no effect and will not modify the model.\n\n    Raises:\n        RuntimeError: If select_dimse was not called before select_role.\n\n    \"\"\"\n    if role is None:\n        return\n    if self.dimse is None or self.dimse == \"ALL_DIMSE\":\n        raise RuntimeError(\"select_dimse must be called before select_role.\")\n    self.role = role\n\n    dimse_info = self.DIMSE_MAPPING[self.dimse]\n    req_attributes = dimse_info.get(\"req_attributes\", [])\n    req_separator = dimse_info.get(\"req_separator\", None)\n\n    comment_needed = self._filter_role_attributes(req_attributes, req_separator, role)\n    self._update_metadata_for_role(comment_needed, role)\n</code></pre>"},{"location":"api/spec_factory/","title":"SpecFactory","text":""},{"location":"api/spec_factory/#dcmspec.spec_factory.SpecFactory","title":"<code>dcmspec.spec_factory.SpecFactory</code>","text":"<p>Factory for DICOM specification models.</p> <p>Coordinates the downloading, parsing, and caching of DICOM specification tables. Uses input handlers, table parsers, and model stores to produce SpecModel objects from URLs or cached files. Supports flexible configuration and caching strategies.</p> Typical usage <p>factory = SpecFactory(...) model = factory.create_model(...)</p> Source code in <code>src/dcmspec/spec_factory.py</code> <pre><code>class SpecFactory:\n    \"\"\"Factory for DICOM specification models.\n\n    Coordinates the downloading, parsing, and caching of DICOM specification tables.\n    Uses input handlers, table parsers, and model stores to produce SpecModel objects\n    from URLs or cached files. Supports flexible configuration and caching strategies.\n\n    Typical usage:\n        factory = SpecFactory(...)\n        model = factory.create_model(...)\n    \"\"\"\n\n    def __init__(\n        self,\n        input_handler: Optional[DocHandler] = None,\n        model_class: Optional[Type[SpecModel]] = None,\n        model_store: Optional[SpecStore] = None,\n        table_parser: Optional[SpecParser] = None,\n        column_to_attr: Optional[Dict[int, str]] = None,\n        name_attr: Optional[str] = None,\n        config: Optional[Config] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        \"\"\"Initialize the SpecFactory.\n\n        The default values for `column_to_attr` and `name_attr` are designed for parsing\n        DICOM PS3.3 module attribute tables, where columns typically represent element name,\n        tag, type, and description.\n\n        Args:\n            input_handler (Optional[DocHandler]): Handler for downloading and parsing input files.\n                If None, a default XHTMLDocHandler is used.\n            model_class (Optional[Type[SpecModel]]): The class to instantiate for the model.\n                If None, defaults to SpecModel.\n            model_store (Optional[SpecStore]): Store for loading and saving models.\n                If None, a default JSONSpecStore is used.\n            table_parser (Optional[SpecParser]): Parser for extracting tables from documents.\n                If None, a default DOMTableSpecParser is used.\n            column_to_attr (Optional[Dict[int, str]]): Mapping from column indices to names of attributes\n                of model nodes. If None, a default mapping is used.\n            name_attr (Optional[str]): Attribute name to use for node names in the model.\n                If None, defaults to \"elem_name\".\n            config (Optional[Config]): Configuration object. If None, a default Config is created.\n            logger (Optional[logging.Logger]): Logger instance to use.\n                If None, a default logger is created.\n\n        Raises:\n            TypeError: If config is not a Config instance or None.\n\n        \"\"\"\n        import logging\n        if config is not None and not isinstance(config, Config):\n            raise TypeError(\"config must be an instance of Config or None\")\n        self.config = config or Config()\n\n        self.logger = logger or logging.getLogger(self.__class__.__name__)\n        self.model_class = model_class or SpecModel\n        self.input_handler = input_handler or XHTMLDocHandler(config=self.config, logger=self.logger)\n        self.model_store = model_store or JSONSpecStore(logger=self.logger)\n        self.table_parser = table_parser or DOMTableSpecParser(logger=self.logger)\n        self.column_to_attr = column_to_attr or {0: \"elem_name\", 1: \"elem_tag\", 2: \"elem_type\", 3: \"elem_description\"}\n        self.name_attr = name_attr or \"elem_name\"\n\n    def load_dom(self, url: str, cache_file_name: str, force_download: bool = False) -&gt; BeautifulSoup:\n        \"\"\"Download, cache, and parse the specification file from a URL, returning the DOM.\n\n        Args:\n            url (str): The URL to download the input file from.\n            cache_file_name (str): Filename of the cached input file.\n            force_download (bool): If True, always download the input file even if cached.\n\n        Returns:\n            BeautifulSoup: The parsed DOM.\n\n        \"\"\"\n        # This will download if needed and always parse/return the DOM\n        return self.input_handler.get_dom(cache_file_name=cache_file_name, url=url, force_download=force_download)\n\n    def try_load_cache(\n        self,\n        json_file_name: Optional[str],\n        include_depth: Optional[int],\n        model_kwargs: Optional[Dict[str, Any]],\n        force_parse: bool = False,\n    ) -&gt; Optional[SpecModel]:\n        \"\"\"Check for and load a model from cache if available and not force_parse.\"\"\"\n        if json_file_name is None:\n            cache_file_name = getattr(self.input_handler, \"cache_file_name\", None)\n            if cache_file_name is None:\n                raise ValueError(\"input_handler.cache_file_name not set\")\n            json_file_name = f\"{os.path.splitext(cache_file_name)[0]}.json\"\n        json_file_path = os.path.join(self.config.get_param(\"cache_dir\"), \"model\", json_file_name)\n        if os.path.exists(json_file_path) and not force_parse:\n            model = self._load_model_from_cache(json_file_path, include_depth, model_kwargs)\n            if model is not None:\n                return model\n        return None\n\n    def build_model(\n        self,\n        dom: BeautifulSoup,\n        table_id: Optional[str] = None,\n        url: Optional[str] = None,\n        json_file_name: Optional[str] = None,\n        include_depth: Optional[int] = None,\n        force_parse: bool = False,\n        model_kwargs: Optional[Dict[str, Any]] = None,\n    ) -&gt; SpecModel:\n        \"\"\"Build and cache a DICOM specification model from a parsed DOM.\n\n        Args:\n            dom (BeautifulSoup): The parsed DOM object (e.g., BeautifulSoup).\n            table_id (Optional[str]): Table identifier for model parsing.\n            url (Optional[str]): The URL the DOM was fetched from (for metadata).\n            json_file_name (Optional[str]): Filename to save the cached JSON model.\n            include_depth (Optional[int]): The depth to which included tables should be parsed.\n            force_parse (bool): If True, always parse and (over)write the JSON cache file.\n            model_kwargs (Optional[Dict[str, Any]]): Additional keyword arguments for model construction.\n                Use this to supply extra parameters required by custom SpecModel subclasses.\n                For example, if your model class is `MyModel(metadata, content, foo, bar)`, pass\n                `model_kwargs={\"foo\": foo_value, \"bar\": bar_value}`.\n\n        If `json_file_name` is not provided, the factory will attempt to use\n        `self.input_handler.cache_file_name` to generate a default JSON file name.\n        If neither is set, a ValueError is raised.\n\n        Returns:\n            SpecModel: The constructed model.\n\n        \"\"\"\n        # Try to load from cache first\n        model = self.try_load_cache(json_file_name, include_depth, model_kwargs, force_parse)\n        if model is not None:\n            return model\n\n        # Parse provided DOM otherwise\n        model = self._parse_and_build_model(\n            dom, table_id, url, include_depth, model_kwargs\n        )\n\n        # Cache the newly built model if requested\n        if json_file_name:\n            json_file_path = os.path.join(self.config.get_param(\"cache_dir\"), \"model\", json_file_name)\n            try:\n                self.model_store.save(model, json_file_path)\n            except Exception as e:\n                self.logger.warning(f\"Failed to cache model to {json_file_path}: {e}\")\n\n        return model\n\n    def create_model(\n        self,\n        url: str,\n        cache_file_name: str,\n        table_id: Optional[str] = None,\n        force_parse: bool = False,\n        force_download: bool = False,\n        json_file_name: Optional[str] = None,\n        include_depth: Optional[int] = None,\n        model_kwargs: Optional[Dict[str, Any]] = None,\n    ) -&gt; SpecModel:\n        \"\"\"Integrated, one-step method to fetch, parse, and build a DICOM specification model from a URL.\n\n        Args:\n            url (str): The URL to download the input file from.\n            cache_file_name (str): Filename of the cached input file.\n            table_id (Optional[str]): Table identifier for model parsing.\n            force_parse (bool): If True, always parse the DOM and generate the JSON model, even if cached.\n            force_download (bool): If True, always download the input file and generate the model even if cached.\n                Note: force_download also implies force_parse.\n            json_file_name (Optional[str]): Filename to save the cached JSON model.\n            include_depth (Optional[int]): The depth to which included tables should be parsed.\n            model_kwargs (Optional[Dict[str, Any]]): Additional keyword arguments for model construction.\n                Use this to supply extra parameters required by custom SpecModel subclasses.\n                For example, if your model class is `MyModel(metadata, content, foo, bar)`, pass\n                `model_kwargs={\"foo\": foo_value, \"bar\": bar_value}`.\n\n        Returns:\n            SpecModel: The constructed model.\n\n        \"\"\"\n        # Try to load from cache before loading DOM\n        model = self.try_load_cache(json_file_name, include_depth, model_kwargs, force_parse or force_download)\n        if model is not None:\n            return model\n\n        dom = self.load_dom(url=url, cache_file_name=cache_file_name, force_download=force_download)\n        return self.build_model(\n            dom=dom,\n            table_id=table_id,\n            url=url,\n            json_file_name=json_file_name,\n            include_depth=include_depth,\n            force_parse=force_parse or force_download,\n            model_kwargs=model_kwargs,\n        )\n\n    def _load_model_from_cache(\n        self,\n        json_file_path: str,\n        include_depth: Optional[int],\n        model_kwargs: Optional[Dict[str, Any]],\n    ) -&gt; Optional[SpecModel]:\n        \"\"\"Load model from cache file if include depth is valid.\"\"\"\n        try:\n            # Load the model from cache\n            model = self.model_store.load(json_file_path)\n            self.logger.info(f\"Loaded model from cache {json_file_path}\")\n\n            # Do not use cache if include_depth does not match the cached model's metadata\n            cached_depth = getattr(model.metadata, \"include_depth\", None)\n            if (\n                (include_depth is not None and cached_depth is not None and int(cached_depth) != int(include_depth))\n                or (include_depth is None and cached_depth is not None)\n                or (include_depth is not None and cached_depth is None)\n            ):\n                self.logger.info(\n                    (\n                        f\"Cached model include_depth ({cached_depth}) \"\n                        f\"does not match requested ({include_depth}), reparsing.\"\n                    )\n                )\n                return None\n\n            # Return the cached model, reconstructing it to the required subclass if necessary        \n            if isinstance(model, self.model_class):\n                return model\n            return self.model_class(\n                metadata=model.metadata,\n                content=model.content,\n                **(model_kwargs or {}),\n            )\n\n        except Exception as e:\n            self.logger.warning(f\"Failed to load model from cache {json_file_path}: {e}\")\n            return None\n\n    def _parse_and_build_model(\n        self,\n        dom: BeautifulSoup,\n        table_id: Optional[str],\n        url: Optional[str],\n        include_depth: Optional[int],\n        model_kwargs: Optional[Dict[str, Any]],\n    ) -&gt; SpecModel:\n        \"\"\"Parse and Build model from provided DOM object.\"\"\"\n        # Parse content and some metadata from DOM\n        metadata, content = self.table_parser.parse(\n            dom,\n            table_id=table_id,\n            include_depth=include_depth,\n            column_to_attr=self.column_to_attr,\n            name_attr=self.name_attr,\n        )\n\n        # Add args values to model metadata\n        metadata.url = url\n        metadata.table_id = table_id\n        if include_depth is not None:\n            metadata.include_depth = int(include_depth)\n        metadata.column_to_attr = self.column_to_attr\n        metadata.name_attr = self.name_attr\n\n        # Build the model from parsed content and metadata\n        model = self.model_class(\n            metadata=metadata,\n            content=content,\n            **(model_kwargs or {}),\n        )\n\n        # Clean up model from title nodes\n        model.exclude_titles()\n\n        return model\n</code></pre>"},{"location":"api/spec_factory/#dcmspec.spec_factory.SpecFactory.__init__","title":"<code>__init__(input_handler=None, model_class=None, model_store=None, table_parser=None, column_to_attr=None, name_attr=None, config=None, logger=None)</code>","text":"<p>Initialize the SpecFactory.</p> <p>The default values for <code>column_to_attr</code> and <code>name_attr</code> are designed for parsing DICOM PS3.3 module attribute tables, where columns typically represent element name, tag, type, and description.</p> PARAMETER DESCRIPTION <code>input_handler</code> <p>Handler for downloading and parsing input files. If None, a default XHTMLDocHandler is used.</p> <p> TYPE: <code>Optional[DocHandler]</code> DEFAULT: <code>None</code> </p> <code>model_class</code> <p>The class to instantiate for the model. If None, defaults to SpecModel.</p> <p> TYPE: <code>Optional[Type[SpecModel]]</code> DEFAULT: <code>None</code> </p> <code>model_store</code> <p>Store for loading and saving models. If None, a default JSONSpecStore is used.</p> <p> TYPE: <code>Optional[SpecStore]</code> DEFAULT: <code>None</code> </p> <code>table_parser</code> <p>Parser for extracting tables from documents. If None, a default DOMTableSpecParser is used.</p> <p> TYPE: <code>Optional[SpecParser]</code> DEFAULT: <code>None</code> </p> <code>column_to_attr</code> <p>Mapping from column indices to names of attributes of model nodes. If None, a default mapping is used.</p> <p> TYPE: <code>Optional[Dict[int, str]]</code> DEFAULT: <code>None</code> </p> <code>name_attr</code> <p>Attribute name to use for node names in the model. If None, defaults to \"elem_name\".</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>Configuration object. If None, a default Config is created.</p> <p> TYPE: <code>Optional[Config]</code> DEFAULT: <code>None</code> </p> <code>logger</code> <p>Logger instance to use. If None, a default logger is created.</p> <p> TYPE: <code>Optional[Logger]</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>If config is not a Config instance or None.</p> Source code in <code>src/dcmspec/spec_factory.py</code> <pre><code>def __init__(\n    self,\n    input_handler: Optional[DocHandler] = None,\n    model_class: Optional[Type[SpecModel]] = None,\n    model_store: Optional[SpecStore] = None,\n    table_parser: Optional[SpecParser] = None,\n    column_to_attr: Optional[Dict[int, str]] = None,\n    name_attr: Optional[str] = None,\n    config: Optional[Config] = None,\n    logger: Optional[logging.Logger] = None,\n):\n    \"\"\"Initialize the SpecFactory.\n\n    The default values for `column_to_attr` and `name_attr` are designed for parsing\n    DICOM PS3.3 module attribute tables, where columns typically represent element name,\n    tag, type, and description.\n\n    Args:\n        input_handler (Optional[DocHandler]): Handler for downloading and parsing input files.\n            If None, a default XHTMLDocHandler is used.\n        model_class (Optional[Type[SpecModel]]): The class to instantiate for the model.\n            If None, defaults to SpecModel.\n        model_store (Optional[SpecStore]): Store for loading and saving models.\n            If None, a default JSONSpecStore is used.\n        table_parser (Optional[SpecParser]): Parser for extracting tables from documents.\n            If None, a default DOMTableSpecParser is used.\n        column_to_attr (Optional[Dict[int, str]]): Mapping from column indices to names of attributes\n            of model nodes. If None, a default mapping is used.\n        name_attr (Optional[str]): Attribute name to use for node names in the model.\n            If None, defaults to \"elem_name\".\n        config (Optional[Config]): Configuration object. If None, a default Config is created.\n        logger (Optional[logging.Logger]): Logger instance to use.\n            If None, a default logger is created.\n\n    Raises:\n        TypeError: If config is not a Config instance or None.\n\n    \"\"\"\n    import logging\n    if config is not None and not isinstance(config, Config):\n        raise TypeError(\"config must be an instance of Config or None\")\n    self.config = config or Config()\n\n    self.logger = logger or logging.getLogger(self.__class__.__name__)\n    self.model_class = model_class or SpecModel\n    self.input_handler = input_handler or XHTMLDocHandler(config=self.config, logger=self.logger)\n    self.model_store = model_store or JSONSpecStore(logger=self.logger)\n    self.table_parser = table_parser or DOMTableSpecParser(logger=self.logger)\n    self.column_to_attr = column_to_attr or {0: \"elem_name\", 1: \"elem_tag\", 2: \"elem_type\", 3: \"elem_description\"}\n    self.name_attr = name_attr or \"elem_name\"\n</code></pre>"},{"location":"api/spec_factory/#dcmspec.spec_factory.SpecFactory.build_model","title":"<code>build_model(dom, table_id=None, url=None, json_file_name=None, include_depth=None, force_parse=False, model_kwargs=None)</code>","text":"<p>Build and cache a DICOM specification model from a parsed DOM.</p> PARAMETER DESCRIPTION <code>dom</code> <p>The parsed DOM object (e.g., BeautifulSoup).</p> <p> TYPE: <code>BeautifulSoup</code> </p> <code>table_id</code> <p>Table identifier for model parsing.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>url</code> <p>The URL the DOM was fetched from (for metadata).</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>json_file_name</code> <p>Filename to save the cached JSON model.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>include_depth</code> <p>The depth to which included tables should be parsed.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>force_parse</code> <p>If True, always parse and (over)write the JSON cache file.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>model_kwargs</code> <p>Additional keyword arguments for model construction. Use this to supply extra parameters required by custom SpecModel subclasses. For example, if your model class is <code>MyModel(metadata, content, foo, bar)</code>, pass <code>model_kwargs={\"foo\": foo_value, \"bar\": bar_value}</code>.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> <p>If <code>json_file_name</code> is not provided, the factory will attempt to use <code>self.input_handler.cache_file_name</code> to generate a default JSON file name. If neither is set, a ValueError is raised.</p> RETURNS DESCRIPTION <code>SpecModel</code> <p>The constructed model.</p> <p> TYPE: <code>SpecModel</code> </p> Source code in <code>src/dcmspec/spec_factory.py</code> <pre><code>def build_model(\n    self,\n    dom: BeautifulSoup,\n    table_id: Optional[str] = None,\n    url: Optional[str] = None,\n    json_file_name: Optional[str] = None,\n    include_depth: Optional[int] = None,\n    force_parse: bool = False,\n    model_kwargs: Optional[Dict[str, Any]] = None,\n) -&gt; SpecModel:\n    \"\"\"Build and cache a DICOM specification model from a parsed DOM.\n\n    Args:\n        dom (BeautifulSoup): The parsed DOM object (e.g., BeautifulSoup).\n        table_id (Optional[str]): Table identifier for model parsing.\n        url (Optional[str]): The URL the DOM was fetched from (for metadata).\n        json_file_name (Optional[str]): Filename to save the cached JSON model.\n        include_depth (Optional[int]): The depth to which included tables should be parsed.\n        force_parse (bool): If True, always parse and (over)write the JSON cache file.\n        model_kwargs (Optional[Dict[str, Any]]): Additional keyword arguments for model construction.\n            Use this to supply extra parameters required by custom SpecModel subclasses.\n            For example, if your model class is `MyModel(metadata, content, foo, bar)`, pass\n            `model_kwargs={\"foo\": foo_value, \"bar\": bar_value}`.\n\n    If `json_file_name` is not provided, the factory will attempt to use\n    `self.input_handler.cache_file_name` to generate a default JSON file name.\n    If neither is set, a ValueError is raised.\n\n    Returns:\n        SpecModel: The constructed model.\n\n    \"\"\"\n    # Try to load from cache first\n    model = self.try_load_cache(json_file_name, include_depth, model_kwargs, force_parse)\n    if model is not None:\n        return model\n\n    # Parse provided DOM otherwise\n    model = self._parse_and_build_model(\n        dom, table_id, url, include_depth, model_kwargs\n    )\n\n    # Cache the newly built model if requested\n    if json_file_name:\n        json_file_path = os.path.join(self.config.get_param(\"cache_dir\"), \"model\", json_file_name)\n        try:\n            self.model_store.save(model, json_file_path)\n        except Exception as e:\n            self.logger.warning(f\"Failed to cache model to {json_file_path}: {e}\")\n\n    return model\n</code></pre>"},{"location":"api/spec_factory/#dcmspec.spec_factory.SpecFactory.create_model","title":"<code>create_model(url, cache_file_name, table_id=None, force_parse=False, force_download=False, json_file_name=None, include_depth=None, model_kwargs=None)</code>","text":"<p>Integrated, one-step method to fetch, parse, and build a DICOM specification model from a URL.</p> PARAMETER DESCRIPTION <code>url</code> <p>The URL to download the input file from.</p> <p> TYPE: <code>str</code> </p> <code>cache_file_name</code> <p>Filename of the cached input file.</p> <p> TYPE: <code>str</code> </p> <code>table_id</code> <p>Table identifier for model parsing.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>force_parse</code> <p>If True, always parse the DOM and generate the JSON model, even if cached.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>force_download</code> <p>If True, always download the input file and generate the model even if cached. Note: force_download also implies force_parse.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>json_file_name</code> <p>Filename to save the cached JSON model.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>include_depth</code> <p>The depth to which included tables should be parsed.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>model_kwargs</code> <p>Additional keyword arguments for model construction. Use this to supply extra parameters required by custom SpecModel subclasses. For example, if your model class is <code>MyModel(metadata, content, foo, bar)</code>, pass <code>model_kwargs={\"foo\": foo_value, \"bar\": bar_value}</code>.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>SpecModel</code> <p>The constructed model.</p> <p> TYPE: <code>SpecModel</code> </p> Source code in <code>src/dcmspec/spec_factory.py</code> <pre><code>def create_model(\n    self,\n    url: str,\n    cache_file_name: str,\n    table_id: Optional[str] = None,\n    force_parse: bool = False,\n    force_download: bool = False,\n    json_file_name: Optional[str] = None,\n    include_depth: Optional[int] = None,\n    model_kwargs: Optional[Dict[str, Any]] = None,\n) -&gt; SpecModel:\n    \"\"\"Integrated, one-step method to fetch, parse, and build a DICOM specification model from a URL.\n\n    Args:\n        url (str): The URL to download the input file from.\n        cache_file_name (str): Filename of the cached input file.\n        table_id (Optional[str]): Table identifier for model parsing.\n        force_parse (bool): If True, always parse the DOM and generate the JSON model, even if cached.\n        force_download (bool): If True, always download the input file and generate the model even if cached.\n            Note: force_download also implies force_parse.\n        json_file_name (Optional[str]): Filename to save the cached JSON model.\n        include_depth (Optional[int]): The depth to which included tables should be parsed.\n        model_kwargs (Optional[Dict[str, Any]]): Additional keyword arguments for model construction.\n            Use this to supply extra parameters required by custom SpecModel subclasses.\n            For example, if your model class is `MyModel(metadata, content, foo, bar)`, pass\n            `model_kwargs={\"foo\": foo_value, \"bar\": bar_value}`.\n\n    Returns:\n        SpecModel: The constructed model.\n\n    \"\"\"\n    # Try to load from cache before loading DOM\n    model = self.try_load_cache(json_file_name, include_depth, model_kwargs, force_parse or force_download)\n    if model is not None:\n        return model\n\n    dom = self.load_dom(url=url, cache_file_name=cache_file_name, force_download=force_download)\n    return self.build_model(\n        dom=dom,\n        table_id=table_id,\n        url=url,\n        json_file_name=json_file_name,\n        include_depth=include_depth,\n        force_parse=force_parse or force_download,\n        model_kwargs=model_kwargs,\n    )\n</code></pre>"},{"location":"api/spec_factory/#dcmspec.spec_factory.SpecFactory.load_dom","title":"<code>load_dom(url, cache_file_name, force_download=False)</code>","text":"<p>Download, cache, and parse the specification file from a URL, returning the DOM.</p> PARAMETER DESCRIPTION <code>url</code> <p>The URL to download the input file from.</p> <p> TYPE: <code>str</code> </p> <code>cache_file_name</code> <p>Filename of the cached input file.</p> <p> TYPE: <code>str</code> </p> <code>force_download</code> <p>If True, always download the input file even if cached.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>BeautifulSoup</code> <p>The parsed DOM.</p> <p> TYPE: <code>BeautifulSoup</code> </p> Source code in <code>src/dcmspec/spec_factory.py</code> <pre><code>def load_dom(self, url: str, cache_file_name: str, force_download: bool = False) -&gt; BeautifulSoup:\n    \"\"\"Download, cache, and parse the specification file from a URL, returning the DOM.\n\n    Args:\n        url (str): The URL to download the input file from.\n        cache_file_name (str): Filename of the cached input file.\n        force_download (bool): If True, always download the input file even if cached.\n\n    Returns:\n        BeautifulSoup: The parsed DOM.\n\n    \"\"\"\n    # This will download if needed and always parse/return the DOM\n    return self.input_handler.get_dom(cache_file_name=cache_file_name, url=url, force_download=force_download)\n</code></pre>"},{"location":"api/spec_factory/#dcmspec.spec_factory.SpecFactory.try_load_cache","title":"<code>try_load_cache(json_file_name, include_depth, model_kwargs, force_parse=False)</code>","text":"<p>Check for and load a model from cache if available and not force_parse.</p> Source code in <code>src/dcmspec/spec_factory.py</code> <pre><code>def try_load_cache(\n    self,\n    json_file_name: Optional[str],\n    include_depth: Optional[int],\n    model_kwargs: Optional[Dict[str, Any]],\n    force_parse: bool = False,\n) -&gt; Optional[SpecModel]:\n    \"\"\"Check for and load a model from cache if available and not force_parse.\"\"\"\n    if json_file_name is None:\n        cache_file_name = getattr(self.input_handler, \"cache_file_name\", None)\n        if cache_file_name is None:\n            raise ValueError(\"input_handler.cache_file_name not set\")\n        json_file_name = f\"{os.path.splitext(cache_file_name)[0]}.json\"\n    json_file_path = os.path.join(self.config.get_param(\"cache_dir\"), \"model\", json_file_name)\n    if os.path.exists(json_file_path) and not force_parse:\n        model = self._load_model_from_cache(json_file_path, include_depth, model_kwargs)\n        if model is not None:\n            return model\n    return None\n</code></pre>"},{"location":"api/spec_merger/","title":"SpecMerger","text":""},{"location":"api/spec_merger/#dcmspec.spec_merger.SpecMerger","title":"<code>dcmspec.spec_merger.SpecMerger</code>","text":"<p>Merges multiple DICOM specification models.</p> <p>The SpecMerger class provides methods to combine and enrich DICOM SpecModel objects, supporting both path-based and node-based merging strategies. This is useful for workflows where you need to sequentially merge two or more models, such as enriching PS3.3 module attributes models with definitions from the PS3.6 data elements dictionary, or combining a PS3.3 specification with a PS3.4 SOP class and then enriching with an  IHE profile specification.</p> Source code in <code>src/dcmspec/spec_merger.py</code> <pre><code>class SpecMerger:\n    \"\"\"Merges multiple DICOM specification models.\n\n    The SpecMerger class provides methods to combine and enrich DICOM SpecModel objects,\n    supporting both path-based and node-based merging strategies. This is useful for\n    workflows where you need to sequentially merge two or more models, such as enriching\n    PS3.3 module attributes models with definitions from the PS3.6 data elements dictionary,\n    or combining a PS3.3 specification with a PS3.4 SOP class and then enriching with an \n    IHE profile specification.\n    \"\"\"\n\n    def __init__(self, config: Config = None, model_store: SpecStore = None, logger: logging.Logger = None):\n        \"\"\"Initialize the SpecMerger.\n\n        Sets up the logger for the merger. If no logger is provided, a default logger is created.\n        If no model_store is provided, defaults to JSONSpecStore.\n\n        Args:\n            config (Optional[Config]): Configuration object. If None, a default Config is created.\n            model_store (Optional[SpecStore]): Store for loading and saving models. Defaults to JSONSpecStore.\n            logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n        \"\"\"\n        self.logger = logger or logging.getLogger(self.__class__.__name__)\n        self.config = config or Config()\n        self.model_store = model_store or JSONSpecStore(logger=self.logger)\n\n    def merge_node(\n        self,\n        model1: SpecModel,\n        model2: SpecModel,\n        match_by: str = \"name\",\n        attribute_name: str = None,\n        merge_attrs: list[str] = None,\n        json_file_name: str = None,\n        force_update: bool = False,\n    ) -&gt; SpecModel:\n        \"\"\"Merge two DICOM SpecModel objects using the node merge method, with optional caching.\n\n        This is a convenience method that calls merge_many with two models.\n\n        Args:\n            model1 (SpecModel): The first model.\n            model2 (SpecModel): The second model to merge with the first.\n            match_by (str, optional): \"name\" to match by node name, \"attribute\" to match by a specific attribute.\n            attribute_name (str, optional): The attribute name to use for matching.\n            merge_attrs (list[str], optional): List of attribute names to merge from the other model's node.\n            json_file_name (str, optional): If provided, cache/load the merged model to/from this file.\n            force_update (bool, optional): If True, always perform the merge and overwrite the cache.\n\n        Returns:\n            SpecModel: The merged SpecModel instance.\n\n        \"\"\"\n        return self.merge_many(\n            [model1, model2],\n            method = \"matching_node\",\n            match_by=match_by,\n            attribute_names=[attribute_name],\n            merge_attrs_list=[merge_attrs],\n            json_file_name=json_file_name,\n            force_update=force_update,\n        )\n\n    def merge_path(\n        self,\n        model1: SpecModel,\n        model2: SpecModel,\n        match_by: str = \"name\",\n        attribute_name: str = None,\n        merge_attrs: list[str] = None,\n        json_file_name: str = None,\n        force_update: bool = False,\n    ) -&gt; SpecModel:\n        \"\"\"Merge two DICOM SpecModel objects using the path merge method, with optional caching.\n\n        This is a convenience method that calls merge_many with two models.\n\n        Args:\n            model1 (SpecModel): The first model.\n            model2 (SpecModel): The second model to merge with the first.\n            match_by (str, optional): \"name\" to match by node name, \"attribute\" to match by a specific attribute.\n            attribute_name (str, optional): The attribute name to use for matching.\n            merge_attrs (list[str], optional): List of attribute names to merge from the other model's node.\n            json_file_name (str, optional): If provided, cache/load the merged model to/from this file.\n            force_update (bool, optional): If True, always perform the merge and overwrite the cache.\n\n        Returns:\n            SpecModel: The merged SpecModel instance.\n\n        \"\"\"\n        return self.merge_many(\n            [model1, model2],\n            method = \"matching_path\",\n            match_by=match_by,\n            attribute_names=[attribute_name],\n            merge_attrs_list=[merge_attrs],\n            json_file_name=json_file_name,\n            force_update=force_update,\n        )\n\n    def merge_many(\n        self,\n        models: list[SpecModel],\n        method: str = \"matching_path\",\n        match_by: str = \"name\",\n        attribute_names: list = None,\n        merge_attrs_list: list = None,\n        json_file_name: str = None,\n        force_update: bool = False,\n    ) -&gt; SpecModel:\n        \"\"\"Merge a sequence of DICOM SpecModel objects using the specified merge method, with optional caching.\n\n        This method merges a list of models in order, applying either path-based or node-based\n        merging at each step. You can specify different attribute names and lists of attributes\n        to merge for each step, allowing for flexible, multi-stage enrichment of DICOM models.\n        If json_file_name is provided, the merged model will be cached to that file, and loaded from\n        cache if available and force_update is False.\n\n        Args:\n            models (list of SpecModel): The models to merge, in order.\n            method (str): Merge method to use (\"matching_path\" or \"matching_node\").\n            match_by (str, optional): \"name\" to match by node name, \"attribute\" to match by a specific attribute.\n            attribute_names (list, optional): List of attribute names to use for each merge step.\n                Each entry corresponds to a merge operation between two models.\n            merge_attrs_list (list, optional): List of lists of attribute names to merge for each merge step.\n                Each entry corresponds to a merge operation between two models.\n            json_file_name (str, optional): If provided, cache/load the merged model to/from this file.\n            force_update (bool, optional): If True, always perform the merge and overwrite the cache.\n\n        Returns:\n            SpecModel: The final merged SpecModel instance.\n\n        Raises:\n            ValueError: If models is empty, method is unknown, or attribute_names/merge_attrs_list\n                have incorrect length.\n\n        \"\"\"\n        orig_col2attr = None\n        if models and hasattr(models[0].metadata, \"column_to_attr\"):\n            orig_col2attr = models[0].metadata.column_to_attr\n        cached_model = self._load_merged_model_from_cache(json_file_name, force_update, merge_attrs_list, orig_col2attr)\n        if cached_model is not None:\n            return cached_model\n\n        self._validate_merge_args(models, attribute_names, merge_attrs_list)\n        merged = self._merge_models(\n            models, method=method, match_by=match_by, attribute_names=attribute_names, merge_attrs_list=merge_attrs_list\n            )\n        self._update_metadata(merged, models, merge_attrs_list)\n        self._save_cache(merged, json_file_name)\n        return merged\n\n    def _validate_merge_args(\n        self,\n        models: list[SpecModel],\n        attribute_names: list,\n        merge_attrs_list: list,\n    ) -&gt; None:\n        \"\"\"Validate and normalize merge arguments for merging models.\"\"\"\n        if not models:\n            raise ValueError(\"No models to merge\")\n        n_merges = len(models) - 1\n        if attribute_names is None:\n            attribute_names = [None] * n_merges\n        elif not isinstance(attribute_names, list):\n            attribute_names = [attribute_names] * n_merges\n        if merge_attrs_list is None:\n            merge_attrs_list = [None] * n_merges\n        elif (\n            not isinstance(merge_attrs_list, list)\n            or (\n                merge_attrs_list\n                and not isinstance(merge_attrs_list[0], list)\n            )\n        ):\n            merge_attrs_list = [merge_attrs_list] * n_merges\n        if len(attribute_names) != n_merges:\n            raise ValueError(\n                f\"Length of attribute_names ({len(attribute_names)}) \"\n                f\"does not match number of merges ({n_merges})\"\n            )\n        if len(merge_attrs_list) != n_merges:\n            raise ValueError(\n                f\"Length of merge_attrs_list ({len(merge_attrs_list)}) \"\n                f\"does not match number of merges ({n_merges})\"\n                )\n\n    def _merge_models(\n        self,\n        models: list[SpecModel],\n        method: str = \"matching_path\",\n        match_by: str = \"name\",\n        attribute_names: list = None,\n        merge_attrs_list: list = None,\n    ) -&gt; SpecModel:\n        \"\"\"Perform the actual merging of models using the specified method.\"\"\"\n        merged = models[0]\n        if method not in (\"matching_path\", \"matching_node\"):\n            raise ValueError(f\"Unknown merge method: {method}\")\n\n        for i, model in enumerate(models[1:]):\n            attribute_name = attribute_names[i]\n            merge_attrs = merge_attrs_list[i]\n            if method == \"matching_path\":\n                merged = merged.merge_matching_path(\n                    model, match_by=match_by, attribute_name=attribute_name, merge_attrs=merge_attrs\n                    )\n            elif method == \"matching_node\":\n                merged = merged.merge_matching_node(\n                    model, match_by=match_by, attribute_name=attribute_name, merge_attrs=merge_attrs\n                    )\n        return merged\n\n    def _update_metadata(\n        self,\n        merged: SpecModel,\n        models: list[SpecModel],\n        merge_attrs_list: list,\n    ) -&gt; None:\n        \"\"\"Update the metadata of the merged model to reflect merged attributes.\"\"\"\n        # Start with the original metadata\n        meta = merged.metadata\n        orig_header = list(getattr(meta, \"header\", []))\n        orig_col2attr = dict(getattr(meta, \"column_to_attr\", {}))\n\n        # Find the next available column index\n        next_col = max(int(idx) for idx in orig_col2attr) + 1 if orig_col2attr else 0\n        # For each merged-in model, add new merged attributes if not already present\n        for i, model in enumerate(models[1:]):\n            merge_attrs = merge_attrs_list[i]\n            other_meta = getattr(model, \"metadata\", None)\n            if other_meta is not None and merge_attrs:\n                other_header = getattr(other_meta, \"header\", None)\n                other_col2attr = getattr(other_meta, \"column_to_attr\", None)\n                if other_header and other_col2attr:\n                    for idx, attr in other_col2attr.items():\n                        if attr in merge_attrs and attr not in orig_col2attr.values():\n                            # Add new column for this attribute\n                            if isinstance(other_header, list) and int(idx) &lt; len(other_header):\n                                orig_header.append(other_header[int(idx)])\n                            else:\n                                orig_header.append(attr)\n                            orig_col2attr[next_col] = attr\n                            next_col += 1\n\n        if hasattr(meta, \"header\"):\n            meta.header = orig_header\n        if hasattr(meta, \"column_to_attr\"):\n            meta.column_to_attr = orig_col2attr\n\n    def _save_cache(\n        self,\n        merged: SpecModel,\n        json_file_name: str,\n    ) -&gt; None:\n        \"\"\"Save the merged model to cache if a json_file_name is provided.\"\"\"\n        if json_file_name:\n            merged_json_file_path = os.path.join(\n                self.config.get_param(\"cache_dir\"), \"model\", json_file_name\n            )\n            try:\n                self.model_store.save(merged, merged_json_file_path)\n            except Exception as e:\n                self.logger.warning(f\"Failed to cache merged model to {merged_json_file_path}: {e}\")\n        else:\n            self.logger.info(\"No json_file_name specified; merged model not cached.\")\n\n    def _load_merged_model_from_cache(\n        self,\n        json_file_name: str,\n        force_update: bool,\n        merge_attrs_list: list = None,\n        orig_col2attr: dict = None,\n    ) -&gt; SpecModel | None:\n        \"\"\"Return the cached merged model if available, valid, and not force_update, else None.\"\"\"\n        merged_json_file_path = None\n        if json_file_name:\n            merged_json_file_path = os.path.join(\n                self.config.get_param(\"cache_dir\"), \"model\", json_file_name\n            )\n        if merged_json_file_path and os.path.exists(merged_json_file_path) and not force_update:\n            try:\n                model = self.model_store.load(merged_json_file_path)\n                # Check that all requested merge attributes are present in the cached model's metadata\n                if merge_attrs_list:\n                    all_attrs = set()\n                    for attrs in merge_attrs_list:\n                        if attrs:\n                            all_attrs.update(attrs)\n                    col2attr = getattr(model.metadata, \"column_to_attr\", {})\n                    orig_attrs = set(orig_col2attr.values()) if orig_col2attr else set()\n                    # All requested attributes must be present\n                    if any(attr not in col2attr.values() for attr in all_attrs):\n                        self.logger.info(\n                            f\"Cached model at {merged_json_file_path} missing required merged attributes {all_attrs}; \"\n                            f\"ignoring cache.\"\n                        )\n                        return None\n                    # No extra attributes except those in the original model\n                    allowed_attrs = all_attrs | orig_attrs\n                    extra_attrs = set(col2attr.values()) - allowed_attrs\n                    if extra_attrs:\n                        self.logger.info(\n                            f\"Cached model at {merged_json_file_path} contains extra attributes {extra_attrs} \"\n                            f\"not requested; ignoring cache.\"\n                        )\n                        return None\n                self.logger.info(\n                    f\"Loaded model from cache {merged_json_file_path}\"\n                )\n                return model\n            except Exception as e:\n                self.logger.warning(\n                    f\"Failed to load merged model from cache {merged_json_file_path}: {e}\"\n                )\n        return None\n</code></pre>"},{"location":"api/spec_merger/#dcmspec.spec_merger.SpecMerger.__init__","title":"<code>__init__(config=None, model_store=None, logger=None)</code>","text":"<p>Initialize the SpecMerger.</p> <p>Sets up the logger for the merger. If no logger is provided, a default logger is created. If no model_store is provided, defaults to JSONSpecStore.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object. If None, a default Config is created.</p> <p> TYPE: <code>Optional[Config]</code> DEFAULT: <code>None</code> </p> <code>model_store</code> <p>Store for loading and saving models. Defaults to JSONSpecStore.</p> <p> TYPE: <code>Optional[SpecStore]</code> DEFAULT: <code>None</code> </p> <code>logger</code> <p>Logger instance to use. If None, a default logger is created.</p> <p> TYPE: <code>Optional[Logger]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/dcmspec/spec_merger.py</code> <pre><code>def __init__(self, config: Config = None, model_store: SpecStore = None, logger: logging.Logger = None):\n    \"\"\"Initialize the SpecMerger.\n\n    Sets up the logger for the merger. If no logger is provided, a default logger is created.\n    If no model_store is provided, defaults to JSONSpecStore.\n\n    Args:\n        config (Optional[Config]): Configuration object. If None, a default Config is created.\n        model_store (Optional[SpecStore]): Store for loading and saving models. Defaults to JSONSpecStore.\n        logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n    \"\"\"\n    self.logger = logger or logging.getLogger(self.__class__.__name__)\n    self.config = config or Config()\n    self.model_store = model_store or JSONSpecStore(logger=self.logger)\n</code></pre>"},{"location":"api/spec_merger/#dcmspec.spec_merger.SpecMerger.merge_many","title":"<code>merge_many(models, method='matching_path', match_by='name', attribute_names=None, merge_attrs_list=None, json_file_name=None, force_update=False)</code>","text":"<p>Merge a sequence of DICOM SpecModel objects using the specified merge method, with optional caching.</p> <p>This method merges a list of models in order, applying either path-based or node-based merging at each step. You can specify different attribute names and lists of attributes to merge for each step, allowing for flexible, multi-stage enrichment of DICOM models. If json_file_name is provided, the merged model will be cached to that file, and loaded from cache if available and force_update is False.</p> PARAMETER DESCRIPTION <code>models</code> <p>The models to merge, in order.</p> <p> TYPE: <code>list of SpecModel</code> </p> <code>method</code> <p>Merge method to use (\"matching_path\" or \"matching_node\").</p> <p> TYPE: <code>str</code> DEFAULT: <code>'matching_path'</code> </p> <code>match_by</code> <p>\"name\" to match by node name, \"attribute\" to match by a specific attribute.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'name'</code> </p> <code>attribute_names</code> <p>List of attribute names to use for each merge step. Each entry corresponds to a merge operation between two models.</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>merge_attrs_list</code> <p>List of lists of attribute names to merge for each merge step. Each entry corresponds to a merge operation between two models.</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>json_file_name</code> <p>If provided, cache/load the merged model to/from this file.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>force_update</code> <p>If True, always perform the merge and overwrite the cache.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>SpecModel</code> <p>The final merged SpecModel instance.</p> <p> TYPE: <code>SpecModel</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If models is empty, method is unknown, or attribute_names/merge_attrs_list have incorrect length.</p> Source code in <code>src/dcmspec/spec_merger.py</code> <pre><code>def merge_many(\n    self,\n    models: list[SpecModel],\n    method: str = \"matching_path\",\n    match_by: str = \"name\",\n    attribute_names: list = None,\n    merge_attrs_list: list = None,\n    json_file_name: str = None,\n    force_update: bool = False,\n) -&gt; SpecModel:\n    \"\"\"Merge a sequence of DICOM SpecModel objects using the specified merge method, with optional caching.\n\n    This method merges a list of models in order, applying either path-based or node-based\n    merging at each step. You can specify different attribute names and lists of attributes\n    to merge for each step, allowing for flexible, multi-stage enrichment of DICOM models.\n    If json_file_name is provided, the merged model will be cached to that file, and loaded from\n    cache if available and force_update is False.\n\n    Args:\n        models (list of SpecModel): The models to merge, in order.\n        method (str): Merge method to use (\"matching_path\" or \"matching_node\").\n        match_by (str, optional): \"name\" to match by node name, \"attribute\" to match by a specific attribute.\n        attribute_names (list, optional): List of attribute names to use for each merge step.\n            Each entry corresponds to a merge operation between two models.\n        merge_attrs_list (list, optional): List of lists of attribute names to merge for each merge step.\n            Each entry corresponds to a merge operation between two models.\n        json_file_name (str, optional): If provided, cache/load the merged model to/from this file.\n        force_update (bool, optional): If True, always perform the merge and overwrite the cache.\n\n    Returns:\n        SpecModel: The final merged SpecModel instance.\n\n    Raises:\n        ValueError: If models is empty, method is unknown, or attribute_names/merge_attrs_list\n            have incorrect length.\n\n    \"\"\"\n    orig_col2attr = None\n    if models and hasattr(models[0].metadata, \"column_to_attr\"):\n        orig_col2attr = models[0].metadata.column_to_attr\n    cached_model = self._load_merged_model_from_cache(json_file_name, force_update, merge_attrs_list, orig_col2attr)\n    if cached_model is not None:\n        return cached_model\n\n    self._validate_merge_args(models, attribute_names, merge_attrs_list)\n    merged = self._merge_models(\n        models, method=method, match_by=match_by, attribute_names=attribute_names, merge_attrs_list=merge_attrs_list\n        )\n    self._update_metadata(merged, models, merge_attrs_list)\n    self._save_cache(merged, json_file_name)\n    return merged\n</code></pre>"},{"location":"api/spec_merger/#dcmspec.spec_merger.SpecMerger.merge_node","title":"<code>merge_node(model1, model2, match_by='name', attribute_name=None, merge_attrs=None, json_file_name=None, force_update=False)</code>","text":"<p>Merge two DICOM SpecModel objects using the node merge method, with optional caching.</p> <p>This is a convenience method that calls merge_many with two models.</p> PARAMETER DESCRIPTION <code>model1</code> <p>The first model.</p> <p> TYPE: <code>SpecModel</code> </p> <code>model2</code> <p>The second model to merge with the first.</p> <p> TYPE: <code>SpecModel</code> </p> <code>match_by</code> <p>\"name\" to match by node name, \"attribute\" to match by a specific attribute.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'name'</code> </p> <code>attribute_name</code> <p>The attribute name to use for matching.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>merge_attrs</code> <p>List of attribute names to merge from the other model's node.</p> <p> TYPE: <code>list[str]</code> DEFAULT: <code>None</code> </p> <code>json_file_name</code> <p>If provided, cache/load the merged model to/from this file.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>force_update</code> <p>If True, always perform the merge and overwrite the cache.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>SpecModel</code> <p>The merged SpecModel instance.</p> <p> TYPE: <code>SpecModel</code> </p> Source code in <code>src/dcmspec/spec_merger.py</code> <pre><code>def merge_node(\n    self,\n    model1: SpecModel,\n    model2: SpecModel,\n    match_by: str = \"name\",\n    attribute_name: str = None,\n    merge_attrs: list[str] = None,\n    json_file_name: str = None,\n    force_update: bool = False,\n) -&gt; SpecModel:\n    \"\"\"Merge two DICOM SpecModel objects using the node merge method, with optional caching.\n\n    This is a convenience method that calls merge_many with two models.\n\n    Args:\n        model1 (SpecModel): The first model.\n        model2 (SpecModel): The second model to merge with the first.\n        match_by (str, optional): \"name\" to match by node name, \"attribute\" to match by a specific attribute.\n        attribute_name (str, optional): The attribute name to use for matching.\n        merge_attrs (list[str], optional): List of attribute names to merge from the other model's node.\n        json_file_name (str, optional): If provided, cache/load the merged model to/from this file.\n        force_update (bool, optional): If True, always perform the merge and overwrite the cache.\n\n    Returns:\n        SpecModel: The merged SpecModel instance.\n\n    \"\"\"\n    return self.merge_many(\n        [model1, model2],\n        method = \"matching_node\",\n        match_by=match_by,\n        attribute_names=[attribute_name],\n        merge_attrs_list=[merge_attrs],\n        json_file_name=json_file_name,\n        force_update=force_update,\n    )\n</code></pre>"},{"location":"api/spec_merger/#dcmspec.spec_merger.SpecMerger.merge_path","title":"<code>merge_path(model1, model2, match_by='name', attribute_name=None, merge_attrs=None, json_file_name=None, force_update=False)</code>","text":"<p>Merge two DICOM SpecModel objects using the path merge method, with optional caching.</p> <p>This is a convenience method that calls merge_many with two models.</p> PARAMETER DESCRIPTION <code>model1</code> <p>The first model.</p> <p> TYPE: <code>SpecModel</code> </p> <code>model2</code> <p>The second model to merge with the first.</p> <p> TYPE: <code>SpecModel</code> </p> <code>match_by</code> <p>\"name\" to match by node name, \"attribute\" to match by a specific attribute.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'name'</code> </p> <code>attribute_name</code> <p>The attribute name to use for matching.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>merge_attrs</code> <p>List of attribute names to merge from the other model's node.</p> <p> TYPE: <code>list[str]</code> DEFAULT: <code>None</code> </p> <code>json_file_name</code> <p>If provided, cache/load the merged model to/from this file.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>force_update</code> <p>If True, always perform the merge and overwrite the cache.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>SpecModel</code> <p>The merged SpecModel instance.</p> <p> TYPE: <code>SpecModel</code> </p> Source code in <code>src/dcmspec/spec_merger.py</code> <pre><code>def merge_path(\n    self,\n    model1: SpecModel,\n    model2: SpecModel,\n    match_by: str = \"name\",\n    attribute_name: str = None,\n    merge_attrs: list[str] = None,\n    json_file_name: str = None,\n    force_update: bool = False,\n) -&gt; SpecModel:\n    \"\"\"Merge two DICOM SpecModel objects using the path merge method, with optional caching.\n\n    This is a convenience method that calls merge_many with two models.\n\n    Args:\n        model1 (SpecModel): The first model.\n        model2 (SpecModel): The second model to merge with the first.\n        match_by (str, optional): \"name\" to match by node name, \"attribute\" to match by a specific attribute.\n        attribute_name (str, optional): The attribute name to use for matching.\n        merge_attrs (list[str], optional): List of attribute names to merge from the other model's node.\n        json_file_name (str, optional): If provided, cache/load the merged model to/from this file.\n        force_update (bool, optional): If True, always perform the merge and overwrite the cache.\n\n    Returns:\n        SpecModel: The merged SpecModel instance.\n\n    \"\"\"\n    return self.merge_many(\n        [model1, model2],\n        method = \"matching_path\",\n        match_by=match_by,\n        attribute_names=[attribute_name],\n        merge_attrs_list=[merge_attrs],\n        json_file_name=json_file_name,\n        force_update=force_update,\n    )\n</code></pre>"},{"location":"api/spec_model/","title":"SpecModel","text":""},{"location":"api/spec_model/#dcmspec.spec_model.SpecModel","title":"<code>dcmspec.spec_model.SpecModel</code>","text":"<p>Represent a hierarchical information model from any table of DICOM documents.</p> <p>This class holds the DICOM specification model, structured into a hierarchical tree of DICOM components such as Data Elements, UIDs, Attributes, and others.</p> The model contains two main parts <ul> <li>metadata: a node holding table and document metadata</li> <li>content: a node holding the hierarchical content tree</li> </ul> <p>The model can be filtered.</p> Source code in <code>src/dcmspec/spec_model.py</code> <pre><code>class SpecModel:\n    \"\"\"Represent a hierarchical information model from any table of DICOM documents.\n\n    This class holds the DICOM specification model, structured into a hierarchical tree\n    of DICOM components such as Data Elements, UIDs, Attributes, and others.\n\n    The model contains two main parts:\n        - metadata: a node holding table and document metadata\n        - content: a node holding the hierarchical content tree\n\n    The model can be filtered.\n    \"\"\"\n\n    def __init__(\n        self,\n        metadata: Node,\n        content: Node,\n        logger: logging.Logger = None,\n    ):\n        \"\"\"Initialize the SpecModel.\n\n        Sets up the logger and initializes the specification model.\n\n        Args:\n            metadata (Node): Node holding table and document metadata, such as headers, version, and table ID.\n            content (Node): Node holding the hierarchical content tree of the DICOM specification.\n            logger (logging.Logger, optional): A pre-configured logger instance to use.\n                If None, a default logger will be created.\n\n        \"\"\"\n        self.logger = logger or logging.getLogger(self.__class__.__name__)\n\n        self.metadata = metadata\n        self.content = content\n\n    def exclude_titles(self) -&gt; None:\n        \"\"\"Remove nodes corresponding to title rows from the content tree.\n\n        Title rows are typically found in some DICOM tables and represent section headers\n        rather than actual data elements (such as Module titles in PS3.4). \n        This method traverses the content tree and removes any node identified as a title,\n        cleaning up the model for further processing.\n\n        The method operates on the content tree and does not affect the metadata node.\n\n        Returns:\n            None\n\n        \"\"\"\n        # Traverse the tree and remove nodes where is_title is True\n        for node in list(PreOrderIter(self.content)):\n            if self._is_title(node):\n                self.logger.debug(f\"Removing title node: {node.name}\")\n                node.parent = None\n\n    def filter_required(\n        self,\n        type_attr_name: str,\n        keep: Optional[list[str]] = None,\n        remove: Optional[list[str]] = None\n    ) -&gt; None:\n        \"\"\"Remove nodes that are considered optional according to DICOM requirements.\n\n        This method traverses the content tree and removes nodes whose requirement\n        (e.g., \"Type\", \"Matching\", or \"Return Key\") indicates that they are optional. \n        Nodes with conditional or required types (e.g., \"1\", \"1C\", \"2\", \"2C\")\n        are retained. The method can be customized by specifying which types to keep or remove.\n\n        Additionally, for nodes representing Sequences (node names containing \"_sequence\"), \n        this method removes all subelements if the sequence itself is not required or can be empty\n        (e.g., type \"3\", \"2\", \"2C\", \"-\", \"O\", or \"Not allowed\").\n\n        Args:\n            type_attr_name (str): Name of the node attribute holding the optionality requirement,\n                for example \"Type\" of an attribute, \"Matching\", or \"Return Key\".\n            keep (Optional[list[str]]): List of type values to keep (default: [\"1\", \"1C\", \"2\", \"2C\"]).\n            remove (Optional[list[str]]): List of type values to remove (default: [\"3\"]).\n\n        Returns:\n            None\n\n        \"\"\"\n        if keep is None:\n            keep = [\"1\", \"1C\", \"2\", \"2C\"]\n        if remove is None:\n            remove = [\"3\"]\n        types_to_keep = keep\n        types_to_remove = remove\n        attribute_name = type_attr_name\n\n        for node in PreOrderIter(self.content):\n            if hasattr(node, attribute_name):\n                dcmtype = getattr(node, attribute_name)\n                if dcmtype in types_to_remove and dcmtype not in types_to_keep:\n                    self.logger.debug(f\"[{dcmtype.rjust(3)}] : Removing {node.name} element\")\n                    node.parent = None\n                # Remove nodes under \"Sequence\" nodes which are not required or which can be empty\n                if \"_sequence\" in node.name and dcmtype in [\"3\", \"2\", \"2C\", \"-\", \"O\", \"Not allowed\"]:\n                    self.logger.debug(f\"[{dcmtype.rjust(3)}] : Removing {node.name} subelements\")\n                    for descendant in node.descendants:\n                        descendant.parent = None\n\n    def merge_matching_path(\n        self,\n        other: \"SpecModel\",\n        match_by: str = \"name\",\n        attribute_name: Optional[str] = None,\n        merge_attrs: Optional[list[str]] = None,\n    ) -&gt; \"SpecModel\":\n        \"\"\"Merge with another SpecModel, producing a new model with attributes merged for nodes with matching paths.\n\n        The path for matching is constructed at each level using either the node's `name`\n        (if match_by=\"name\") or a specified attribute (if match_by=\"attribute\" and attribute_name is given).\n        Only nodes whose full path matches (by the chosen key) will be merged.\n\n        This method is useful for combining DICOM specification models from different parts of the standard.\n        For example, it can be used to merge a PS3.3 model of a normalized IOD specification with a PS3.4 model of a\n        SOP class specification.\n\n        Args:\n            other (SpecModel): The other model to merge with the current model.\n            match_by (str): \"name\" to match by node.name path, \"attribute\" to match by a specific attribute path.\n            attribute_name (str, optional): The attribute name to use for matching if match_by=\"attribute\".\n            merge_attrs (list[str], optional): List of attribute names to merge from the other model's node.\n\n        Returns:\n            SpecModel: A new merged SpecModel.\n\n        \"\"\"        \n        return self._merge_nodes(\n            other,\n            match_by=match_by,\n            attribute_name=attribute_name,\n            merge_attrs=merge_attrs,\n            is_path_based=True\n        )\n\n    def merge_matching_node(\n        self,\n        other: \"SpecModel\",\n        match_by: str = \"name\",\n        attribute_name: Optional[str] = None,\n        merge_attrs: Optional[list[str]] = None,\n    ) -&gt; \"SpecModel\":\n        \"\"\"Merge two SpecModel trees by matching nodes at any level using a single key (name or attribute).\n\n        For each node in the current model, this method finds a matching node in the other model\n        using either the node's name (if match_by=\"name\") or a specified attribute (if match_by=\"attribute\").\n        If a match is found, the specified attributes from the other model's node are merged into the current node.\n\n        This is useful for enrichment scenarios, such as adding VR/VM/Keyword from the Part 6 dictionary\n        to a Part 3 module, where nodes are matched by a unique attribute like elem_tag.\n\n        - Matching is performed globally (not by path): any node in the current model is matched to any node\n          in the other model with the same key value, regardless of their position in the tree.\n        - It is expected that there is only one matching node per key in the other model.\n        - If multiple nodes in the other model have the same key, a warning is logged and only the last one\n          found in pre-order traversal is used for merging.\n\n        Example use cases:\n            - Enrich a PS3.3 module attribute specification with VR/VM from the PS3.6 data elements dictionary.\n            - Merge any two models where a unique key (name or attribute) can be used for node correspondence.\n\n        Args:\n            other (SpecModel): The other model to merge with the current model.\n            match_by (str): \"name\" to match by node.name (stripped of leading '&gt;' and whitespace),\n                or \"attribute\" to match by a specific attribute value.\n            attribute_name (str, optional): The attribute name to use for matching if match_by=\"attribute\".\n            merge_attrs (list[str], optional): List of attribute names to merge from the other model's node.\n\n        Returns:\n            SpecModel: A new merged SpecModel with attributes from the other model merged in.\n\n        Raises:\n            ValueError: If match_by is invalid or attribute_name is missing when required.\n\n        \"\"\"        \n        return self._merge_nodes(\n            other,\n            match_by=match_by,\n            attribute_name=attribute_name,\n            merge_attrs=merge_attrs,\n            is_path_based=False\n        )\n    def _strip_leading_gt(self, name):\n        \"\"\"Strip leading '&gt;' and whitespace from a node name for matching.\"\"\"\n        return name.lstrip(\"&gt;\").lstrip().rstrip() if isinstance(name, str) else name\n\n    def _is_include(self, node: Node) -&gt; bool:\n        \"\"\"Determine if a node represents an 'Include' of a Macro table.\n\n        Args:\n            node: The node to check.\n\n        Returns:\n            True if the node represents an 'Include' of a Macro table, False otherwise.\n\n        \"\"\"\n        return \"include_table\" in node.name\n\n    def _is_title(self, node: Node) -&gt; bool:\n        \"\"\"Determine if a node is a title.\n\n        Args:\n            node: The node to check.\n\n        Returns:\n            True if the node is a title, False otherwise.\n\n        \"\"\"\n        return (\n            self._has_only_key_0_attr(node, self.metadata.column_to_attr)\n            and not self._is_include(node)\n            and node.name != \"content\"\n        )\n\n    def _has_only_key_0_attr(self, node: Node, column_to_attr: Dict[int, str]) -&gt; bool:\n        # sourcery skip: merge-duplicate-blocks, use-any\n        \"\"\"Check that only the key 0 attribute is present.\n\n        Determines if a node has only the attribute specified by the item with key \"0\"\n        in column_to_attr, corresponding to the first column of the table.\n\n        Args:\n            node: The node to check.\n            column_to_attr: Mapping between column number and attribute name.\n\n        Returns:\n            True if the node has only the key \"0\" attribute, False otherwise.\n\n        \"\"\"\n        # Irrelevant if columns 0 not extracted\n        if 0 not in column_to_attr:\n            return False\n\n        # Check that only the key 0 attribute is present\n        key_0_attr = column_to_attr[0]\n        for key, attr_name in column_to_attr.items():\n            if key == 0:\n                if not hasattr(node, key_0_attr):\n                    return False\n            elif hasattr(node, attr_name):\n                return False\n        return True\n\n    @staticmethod\n    def _get_node_path(node: Node, attr: str = \"name\") -&gt; tuple:\n        \"\"\"Return a tuple representing the path of the node using the given attribute.\"\"\"\n        return tuple(getattr(n, attr, None) for n in node.path)\n\n\n    @staticmethod\n    def _get_path_by_name(node: Node) -&gt; tuple:\n        \"\"\"Return the path of the node using node.name at each level.\"\"\"\n        return SpecModel._get_node_path(node, \"name\")\n\n    @staticmethod\n    def _get_path_by_attr(node: Node, attr: str) -&gt; tuple:\n        \"\"\"Return the path of the node using the given attribute at each level.\"\"\"\n        return SpecModel._get_node_path(node, attr)\n\n    def _build_node_map(\n        self,\n        other: \"SpecModel\",\n        match_by: str,\n        attribute_name: Optional[str] = None,\n        is_path_based: bool = False\n    ) -&gt; tuple[dict, callable]:\n        \"\"\"Construct a mapping from keys to nodes in the other model, and a key function for matching.\n\n        This method prepares the data structures needed for merging two SpecModel trees. It builds a mapping\n        from a key (either a node's name, a specified attribute, or a path of such values) to nodes in the\n        `other` model, and returns a function that computes the same key for nodes in the current model.\n\n        Args:\n            other (SpecModel): The other model to merge with.\n            match_by (str): \"name\" to match by node name, or \"attribute\" to match by a specific attribute.\n            attribute_name (str, optional): The attribute name to use for matching if match_by=\"attribute\".\n            is_path_based (bool): If True, use the full path of names/attributes as the key; if False, \n                use only the value.\n\n        Returns:\n            tuple: (node_map, key_func)\n                node_map (dict): Mapping from key to node in the other model.\n                key_func (callable): Function that computes the key for a node in the current model.\n\n        Raises:\n            ValueError: If match_by is invalid or attribute_name is missing when required.\n\n        \"\"\"\n        if match_by == \"name\":\n            self.logger.debug(\"Matching models by node name.\")\n            if is_path_based:\n                node_map = {\n                    self._get_path_by_name(node): node\n                    for node in PreOrderIter(other.content)\n                }\n                def key_func(node):\n                    return self._get_path_by_name(node)\n            else:\n                def key_func(node):\n                    return self._strip_leading_gt(node.name)\n                # Build mapping with handling of duplicates\n                key_to_nodes = defaultdict(list)\n                for node in PreOrderIter(other.content):\n                    key = key_func(node)\n                    key_to_nodes[key].append(node)\n\n                self._warn_multiple_matches(key_to_nodes)\n                node_map = {key: nodes[-1] for key, nodes in key_to_nodes.items()}\n\n        elif match_by == \"attribute\" and attribute_name:\n            self.logger.debug(f\"Matching models by attribute: {attribute_name}\")\n            if is_path_based:\n                node_map = {\n                    self._get_path_by_attr(node, attribute_name): node\n                    for node in PreOrderIter(other.content)\n                }\n                def key_func(node):\n                    return self._get_path_by_attr(node, attribute_name)\n            else:\n                def key_func(node):\n                    return getattr(node, attribute_name, None)\n                # Build mapping with handling of duplicates\n                key_to_nodes = defaultdict(list)\n                for node in PreOrderIter(other.content):\n                    key = key_func(node)\n                    key_to_nodes[key].append(node)\n\n                self._warn_multiple_matches(key_to_nodes)\n                node_map = {key: nodes[-1] for key, nodes in key_to_nodes.items()}\n        else:\n            raise ValueError(\"Invalid match_by or missing attribute_name\")\n\n        return node_map, key_func\n\n    def _warn_multiple_matches(self, key_to_nodes: dict):\n        \"\"\"Log a warning if any key in the mapping corresponds to multiple nodes.\n\n        Args:\n            key_to_nodes (dict): A mapping from key to a list of nodes with that key.\n\n        Returns:\n            None\n\n        \"\"\"\n        for key, nodes in key_to_nodes.items():\n            if key is not None and len(nodes) &gt; 1:\n                self.logger.warning(\n                    f\"Multiple nodes found for key '{key}': \"\n                    f\"{[getattr(n, 'name', None) for n in nodes]}. \"\n                    \"Only the last one will be used for merging.\"\n                )\n\n    def _merge_nodes(\n        self,\n        other: \"SpecModel\",\n        match_by: str,\n        attribute_name: Optional[str] = None,\n        merge_attrs: Optional[list[str]] = None,\n        is_path_based: bool = False\n    ) -&gt; \"SpecModel\":\n        \"\"\"Merge this SpecModel with another, enriching nodes by matching keys.\n\n        This is the core logic for merging two SpecModel trees. For each node in the current model,\n        it attempts to find a matching node in the other model using the specified matching strategy.\n        If a match is found, the specified attributes from the other node are copied into the current node.\n\n        Args:\n            other (SpecModel): The other model to merge from.\n            match_by (str): \"name\" to match by node name, or \"attribute\" to match by a specific attribute.\n            attribute_name (str, optional): The attribute name to use for matching if match_by=\"attribute\".\n            merge_attrs (list[str], optional): List of attribute names to copy from the matching node.\n            is_path_based (bool): If True, match nodes by their full path; if False, match globally by key.\n\n        Returns:\n            SpecModel: A deep copy of this model, with attributes merged from the other model where matches are found.\n\n        Notes:\n            - If multiple nodes in the other model have the same key, only the last one is used (a warning is logged).\n            - If a node in this model has no match in the other model, it is left unchanged.\n            - The merge is non-destructive: a new SpecModel is returned.\n\n        \"\"\"\n        merged = copy.deepcopy(self)\n\n        node_map, key_func = self._build_node_map(\n            other, match_by, attribute_name, is_path_based\n        )\n\n        for node in PreOrderIter(merged.content):\n            key = key_func(node)\n            if key in node_map and key is not None:\n                other_node = node_map[key]\n                for attr in (merge_attrs or []):\n                    if attr is not None and hasattr(other_node, attr):\n                        setattr(node, attr, getattr(other_node, attr))\n                        self.logger.debug(f\"Enriched node {getattr(node, 'name', None)} \"\n                                        f\"(key={key}) with {attr}={getattr(other_node, attr)}\")\n            else:\n                self.logger.debug(f\"No match for node {getattr(node, 'name', None)} (key={key})\")\n\n        return merged\n</code></pre>"},{"location":"api/spec_model/#dcmspec.spec_model.SpecModel.__init__","title":"<code>__init__(metadata, content, logger=None)</code>","text":"<p>Initialize the SpecModel.</p> <p>Sets up the logger and initializes the specification model.</p> PARAMETER DESCRIPTION <code>metadata</code> <p>Node holding table and document metadata, such as headers, version, and table ID.</p> <p> TYPE: <code>Node</code> </p> <code>content</code> <p>Node holding the hierarchical content tree of the DICOM specification.</p> <p> TYPE: <code>Node</code> </p> <code>logger</code> <p>A pre-configured logger instance to use. If None, a default logger will be created.</p> <p> TYPE: <code>Logger</code> DEFAULT: <code>None</code> </p> Source code in <code>src/dcmspec/spec_model.py</code> <pre><code>def __init__(\n    self,\n    metadata: Node,\n    content: Node,\n    logger: logging.Logger = None,\n):\n    \"\"\"Initialize the SpecModel.\n\n    Sets up the logger and initializes the specification model.\n\n    Args:\n        metadata (Node): Node holding table and document metadata, such as headers, version, and table ID.\n        content (Node): Node holding the hierarchical content tree of the DICOM specification.\n        logger (logging.Logger, optional): A pre-configured logger instance to use.\n            If None, a default logger will be created.\n\n    \"\"\"\n    self.logger = logger or logging.getLogger(self.__class__.__name__)\n\n    self.metadata = metadata\n    self.content = content\n</code></pre>"},{"location":"api/spec_model/#dcmspec.spec_model.SpecModel.exclude_titles","title":"<code>exclude_titles()</code>","text":"<p>Remove nodes corresponding to title rows from the content tree.</p> <p>Title rows are typically found in some DICOM tables and represent section headers rather than actual data elements (such as Module titles in PS3.4).  This method traverses the content tree and removes any node identified as a title, cleaning up the model for further processing.</p> <p>The method operates on the content tree and does not affect the metadata node.</p> RETURNS DESCRIPTION <code>None</code> <p>None</p> Source code in <code>src/dcmspec/spec_model.py</code> <pre><code>def exclude_titles(self) -&gt; None:\n    \"\"\"Remove nodes corresponding to title rows from the content tree.\n\n    Title rows are typically found in some DICOM tables and represent section headers\n    rather than actual data elements (such as Module titles in PS3.4). \n    This method traverses the content tree and removes any node identified as a title,\n    cleaning up the model for further processing.\n\n    The method operates on the content tree and does not affect the metadata node.\n\n    Returns:\n        None\n\n    \"\"\"\n    # Traverse the tree and remove nodes where is_title is True\n    for node in list(PreOrderIter(self.content)):\n        if self._is_title(node):\n            self.logger.debug(f\"Removing title node: {node.name}\")\n            node.parent = None\n</code></pre>"},{"location":"api/spec_model/#dcmspec.spec_model.SpecModel.filter_required","title":"<code>filter_required(type_attr_name, keep=None, remove=None)</code>","text":"<p>Remove nodes that are considered optional according to DICOM requirements.</p> <p>This method traverses the content tree and removes nodes whose requirement (e.g., \"Type\", \"Matching\", or \"Return Key\") indicates that they are optional.  Nodes with conditional or required types (e.g., \"1\", \"1C\", \"2\", \"2C\") are retained. The method can be customized by specifying which types to keep or remove.</p> <p>Additionally, for nodes representing Sequences (node names containing \"_sequence\"),  this method removes all subelements if the sequence itself is not required or can be empty (e.g., type \"3\", \"2\", \"2C\", \"-\", \"O\", or \"Not allowed\").</p> PARAMETER DESCRIPTION <code>type_attr_name</code> <p>Name of the node attribute holding the optionality requirement, for example \"Type\" of an attribute, \"Matching\", or \"Return Key\".</p> <p> TYPE: <code>str</code> </p> <code>keep</code> <p>List of type values to keep (default: [\"1\", \"1C\", \"2\", \"2C\"]).</p> <p> TYPE: <code>Optional[list[str]]</code> DEFAULT: <code>None</code> </p> <code>remove</code> <p>List of type values to remove (default: [\"3\"]).</p> <p> TYPE: <code>Optional[list[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>None</code> <p>None</p> Source code in <code>src/dcmspec/spec_model.py</code> <pre><code>def filter_required(\n    self,\n    type_attr_name: str,\n    keep: Optional[list[str]] = None,\n    remove: Optional[list[str]] = None\n) -&gt; None:\n    \"\"\"Remove nodes that are considered optional according to DICOM requirements.\n\n    This method traverses the content tree and removes nodes whose requirement\n    (e.g., \"Type\", \"Matching\", or \"Return Key\") indicates that they are optional. \n    Nodes with conditional or required types (e.g., \"1\", \"1C\", \"2\", \"2C\")\n    are retained. The method can be customized by specifying which types to keep or remove.\n\n    Additionally, for nodes representing Sequences (node names containing \"_sequence\"), \n    this method removes all subelements if the sequence itself is not required or can be empty\n    (e.g., type \"3\", \"2\", \"2C\", \"-\", \"O\", or \"Not allowed\").\n\n    Args:\n        type_attr_name (str): Name of the node attribute holding the optionality requirement,\n            for example \"Type\" of an attribute, \"Matching\", or \"Return Key\".\n        keep (Optional[list[str]]): List of type values to keep (default: [\"1\", \"1C\", \"2\", \"2C\"]).\n        remove (Optional[list[str]]): List of type values to remove (default: [\"3\"]).\n\n    Returns:\n        None\n\n    \"\"\"\n    if keep is None:\n        keep = [\"1\", \"1C\", \"2\", \"2C\"]\n    if remove is None:\n        remove = [\"3\"]\n    types_to_keep = keep\n    types_to_remove = remove\n    attribute_name = type_attr_name\n\n    for node in PreOrderIter(self.content):\n        if hasattr(node, attribute_name):\n            dcmtype = getattr(node, attribute_name)\n            if dcmtype in types_to_remove and dcmtype not in types_to_keep:\n                self.logger.debug(f\"[{dcmtype.rjust(3)}] : Removing {node.name} element\")\n                node.parent = None\n            # Remove nodes under \"Sequence\" nodes which are not required or which can be empty\n            if \"_sequence\" in node.name and dcmtype in [\"3\", \"2\", \"2C\", \"-\", \"O\", \"Not allowed\"]:\n                self.logger.debug(f\"[{dcmtype.rjust(3)}] : Removing {node.name} subelements\")\n                for descendant in node.descendants:\n                    descendant.parent = None\n</code></pre>"},{"location":"api/spec_model/#dcmspec.spec_model.SpecModel.merge_matching_node","title":"<code>merge_matching_node(other, match_by='name', attribute_name=None, merge_attrs=None)</code>","text":"<p>Merge two SpecModel trees by matching nodes at any level using a single key (name or attribute).</p> <p>For each node in the current model, this method finds a matching node in the other model using either the node's name (if match_by=\"name\") or a specified attribute (if match_by=\"attribute\"). If a match is found, the specified attributes from the other model's node are merged into the current node.</p> <p>This is useful for enrichment scenarios, such as adding VR/VM/Keyword from the Part 6 dictionary to a Part 3 module, where nodes are matched by a unique attribute like elem_tag.</p> <ul> <li>Matching is performed globally (not by path): any node in the current model is matched to any node   in the other model with the same key value, regardless of their position in the tree.</li> <li>It is expected that there is only one matching node per key in the other model.</li> <li>If multiple nodes in the other model have the same key, a warning is logged and only the last one   found in pre-order traversal is used for merging.</li> </ul> Example use cases <ul> <li>Enrich a PS3.3 module attribute specification with VR/VM from the PS3.6 data elements dictionary.</li> <li>Merge any two models where a unique key (name or attribute) can be used for node correspondence.</li> </ul> PARAMETER DESCRIPTION <code>other</code> <p>The other model to merge with the current model.</p> <p> TYPE: <code>SpecModel</code> </p> <code>match_by</code> <p>\"name\" to match by node.name (stripped of leading '&gt;' and whitespace), or \"attribute\" to match by a specific attribute value.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'name'</code> </p> <code>attribute_name</code> <p>The attribute name to use for matching if match_by=\"attribute\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>merge_attrs</code> <p>List of attribute names to merge from the other model's node.</p> <p> TYPE: <code>list[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>SpecModel</code> <p>A new merged SpecModel with attributes from the other model merged in.</p> <p> TYPE: <code>SpecModel</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If match_by is invalid or attribute_name is missing when required.</p> Source code in <code>src/dcmspec/spec_model.py</code> <pre><code>def merge_matching_node(\n    self,\n    other: \"SpecModel\",\n    match_by: str = \"name\",\n    attribute_name: Optional[str] = None,\n    merge_attrs: Optional[list[str]] = None,\n) -&gt; \"SpecModel\":\n    \"\"\"Merge two SpecModel trees by matching nodes at any level using a single key (name or attribute).\n\n    For each node in the current model, this method finds a matching node in the other model\n    using either the node's name (if match_by=\"name\") or a specified attribute (if match_by=\"attribute\").\n    If a match is found, the specified attributes from the other model's node are merged into the current node.\n\n    This is useful for enrichment scenarios, such as adding VR/VM/Keyword from the Part 6 dictionary\n    to a Part 3 module, where nodes are matched by a unique attribute like elem_tag.\n\n    - Matching is performed globally (not by path): any node in the current model is matched to any node\n      in the other model with the same key value, regardless of their position in the tree.\n    - It is expected that there is only one matching node per key in the other model.\n    - If multiple nodes in the other model have the same key, a warning is logged and only the last one\n      found in pre-order traversal is used for merging.\n\n    Example use cases:\n        - Enrich a PS3.3 module attribute specification with VR/VM from the PS3.6 data elements dictionary.\n        - Merge any two models where a unique key (name or attribute) can be used for node correspondence.\n\n    Args:\n        other (SpecModel): The other model to merge with the current model.\n        match_by (str): \"name\" to match by node.name (stripped of leading '&gt;' and whitespace),\n            or \"attribute\" to match by a specific attribute value.\n        attribute_name (str, optional): The attribute name to use for matching if match_by=\"attribute\".\n        merge_attrs (list[str], optional): List of attribute names to merge from the other model's node.\n\n    Returns:\n        SpecModel: A new merged SpecModel with attributes from the other model merged in.\n\n    Raises:\n        ValueError: If match_by is invalid or attribute_name is missing when required.\n\n    \"\"\"        \n    return self._merge_nodes(\n        other,\n        match_by=match_by,\n        attribute_name=attribute_name,\n        merge_attrs=merge_attrs,\n        is_path_based=False\n    )\n</code></pre>"},{"location":"api/spec_model/#dcmspec.spec_model.SpecModel.merge_matching_path","title":"<code>merge_matching_path(other, match_by='name', attribute_name=None, merge_attrs=None)</code>","text":"<p>Merge with another SpecModel, producing a new model with attributes merged for nodes with matching paths.</p> <p>The path for matching is constructed at each level using either the node's <code>name</code> (if match_by=\"name\") or a specified attribute (if match_by=\"attribute\" and attribute_name is given). Only nodes whose full path matches (by the chosen key) will be merged.</p> <p>This method is useful for combining DICOM specification models from different parts of the standard. For example, it can be used to merge a PS3.3 model of a normalized IOD specification with a PS3.4 model of a SOP class specification.</p> PARAMETER DESCRIPTION <code>other</code> <p>The other model to merge with the current model.</p> <p> TYPE: <code>SpecModel</code> </p> <code>match_by</code> <p>\"name\" to match by node.name path, \"attribute\" to match by a specific attribute path.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'name'</code> </p> <code>attribute_name</code> <p>The attribute name to use for matching if match_by=\"attribute\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>merge_attrs</code> <p>List of attribute names to merge from the other model's node.</p> <p> TYPE: <code>list[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>SpecModel</code> <p>A new merged SpecModel.</p> <p> TYPE: <code>SpecModel</code> </p> Source code in <code>src/dcmspec/spec_model.py</code> <pre><code>def merge_matching_path(\n    self,\n    other: \"SpecModel\",\n    match_by: str = \"name\",\n    attribute_name: Optional[str] = None,\n    merge_attrs: Optional[list[str]] = None,\n) -&gt; \"SpecModel\":\n    \"\"\"Merge with another SpecModel, producing a new model with attributes merged for nodes with matching paths.\n\n    The path for matching is constructed at each level using either the node's `name`\n    (if match_by=\"name\") or a specified attribute (if match_by=\"attribute\" and attribute_name is given).\n    Only nodes whose full path matches (by the chosen key) will be merged.\n\n    This method is useful for combining DICOM specification models from different parts of the standard.\n    For example, it can be used to merge a PS3.3 model of a normalized IOD specification with a PS3.4 model of a\n    SOP class specification.\n\n    Args:\n        other (SpecModel): The other model to merge with the current model.\n        match_by (str): \"name\" to match by node.name path, \"attribute\" to match by a specific attribute path.\n        attribute_name (str, optional): The attribute name to use for matching if match_by=\"attribute\".\n        merge_attrs (list[str], optional): List of attribute names to merge from the other model's node.\n\n    Returns:\n        SpecModel: A new merged SpecModel.\n\n    \"\"\"        \n    return self._merge_nodes(\n        other,\n        match_by=match_by,\n        attribute_name=attribute_name,\n        merge_attrs=merge_attrs,\n        is_path_based=True\n    )\n</code></pre>"},{"location":"api/spec_parser/","title":"SpecParser","text":""},{"location":"api/spec_parser/#dcmspec.spec_parser.SpecParser","title":"<code>dcmspec.spec_parser.SpecParser</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for DICOM specification parsers.</p> <p>Handles DICOM specifications in various in-memory formats (e.g., DOM for XHTML/XML, CSV). Subclasses must implement the <code>parse</code> method to parse the specification content and build a structured model.</p> Source code in <code>src/dcmspec/spec_parser.py</code> <pre><code>class SpecParser(ABC):\n    \"\"\"Abstract base class for DICOM specification parsers.\n\n    Handles DICOM specifications in various in-memory formats (e.g., DOM for XHTML/XML, CSV).\n    Subclasses must implement the `parse` method to parse the specification content and build a structured model.\n    \"\"\"\n\n    def __init__(self, logger: Optional[logging.Logger] = None):\n        \"\"\"Initialize the DICOM Specification parser with an optional logger.\n\n        Args:\n            logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n        \"\"\"\n        if logger is not None and not isinstance(logger, logging.Logger):\n            raise TypeError(\"logger must be an instance of logging.Logger or None\")\n        self.logger = logger or logging.getLogger(self.__class__.__name__)\n\n    @abstractmethod\n    def parse(self, *args, **kwargs) -&gt; Tuple[Node, Node]:\n        \"\"\"Parse the DICOM specification and return metadata and attribute tree nodes.\n\n        Returns:\n            Tuple[Node, Node]: The metadata node and the content node.\n\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/spec_parser/#dcmspec.spec_parser.SpecParser.__init__","title":"<code>__init__(logger=None)</code>","text":"<p>Initialize the DICOM Specification parser with an optional logger.</p> PARAMETER DESCRIPTION <code>logger</code> <p>Logger instance to use. If None, a default logger is created.</p> <p> TYPE: <code>Optional[Logger]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/dcmspec/spec_parser.py</code> <pre><code>def __init__(self, logger: Optional[logging.Logger] = None):\n    \"\"\"Initialize the DICOM Specification parser with an optional logger.\n\n    Args:\n        logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n    \"\"\"\n    if logger is not None and not isinstance(logger, logging.Logger):\n        raise TypeError(\"logger must be an instance of logging.Logger or None\")\n    self.logger = logger or logging.getLogger(self.__class__.__name__)\n</code></pre>"},{"location":"api/spec_parser/#dcmspec.spec_parser.SpecParser.parse","title":"<code>parse(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Parse the DICOM specification and return metadata and attribute tree nodes.</p> RETURNS DESCRIPTION <code>Tuple[Node, Node]</code> <p>Tuple[Node, Node]: The metadata node and the content node.</p> Source code in <code>src/dcmspec/spec_parser.py</code> <pre><code>@abstractmethod\ndef parse(self, *args, **kwargs) -&gt; Tuple[Node, Node]:\n    \"\"\"Parse the DICOM specification and return metadata and attribute tree nodes.\n\n    Returns:\n        Tuple[Node, Node]: The metadata node and the content node.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/spec_printer/","title":"SpecPrinter","text":""},{"location":"api/spec_printer/#dcmspec.spec_printer.SpecPrinter","title":"<code>dcmspec.spec_printer.SpecPrinter</code>","text":"<p>Printer for DICOM specification models.</p> <p>Provides methods to print a SpecModel as a hierarchical tree or as a flat table, using rich formatting for console output. Supports colorized output and customizable logging.</p> Source code in <code>src/dcmspec/spec_printer.py</code> <pre><code>class SpecPrinter:\n    \"\"\"Printer for DICOM specification models.\n\n    Provides methods to print a SpecModel as a hierarchical tree or as a flat table,\n    using rich formatting for console output. Supports colorized output and customizable logging.\n    \"\"\"\n\n    def __init__(self, model: object, logger: Optional[logging.Logger] = None) -&gt; None:\n        \"\"\"Initialize the input handler with an optional logger.\n\n        Args:\n            model (object): An instance of SpecModel.\n            logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n        \"\"\"\n        if logger is not None and not isinstance(logger, logging.Logger):\n            raise TypeError(\"logger must be an instance of logging.Logger or None\")\n        self.logger = logger or logging.getLogger(self.__class__.__name__)\n\n        self.model = model\n        self.console = Console(highlight=False)\n\n    def print_tree(\n        self,\n        attr_names: Optional[Union[str, List[str]]] = None,\n        attr_widths: Optional[List[int]] = None,\n        colorize: bool = False,\n    ) -&gt; None:\n        \"\"\"Print the specification model as a hierarchical tree to the console.\n\n        Args:\n            attr_names (Optional[Union[str, list[str]]]): Attribute name(s) to display for each node.\n                If None, only the node's name is displayed.\n                If a string, displays that single attribute.\n                If a list of strings, displays all specified attributes.\n            attr_widths (Optional[list[int]]): List of widths for each attribute in attr_names.\n                If provided, each attribute will be padded/truncated to the specified width.\n            colorize (bool): Whether to colorize the output by node depth.\n\n        Example:\n            # This will nicely align the tag, type, and name values in the tree output:\n            printer.print_tree(attr_names=[\"elem_tag\", \"elem_type\", \"elem_name\"], attr_widths=[11, 2, 64])\n\n        Returns:\n            None\n\n        \"\"\"\n        for pre, fill, node in RenderTree(self.model.content):\n            style = LEVEL_COLORS[node.depth % len(LEVEL_COLORS)] if colorize else \"default\"\n            pre_text = Text(pre)\n            if attr_names is None:\n                node_text = Text(str(node.name), style=style)\n            else:\n                if isinstance(attr_names, str):\n                    attr_names = [attr_names]\n                values = [str(getattr(node, attr, \"\")) for attr in attr_names]\n                if attr_widths:\n                    # Pad/truncate each value to the specified width\n                    values = [\n                        v.ljust(w)[:w] if w is not None else v\n                        for v, w in zip(values, attr_widths)\n                    ]\n                attr_text = \" \".join(values)\n                node_text = Text(attr_text, style=style)\n            self.console.print(pre_text + node_text)\n\n    def print_table(self, colorize: bool = False) -&gt; None:\n        \"\"\"Print the specification model as a flat table to the console.\n\n        Traverses the content tree and prints each node's attributes in a flat table,\n        using column headers from the metadata node. Optionally colorizes rows.\n\n        Args:\n            colorize (bool): Whether to colorize the output by node depth.\n\n        Returns:\n            None\n\n        \"\"\"\n        table = Table(show_header=True, header_style=\"bold magenta\", show_lines=True, box=box.ASCII_DOUBLE_HEAD)\n\n        # Define the columns using the extracted headers\n        for header in self.model.metadata.header:\n            table.add_column(header, width=20)\n\n        # Traverse the tree and add rows to the table\n        for node in PreOrderIter(self.model.content):\n            # skip the root node\n            if node.name == \"content\":\n                continue\n            row = [getattr(node, attr, \"\") for attr in self.model.metadata.column_to_attr.values()]\n            row_style = None\n            if colorize:\n                row_style = (\n                    \"yellow\"\n                    if self.model._is_include(node)\n                    else \"magenta\"\n                    if self.model._is_title(node)\n                    else LEVEL_COLORS[(node.depth - 1) % len(LEVEL_COLORS)]\n                )\n            table.add_row(*row, style=row_style)\n\n        self.console.print(table)\n</code></pre>"},{"location":"api/spec_printer/#dcmspec.spec_printer.SpecPrinter.__init__","title":"<code>__init__(model, logger=None)</code>","text":"<p>Initialize the input handler with an optional logger.</p> PARAMETER DESCRIPTION <code>model</code> <p>An instance of SpecModel.</p> <p> TYPE: <code>object</code> </p> <code>logger</code> <p>Logger instance to use. If None, a default logger is created.</p> <p> TYPE: <code>Optional[Logger]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/dcmspec/spec_printer.py</code> <pre><code>def __init__(self, model: object, logger: Optional[logging.Logger] = None) -&gt; None:\n    \"\"\"Initialize the input handler with an optional logger.\n\n    Args:\n        model (object): An instance of SpecModel.\n        logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n    \"\"\"\n    if logger is not None and not isinstance(logger, logging.Logger):\n        raise TypeError(\"logger must be an instance of logging.Logger or None\")\n    self.logger = logger or logging.getLogger(self.__class__.__name__)\n\n    self.model = model\n    self.console = Console(highlight=False)\n</code></pre>"},{"location":"api/spec_printer/#dcmspec.spec_printer.SpecPrinter.print_table","title":"<code>print_table(colorize=False)</code>","text":"<p>Print the specification model as a flat table to the console.</p> <p>Traverses the content tree and prints each node's attributes in a flat table, using column headers from the metadata node. Optionally colorizes rows.</p> PARAMETER DESCRIPTION <code>colorize</code> <p>Whether to colorize the output by node depth.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>None</code> <p>None</p> Source code in <code>src/dcmspec/spec_printer.py</code> <pre><code>def print_table(self, colorize: bool = False) -&gt; None:\n    \"\"\"Print the specification model as a flat table to the console.\n\n    Traverses the content tree and prints each node's attributes in a flat table,\n    using column headers from the metadata node. Optionally colorizes rows.\n\n    Args:\n        colorize (bool): Whether to colorize the output by node depth.\n\n    Returns:\n        None\n\n    \"\"\"\n    table = Table(show_header=True, header_style=\"bold magenta\", show_lines=True, box=box.ASCII_DOUBLE_HEAD)\n\n    # Define the columns using the extracted headers\n    for header in self.model.metadata.header:\n        table.add_column(header, width=20)\n\n    # Traverse the tree and add rows to the table\n    for node in PreOrderIter(self.model.content):\n        # skip the root node\n        if node.name == \"content\":\n            continue\n        row = [getattr(node, attr, \"\") for attr in self.model.metadata.column_to_attr.values()]\n        row_style = None\n        if colorize:\n            row_style = (\n                \"yellow\"\n                if self.model._is_include(node)\n                else \"magenta\"\n                if self.model._is_title(node)\n                else LEVEL_COLORS[(node.depth - 1) % len(LEVEL_COLORS)]\n            )\n        table.add_row(*row, style=row_style)\n\n    self.console.print(table)\n</code></pre>"},{"location":"api/spec_printer/#dcmspec.spec_printer.SpecPrinter.print_tree","title":"<code>print_tree(attr_names=None, attr_widths=None, colorize=False)</code>","text":"<p>Print the specification model as a hierarchical tree to the console.</p> PARAMETER DESCRIPTION <code>attr_names</code> <p>Attribute name(s) to display for each node. If None, only the node's name is displayed. If a string, displays that single attribute. If a list of strings, displays all specified attributes.</p> <p> TYPE: <code>Optional[Union[str, list[str]]]</code> DEFAULT: <code>None</code> </p> <code>attr_widths</code> <p>List of widths for each attribute in attr_names. If provided, each attribute will be padded/truncated to the specified width.</p> <p> TYPE: <code>Optional[list[int]]</code> DEFAULT: <code>None</code> </p> <code>colorize</code> <p>Whether to colorize the output by node depth.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Example RETURNS DESCRIPTION <code>None</code> <p>None</p> Source code in <code>src/dcmspec/spec_printer.py</code> <pre><code>def print_tree(\n    self,\n    attr_names: Optional[Union[str, List[str]]] = None,\n    attr_widths: Optional[List[int]] = None,\n    colorize: bool = False,\n) -&gt; None:\n    \"\"\"Print the specification model as a hierarchical tree to the console.\n\n    Args:\n        attr_names (Optional[Union[str, list[str]]]): Attribute name(s) to display for each node.\n            If None, only the node's name is displayed.\n            If a string, displays that single attribute.\n            If a list of strings, displays all specified attributes.\n        attr_widths (Optional[list[int]]): List of widths for each attribute in attr_names.\n            If provided, each attribute will be padded/truncated to the specified width.\n        colorize (bool): Whether to colorize the output by node depth.\n\n    Example:\n        # This will nicely align the tag, type, and name values in the tree output:\n        printer.print_tree(attr_names=[\"elem_tag\", \"elem_type\", \"elem_name\"], attr_widths=[11, 2, 64])\n\n    Returns:\n        None\n\n    \"\"\"\n    for pre, fill, node in RenderTree(self.model.content):\n        style = LEVEL_COLORS[node.depth % len(LEVEL_COLORS)] if colorize else \"default\"\n        pre_text = Text(pre)\n        if attr_names is None:\n            node_text = Text(str(node.name), style=style)\n        else:\n            if isinstance(attr_names, str):\n                attr_names = [attr_names]\n            values = [str(getattr(node, attr, \"\")) for attr in attr_names]\n            if attr_widths:\n                # Pad/truncate each value to the specified width\n                values = [\n                    v.ljust(w)[:w] if w is not None else v\n                    for v, w in zip(values, attr_widths)\n                ]\n            attr_text = \" \".join(values)\n            node_text = Text(attr_text, style=style)\n        self.console.print(pre_text + node_text)\n</code></pre>"},{"location":"api/spec_printer/#dcmspec.spec_printer.SpecPrinter.print_tree--this-will-nicely-align-the-tag-type-and-name-values-in-the-tree-output","title":"This will nicely align the tag, type, and name values in the tree output:","text":"<p>printer.print_tree(attr_names=[\"elem_tag\", \"elem_type\", \"elem_name\"], attr_widths=[11, 2, 64])</p>"},{"location":"api/spec_store/","title":"SpecStore","text":""},{"location":"api/spec_store/#dcmspec.spec_store.SpecStore","title":"<code>dcmspec.spec_store.SpecStore</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for DICOM specification model storage backends.</p> <p>Subclasses should implement methods for loading and saving models.</p> Source code in <code>src/dcmspec/spec_store.py</code> <pre><code>class SpecStore(ABC):\n    \"\"\"Abstract base class for DICOM specification model storage backends.\n\n    Subclasses should implement methods for loading and saving models.\n    \"\"\"\n\n    def __init__(self, logger: Optional[logging.Logger] = None):\n        \"\"\"Initialize the model store with an optional logger.\n\n        Args:\n            logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n        \"\"\"\n        if logger is not None and not isinstance(logger, logging.Logger):\n            raise TypeError(\"logger must be an instance of logging.Logger or None\")\n        self.logger = logger or logging.getLogger(self.__class__.__name__)\n\n    @abstractmethod\n    def load(self, path: str) -&gt; SpecModel:\n        \"\"\"Load a model from the specified path.\n\n        Args:\n            path (str): The path to the file or resource to load from.\n\n        Returns:\n            SpecModel: The loaded model.\n\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def save(self, model: SpecModel, path: str) -&gt; None:\n        \"\"\"Save a model to the specified path.\n\n        Args:\n            model (SpecModel): The model to save.\n            path (str): The path to the file or resource to save to.\n\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/spec_store/#dcmspec.spec_store.SpecStore.__init__","title":"<code>__init__(logger=None)</code>","text":"<p>Initialize the model store with an optional logger.</p> PARAMETER DESCRIPTION <code>logger</code> <p>Logger instance to use. If None, a default logger is created.</p> <p> TYPE: <code>Optional[Logger]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/dcmspec/spec_store.py</code> <pre><code>def __init__(self, logger: Optional[logging.Logger] = None):\n    \"\"\"Initialize the model store with an optional logger.\n\n    Args:\n        logger (Optional[logging.Logger]): Logger instance to use. If None, a default logger is created.\n\n    \"\"\"\n    if logger is not None and not isinstance(logger, logging.Logger):\n        raise TypeError(\"logger must be an instance of logging.Logger or None\")\n    self.logger = logger or logging.getLogger(self.__class__.__name__)\n</code></pre>"},{"location":"api/spec_store/#dcmspec.spec_store.SpecStore.load","title":"<code>load(path)</code>  <code>abstractmethod</code>","text":"<p>Load a model from the specified path.</p> PARAMETER DESCRIPTION <code>path</code> <p>The path to the file or resource to load from.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>SpecModel</code> <p>The loaded model.</p> <p> TYPE: <code>SpecModel</code> </p> Source code in <code>src/dcmspec/spec_store.py</code> <pre><code>@abstractmethod\ndef load(self, path: str) -&gt; SpecModel:\n    \"\"\"Load a model from the specified path.\n\n    Args:\n        path (str): The path to the file or resource to load from.\n\n    Returns:\n        SpecModel: The loaded model.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/spec_store/#dcmspec.spec_store.SpecStore.save","title":"<code>save(model, path)</code>  <code>abstractmethod</code>","text":"<p>Save a model to the specified path.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to save.</p> <p> TYPE: <code>SpecModel</code> </p> <code>path</code> <p>The path to the file or resource to save to.</p> <p> TYPE: <code>str</code> </p> Source code in <code>src/dcmspec/spec_store.py</code> <pre><code>@abstractmethod\ndef save(self, model: SpecModel, path: str) -&gt; None:\n    \"\"\"Save a model to the specified path.\n\n    Args:\n        model (SpecModel): The model to save.\n        path (str): The path to the file or resource to save to.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ups_xhtml_doc_handler/","title":"UPSXHTMLDocHandler","text":""},{"location":"api/ups_xhtml_doc_handler/#dcmspec.ups_xhtml_doc_handler.UPSXHTMLDocHandler","title":"<code>dcmspec.ups_xhtml_doc_handler.UPSXHTMLDocHandler</code>","text":"<p>               Bases: <code>XHTMLDocHandler</code></p> <p>XHTMLDocHandler subclass for UPS, with table patching.</p> <p>This handler applies UPS-specific patching to DICOM XHTML tables after parsing. It corrects a known issue in the Output Information Sequence table for UPS services, fixing the nesting level of the 'Include' row.</p> Source code in <code>src/dcmspec/ups_xhtml_doc_handler.py</code> <pre><code>class UPSXHTMLDocHandler(XHTMLDocHandler):\n    \"\"\"XHTMLDocHandler subclass for UPS, with table patching.\n\n    This handler applies UPS-specific patching to DICOM XHTML tables after parsing.\n    It corrects a known issue in the Output Information Sequence table for UPS services,\n    fixing the nesting level of the 'Include' row.\n    \"\"\"\n\n    def __init__(self, config=None, logger=None):\n        \"\"\"Initialize the UPSXHTMLDocHandler.\n\n        Sets up the handler with the given configuration and logger, and creates a DOMUtils\n        instance for DOM navigation.\n\n        Args:\n            config (optional): Configuration object for the handler.\n            logger (optional): Logger instance to use. If None, a default logger is created.\n\n        \"\"\"\n        super().__init__(config=config, logger=logger)\n        self.dom_utils = DOMUtils(logger=self.logger)\n\n    def parse_dom(self, file_path: str) -&gt; BeautifulSoup:\n        \"\"\"Parse a cached XHTML file and apply UPS-specific table patching.\n\n        Calls the base class's parse_dom, then patches the Output Information Sequence Include nesting level.\n\n        Args:\n            file_path (str): Path to the cached XHTML file to parse.\n\n        Returns:\n            BeautifulSoup: The patched DOM object.\n\n        \"\"\"\n        dom = super().parse_dom(file_path)\n        # Patch the table after parsing\n        self._patch_table(dom, \"table_CC.2.5-3\")  # or pass table_id dynamically if needed\n        return dom\n\n    def _patch_table(self, dom, table_id):\n        \"\"\"Patch the specified XHTML table to fix an Include nesting level error.\n\n        In the UPS, the 'Include' row under the '&gt;Output Information Sequence' row is missing one '&gt;' nesting symbol.\n\n        Args:\n            dom: The BeautifulSoup DOM object representing the XHTML document.\n            table_id: The ID of the table to patch.\n\n        \"\"\"\n        sequence_label = \"&gt;Output Information Sequence\"\n        target_element_id = self._search_element_id(dom, table_id, sequence_label)\n        if not target_element_id:\n            self.logger.warning(\"Output Information Sequence Include Row element ID not found\")\n            return\n\n        element = dom.find(id=target_element_id).find_parent()\n        span_element = element.find(\"span\", class_=\"italic\")\n        if span_element:\n            children_to_modify = [\n                child for child in span_element.children\n                if isinstance(child, str) and \"&gt;Include\" in child\n            ]\n            for child in children_to_modify:\n                new_text = child.replace(\"&gt;Include\", \"&gt;&gt;Include\")\n                child.replace_with(new_text)\n\n    def _search_element_id(self, dom, table_id, sequence_label):\n        table = self.dom_utils.get_table(dom, table_id)\n        if not table:\n            return None\n\n        self.logger.debug(f\"Table with id {table_id} found\")\n        tr_elements = table.find_all(\"tr\")\n        include_id = self._search_sequence_include_id(tr_elements, sequence_label)\n\n        if include_id is None:\n            self.logger.debug(\"No &lt;tr&gt; matching criteria found\")\n\n        return include_id\n\n    def _search_sequence_include_id(self, tr_elements, sequence_label):\n        target_found = False\n        for tr in tr_elements:\n            first_td = tr.find(\"td\")\n            if first_td and first_td.get_text(strip=True) == sequence_label:\n                self.logger.debug(f\"{sequence_label} row found\")\n                target_found = True\n                break\n\n        if target_found:\n            tr = tr.find_next(\"tr\")\n            if tr is not None:\n                first_td = tr.find(\"td\")\n                if first_td and first_td.get_text(strip=True).startswith(\"&gt;Include\"):\n                    self.logger.debug(\"Include &lt;tr&gt; found\")\n                    return first_td.find(\"a\")[\"id\"]\n\n        return None\n</code></pre>"},{"location":"api/ups_xhtml_doc_handler/#dcmspec.ups_xhtml_doc_handler.UPSXHTMLDocHandler.__init__","title":"<code>__init__(config=None, logger=None)</code>","text":"<p>Initialize the UPSXHTMLDocHandler.</p> <p>Sets up the handler with the given configuration and logger, and creates a DOMUtils instance for DOM navigation.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object for the handler.</p> <p> TYPE: <code>optional</code> DEFAULT: <code>None</code> </p> <code>logger</code> <p>Logger instance to use. If None, a default logger is created.</p> <p> TYPE: <code>optional</code> DEFAULT: <code>None</code> </p> Source code in <code>src/dcmspec/ups_xhtml_doc_handler.py</code> <pre><code>def __init__(self, config=None, logger=None):\n    \"\"\"Initialize the UPSXHTMLDocHandler.\n\n    Sets up the handler with the given configuration and logger, and creates a DOMUtils\n    instance for DOM navigation.\n\n    Args:\n        config (optional): Configuration object for the handler.\n        logger (optional): Logger instance to use. If None, a default logger is created.\n\n    \"\"\"\n    super().__init__(config=config, logger=logger)\n    self.dom_utils = DOMUtils(logger=self.logger)\n</code></pre>"},{"location":"api/ups_xhtml_doc_handler/#dcmspec.ups_xhtml_doc_handler.UPSXHTMLDocHandler.parse_dom","title":"<code>parse_dom(file_path)</code>","text":"<p>Parse a cached XHTML file and apply UPS-specific table patching.</p> <p>Calls the base class's parse_dom, then patches the Output Information Sequence Include nesting level.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to the cached XHTML file to parse.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>BeautifulSoup</code> <p>The patched DOM object.</p> <p> TYPE: <code>BeautifulSoup</code> </p> Source code in <code>src/dcmspec/ups_xhtml_doc_handler.py</code> <pre><code>def parse_dom(self, file_path: str) -&gt; BeautifulSoup:\n    \"\"\"Parse a cached XHTML file and apply UPS-specific table patching.\n\n    Calls the base class's parse_dom, then patches the Output Information Sequence Include nesting level.\n\n    Args:\n        file_path (str): Path to the cached XHTML file to parse.\n\n    Returns:\n        BeautifulSoup: The patched DOM object.\n\n    \"\"\"\n    dom = super().parse_dom(file_path)\n    # Patch the table after parsing\n    self._patch_table(dom, \"table_CC.2.5-3\")  # or pass table_id dynamically if needed\n    return dom\n</code></pre>"},{"location":"api/xhtml_doc_handler/","title":"XHTMLDocHandler","text":""},{"location":"api/xhtml_doc_handler/#dcmspec.xhtml_doc_handler.XHTMLDocHandler","title":"<code>dcmspec.xhtml_doc_handler.XHTMLDocHandler</code>","text":"<p>               Bases: <code>DocHandler</code></p> <p>Handler class for DICOM specifications documents in XHTML format.</p> <p>Provides methods to download, cache, and parse XHTML documents, returning a BeautifulSoup DOM object. Inherits configuration and logging from DocHandler.</p> Source code in <code>src/dcmspec/xhtml_doc_handler.py</code> <pre><code>class XHTMLDocHandler(DocHandler):\n    \"\"\"Handler class for DICOM specifications documents in XHTML format.\n\n    Provides methods to download, cache, and parse XHTML documents, returning a BeautifulSoup DOM object.\n    Inherits configuration and logging from DocHandler.\n    \"\"\"\n\n    def __init__(self, config: Optional[Config] = None, logger: Optional[logging.Logger] = None):\n        \"\"\"Initialize the XHTML document handler and set cache_file_name to None.\"\"\"\n        super().__init__(config=config, logger=logger)\n        self.cache_file_name = None\n\n    def get_dom(self, cache_file_name: str, url: Optional[str] = None, force_download: bool = False) -&gt; BeautifulSoup:\n        \"\"\"Open and parse an XHTML file, downloading it if needed.\n\n        Args:\n            cache_file_name (str): Path to the local cached XHTML file.\n            url (str, optional): URL to download the file from if not cached or if force_download is True.\n            force_download (bool): If True, do not use cache and download the file from the URL.\n\n        Returns:\n            BeautifulSoup: Parsed DOM.\n\n        \"\"\"\n        # Set cache_file_name as an attribute for downstream use (e.g., in SpecFactory)\n        self.cache_file_name = cache_file_name\n\n        cache_file_path = os.path.join(self.config.get_param(\"cache_dir\"), \"standard\", cache_file_name)\n        need_download = force_download or (not os.path.exists(cache_file_path))\n        if need_download:\n            if not url:\n                raise ValueError(\"URL must be provided to download the file.\")\n            cache_file_path = self.download(url, cache_file_name)\n        return self.parse_dom(cache_file_path)\n\n    def download(self, url: str, cache_file_name: str) -&gt; str:\n        \"\"\"Download and cache an XHTML file from a URL.\n\n        Args:\n            url: The URL of the XHTML document to download.\n            cache_file_name: The filename of the cached document.\n\n        Returns:\n            The file path where the document was saved.\n\n        Raises:\n            RuntimeError: If the download or save fails.\n\n        \"\"\"\n        file_path = os.path.join(self.config.get_param(\"cache_dir\"), \"standard\", cache_file_name)\n        self.logger.info(f\"Downloading XHTML document from {url} to {file_path}\")\n        try:\n            self._ensure_dir_exists(file_path)  # Fail before download if directory can't be created\n        except OSError as e:\n            self.logger.error(f\"Failed to create directory for {file_path}: {e}\")\n            raise RuntimeError(f\"Failed to create directory for {file_path}: {e}\") from e\n        try:\n            html_content = self._fetch_url_content(url)\n            self._save_content(file_path, html_content)\n            self.logger.info(f\"Document downloaded to {file_path}\")\n            return file_path\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"Failed to download {url}: {e}\")\n            raise RuntimeError(f\"Failed to download {url}: {e}\") from e\n        except OSError as e:\n            self.logger.error(f\"Failed to save file {file_path}: {e}\")\n            raise RuntimeError(f\"Failed to save file {file_path}: {e}\") from e\n\n    def _ensure_dir_exists(self, file_path: str) -&gt; None:\n        \"\"\"Ensure the directory for the file path exists.\"\"\"\n        dir_name = os.path.dirname(file_path)\n        if dir_name:\n            os.makedirs(dir_name, exist_ok=True)\n\n    def _fetch_url_content(self, url: str) -&gt; str:\n        \"\"\"Fetch content from a URL and return as UTF-8 text.\"\"\"\n        response = requests.get(url, timeout=30)\n        response.raise_for_status()\n        # Force UTF-8 decoding to avoid getting \u00c3 (/u00C3) characters\n        response.encoding = \"utf-8\"\n        # IN the future we may want to manually decode the content and ignore errors:\n        # html_content = response.content.decode(\"utf-8\", errors=\"ignore\")\n        # return html_content\n        return response.text\n\n    def _save_content(self, file_path: str, html_content: str) -&gt; None:\n        \"\"\"Clean and save HTML content to a file.\"\"\"\n        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n            # Replace ZWSP with nothing and NBSP with a regular space\n            cleaned_content = re.sub(r\"\\u200b\", \"\", html_content)\n            cleaned_content = re.sub(r\"\\u00a0\", \" \", cleaned_content)\n            f.write(cleaned_content)\n\n    def parse_dom(self, file_path: str) -&gt; BeautifulSoup:\n        \"\"\"Parse a cached XHTML file into a BeautifulSoup DOM object.\n\n        Args:\n            file_path (str): Path to the cached XHTML file to parse.\n\n        Returns:\n            BeautifulSoup: The parsed DOM object.\n\n        Raises:\n            RuntimeError: If the file cannot be read or parsed.\n\n        \"\"\"\n        self.logger.info(f\"Reading XHTML DOM from {file_path}\")\n        try:\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                content = f.read()\n            # dom = BeautifulSoup(content, \"html.parser\")  # use python HTML parser. Fine for XHTML. Unreliable for XML.\n            # dom = BeautifulSoup(content, \"lxml\")  # use lxml package parser. Default to HTML and generates a warning.\n            dom = BeautifulSoup(content, features=\"xml\")  # use lxml package parser. Force using XML. Safest choice.\n            self.logger.info(\"XHTML DOM read successfully\")\n\n            return dom\n        except OSError as e:\n            self.logger.error(f\"Failed to read file {file_path}: {e}\")\n            raise RuntimeError(f\"Failed to read file {file_path}: {e}\") from e\n        except Exception as e:\n            self.logger.error(f\"Failed to parse XHTML file {file_path}: {e}\")\n            raise RuntimeError(f\"Failed to parse XHTML file {file_path}: {e}\") from e\n\n    def _patch_table(self, dom: BeautifulSoup, table_id: str) -&gt; None:\n        \"\"\"Patch an XHTML table to fix potential errors.\n\n        This method does nothing and may be overridden in derived classes if patching is needed.\n\n        Args:\n            dom (BeautifulSoup): The parsed XHTML DOM object.\n            table_id (str): The ID of the table to patch.\n\n        Returns:\n            None\n\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/xhtml_doc_handler/#dcmspec.xhtml_doc_handler.XHTMLDocHandler.__init__","title":"<code>__init__(config=None, logger=None)</code>","text":"<p>Initialize the XHTML document handler and set cache_file_name to None.</p> Source code in <code>src/dcmspec/xhtml_doc_handler.py</code> <pre><code>def __init__(self, config: Optional[Config] = None, logger: Optional[logging.Logger] = None):\n    \"\"\"Initialize the XHTML document handler and set cache_file_name to None.\"\"\"\n    super().__init__(config=config, logger=logger)\n    self.cache_file_name = None\n</code></pre>"},{"location":"api/xhtml_doc_handler/#dcmspec.xhtml_doc_handler.XHTMLDocHandler.download","title":"<code>download(url, cache_file_name)</code>","text":"<p>Download and cache an XHTML file from a URL.</p> PARAMETER DESCRIPTION <code>url</code> <p>The URL of the XHTML document to download.</p> <p> TYPE: <code>str</code> </p> <code>cache_file_name</code> <p>The filename of the cached document.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The file path where the document was saved.</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If the download or save fails.</p> Source code in <code>src/dcmspec/xhtml_doc_handler.py</code> <pre><code>def download(self, url: str, cache_file_name: str) -&gt; str:\n    \"\"\"Download and cache an XHTML file from a URL.\n\n    Args:\n        url: The URL of the XHTML document to download.\n        cache_file_name: The filename of the cached document.\n\n    Returns:\n        The file path where the document was saved.\n\n    Raises:\n        RuntimeError: If the download or save fails.\n\n    \"\"\"\n    file_path = os.path.join(self.config.get_param(\"cache_dir\"), \"standard\", cache_file_name)\n    self.logger.info(f\"Downloading XHTML document from {url} to {file_path}\")\n    try:\n        self._ensure_dir_exists(file_path)  # Fail before download if directory can't be created\n    except OSError as e:\n        self.logger.error(f\"Failed to create directory for {file_path}: {e}\")\n        raise RuntimeError(f\"Failed to create directory for {file_path}: {e}\") from e\n    try:\n        html_content = self._fetch_url_content(url)\n        self._save_content(file_path, html_content)\n        self.logger.info(f\"Document downloaded to {file_path}\")\n        return file_path\n    except requests.exceptions.RequestException as e:\n        self.logger.error(f\"Failed to download {url}: {e}\")\n        raise RuntimeError(f\"Failed to download {url}: {e}\") from e\n    except OSError as e:\n        self.logger.error(f\"Failed to save file {file_path}: {e}\")\n        raise RuntimeError(f\"Failed to save file {file_path}: {e}\") from e\n</code></pre>"},{"location":"api/xhtml_doc_handler/#dcmspec.xhtml_doc_handler.XHTMLDocHandler.get_dom","title":"<code>get_dom(cache_file_name, url=None, force_download=False)</code>","text":"<p>Open and parse an XHTML file, downloading it if needed.</p> PARAMETER DESCRIPTION <code>cache_file_name</code> <p>Path to the local cached XHTML file.</p> <p> TYPE: <code>str</code> </p> <code>url</code> <p>URL to download the file from if not cached or if force_download is True.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>force_download</code> <p>If True, do not use cache and download the file from the URL.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>BeautifulSoup</code> <p>Parsed DOM.</p> <p> TYPE: <code>BeautifulSoup</code> </p> Source code in <code>src/dcmspec/xhtml_doc_handler.py</code> <pre><code>def get_dom(self, cache_file_name: str, url: Optional[str] = None, force_download: bool = False) -&gt; BeautifulSoup:\n    \"\"\"Open and parse an XHTML file, downloading it if needed.\n\n    Args:\n        cache_file_name (str): Path to the local cached XHTML file.\n        url (str, optional): URL to download the file from if not cached or if force_download is True.\n        force_download (bool): If True, do not use cache and download the file from the URL.\n\n    Returns:\n        BeautifulSoup: Parsed DOM.\n\n    \"\"\"\n    # Set cache_file_name as an attribute for downstream use (e.g., in SpecFactory)\n    self.cache_file_name = cache_file_name\n\n    cache_file_path = os.path.join(self.config.get_param(\"cache_dir\"), \"standard\", cache_file_name)\n    need_download = force_download or (not os.path.exists(cache_file_path))\n    if need_download:\n        if not url:\n            raise ValueError(\"URL must be provided to download the file.\")\n        cache_file_path = self.download(url, cache_file_name)\n    return self.parse_dom(cache_file_path)\n</code></pre>"},{"location":"api/xhtml_doc_handler/#dcmspec.xhtml_doc_handler.XHTMLDocHandler.parse_dom","title":"<code>parse_dom(file_path)</code>","text":"<p>Parse a cached XHTML file into a BeautifulSoup DOM object.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to the cached XHTML file to parse.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>BeautifulSoup</code> <p>The parsed DOM object.</p> <p> TYPE: <code>BeautifulSoup</code> </p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If the file cannot be read or parsed.</p> Source code in <code>src/dcmspec/xhtml_doc_handler.py</code> <pre><code>def parse_dom(self, file_path: str) -&gt; BeautifulSoup:\n    \"\"\"Parse a cached XHTML file into a BeautifulSoup DOM object.\n\n    Args:\n        file_path (str): Path to the cached XHTML file to parse.\n\n    Returns:\n        BeautifulSoup: The parsed DOM object.\n\n    Raises:\n        RuntimeError: If the file cannot be read or parsed.\n\n    \"\"\"\n    self.logger.info(f\"Reading XHTML DOM from {file_path}\")\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            content = f.read()\n        # dom = BeautifulSoup(content, \"html.parser\")  # use python HTML parser. Fine for XHTML. Unreliable for XML.\n        # dom = BeautifulSoup(content, \"lxml\")  # use lxml package parser. Default to HTML and generates a warning.\n        dom = BeautifulSoup(content, features=\"xml\")  # use lxml package parser. Force using XML. Safest choice.\n        self.logger.info(\"XHTML DOM read successfully\")\n\n        return dom\n    except OSError as e:\n        self.logger.error(f\"Failed to read file {file_path}: {e}\")\n        raise RuntimeError(f\"Failed to read file {file_path}: {e}\") from e\n    except Exception as e:\n        self.logger.error(f\"Failed to parse XHTML file {file_path}: {e}\")\n        raise RuntimeError(f\"Failed to parse XHTML file {file_path}: {e}\") from e\n</code></pre>"}]}